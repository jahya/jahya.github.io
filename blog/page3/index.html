<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1.0"><meta name=author content="Andrew McWilliams"><title>Blog - Andrew McWilliams</title><link href=http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css rel=stylesheet><link href="http://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic" rel=stylesheet type=text/css><link rel=stylesheet href=/stylesheets/style.83cf.css><!--[if lt IE 9]>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv-printshiv.min.js"></script>
    <![endif]--><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.4f09.png><link rel="shortcut icon" href=/images/favicon.6537.png></head><body id=blog class=content><header role=banner><h1><a href="/"><span class=shift>Andrew</span> M<span class=sub>c</span>Williams</a></h1><nav role=navigation><h2>Navigation</h2><a id=menu-button href=#><i class="fa fa-bars"></i> <i class="fa fa-times"></i></a><ul class=cta><li><a href="/about/">About</a></li><li><a href="/works/">Works</a></li><li><a href="/blog/">Blog</a></li></ul></nav></header><div class=flex-row><main class=left><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/randomness-in-algorithm>Randomness in the Algorithm</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-09-23>Monday 23 September, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Randomness%20in%20the%20Algorithm&amp;url=http://jahya.net/blog/randomness-in-algorithm&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Randomness%20in%20the%20Algorithm&amp;u=http://jahya.net/blog/randomness-in-algorithm"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Randomness%20in%20the%20Algorithm&amp;u=http://jahya.net/blog/randomness-in-algorithm"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>Wow! Just had to share this for all of you struggling with the <a href=//blog/rgbdtoolkit-calibration-tutorial>RGBDToolkit calibration process</a>.</p><p>I just remounted my depth sensor to my camera, and re-ran the correspondence calibration. It failed completely - a total fail. But then I hit <code>Regenerate RGB/Depth correspondence</code> again, without changing anything - and I got this really solid-looking calibration!</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-xdbG4xOdIz4/Uj_GNTBRdmI/AAAAAAAABWs/6AVTcz-oZlQ/s1600/rgbd_good_correspondence.png><img alt="A solid correspondence" src=http://1.bp.blogspot.com/-xdbG4xOdIz4/Uj_GNTBRdmI/AAAAAAAABWs/6AVTcz-oZlQ/s1600/rgbd_good_correspondence.png width=500 height=281></a></td></tr><tr><td class=tr-caption style="text-align: center">A solid correspondence</td></tr></tbody></table><p>It's weird - same calibration button, same data. James had told me a while back there was a little bit of randomness in the algorithm, and I thought that meant it could go from not-so-good to good-ish. But clearly it can go from fail to great! Some hope for those struggling.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-9uDzn7qIuJI/Uj_F9-OJRpI/AAAAAAAABWk/t67W4UOU5K0/s1600/rgbd_lighting.jpg><img alt="The lighting conditions" src=http://4.bp.blogspot.com/-9uDzn7qIuJI/Uj_F9-OJRpI/AAAAAAAABWk/t67W4UOU5K0/s500/rgbd_lighting.jpg width=500 height=375></a></td></tr><tr><td class=tr-caption style="text-align: center">The lighting conditions</td></tr></tbody></table><p>Here is a picture so you get some sense of the light condition...</p></div><a class=read-more href=/blog/randomness-in-algorithm>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/study-existential>Video: Existential</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-09-08>Sunday 8 September, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Video:%20Existential&amp;url=http://jahya.net/blog/study-existential&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Video:%20Existential&amp;u=http://jahya.net/blog/study-existential"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Video:%20Existential&amp;u=http://jahya.net/blog/study-existential"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>Every day we are surrounded by people we don't know. In every city a mass of people paradoxically leads to greater isolation and individualism - or does it?</p><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/73437198?byline=0&amp;portrait=0"></iframe></div><p>What is beyond this narrative? What social and spatial cues take over when we don't know the people surrounding around us? How are we engaging in a shared sense of space, and are when and where are we conscious of it? What is the role of this limited but intimately connected cultural cohesion? How does our physicality and embodied awareness contribute to this spatial conversation?</p><p>This experimental video shows footage of a commuter walking through the tunnels of the London Underground. It was shot using a <a href=//blog/how-depth-sensor-works-in-5-minutes>depth-sensing camera</a> and treated with a collage technique to overlay color from other video streams onto the point clouds. Moving a virtual camera around the scene in post-production, we can re-examine this daily journey from the perspective of a segment of total space, and see the intersections with passers-by.</p></div><a class=read-more href=/blog/study-existential>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/the-visual-art-of-brian-eno>The Visual Art of Brian Eno</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-08-16>Friday 16 August, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=The%20Visual%20Art%20of%20Brian%20Eno&amp;url=http://jahya.net/blog/the-visual-art-of-brian-eno&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=The%20Visual%20Art%20of%20Brian%20Eno&amp;u=http://jahya.net/blog/the-visual-art-of-brian-eno"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=The%20Visual%20Art%20of%20Brian%20Eno&amp;u=http://jahya.net/blog/the-visual-art-of-brian-eno"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>In this video Brian Eno talks about the motivations behind his visual art practice. There are some great quotes in here, and sentiments I think many of us share, like not being able to sit down and fully appreciate the time-stretched presence of your creation until an audience is in front of it and you can no longer change it.</p><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/67252143?byline=0&amp;portrait=0"></iframe></div><p>A few of the scenes in the video are shot with RGBDToolkit, which is how I came across it, but actually that's a very small fraction of the video. The makers of this film have used a technique I was thinking of experimenting with later - overlaying the 'wrong' video over a depthmap. A bit like projection mapping in virtual space.</p></div><a class=read-more href=/blog/the-visual-art-of-brian-eno>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/rgbdtoolkit-sketch-at-sampler>RGBDToolkit Sketch at The Sampler</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-08-15>Thursday 15 August, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=RGBDToolkit%20Sketch%20at%20The%20Sampler&amp;url=http://jahya.net/blog/rgbdtoolkit-sketch-at-sampler&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=RGBDToolkit%20Sketch%20at%20The%20Sampler&amp;u=http://jahya.net/blog/rgbdtoolkit-sketch-at-sampler"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=RGBDToolkit%20Sketch%20at%20The%20Sampler&amp;u=http://jahya.net/blog/rgbdtoolkit-sketch-at-sampler"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>This is a sketch I made from <a href=http://jahyawords.blogspot.co.uk/2013/07/rgbdtoolkit-workshop-at-eyebeam.html>RGBD</a> footage I took last week, at The Sampler in Bushwick. I'm experimenting with the post-production effects in the <a href=//blog/rgbdtoolkit-visualizer-tutorial>RGBDToolkit Visualize app</a>, creating kind of an avalanche of effects to try to become familiar with the possibilities.</p><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/72454757?byline=0&amp;portrait=0"></iframe></div><p>The effects used in this study are:</p><ul><li>Geometry tab: Edge clipping (generally very low, but increased when the arms are outstretched and the polygons stretch to connect them to the bodies)</li></ul><p>The rest are in the Rendering tab:</p><ul><li>Point Alpha</li><li>Point Size</li><li>Random Point Amount</li><li>Wireframe Alpha</li><li>Wireframe Thickness</li><li>Mesh Alpha (the application of RGB between the polygon lines)</li></ul></div><a class=read-more href=/blog/rgbdtoolkit-sketch-at-sampler>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/the-artist-geek-hybrid>The Artist-Geek Hybrid</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-08-12>Monday 12 August, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=The%20Artist-Geek%20Hybrid&amp;url=http://jahya.net/blog/the-artist-geek-hybrid&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=The%20Artist-Geek%20Hybrid&amp;u=http://jahya.net/blog/the-artist-geek-hybrid"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=The%20Artist-Geek%20Hybrid&amp;u=http://jahya.net/blog/the-artist-geek-hybrid"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>In this talk at TEDx Seattle, John SanGiovanni discusses what he calls the Artist-Geek Hybrid, and crossover skills in general. This is a topic I am discovering more and more about, as I am increasingly playing a variety of roles myself, including artist, experience designer, and software developer.</p><div style="text-align: center"><iframe allowfullscreen frameborder=0 height=328 src=http://www.youtube.com/embed/mQywiaQXUeI title="YouTube video player" width=538></iframe></div><p>Here is a quick summary of the video: John talks about the value of having a 'wildcard' of cross-domain knowledge, which he illustrates with a crazy Teenage Mutant Turtle story.</p><p><strong>Intersections and S-T-E-M</strong><br>John continues by explaining that in the ever-evolving technology sector, real value comes from the intersection between domains. In the commonly-used acronym <code>S-T-E-M</code>, most people focus on the literal meaning (Science, Technology, Engineering, Math). John contends that in this (or any acronym you care to mention) there is now more value in the hyphens.</p><p>He extends this further, and points out that given any two domains or disciplines, there is generally a lot of value in the space in between. Just to add a little of my own feeling here, I think that this is a product of the pace of technological change.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-is61jr8GyWY/UgkgXVRUvpI/AAAAAAAABV8/mZKaAlIszss/s1600/art-geek.png><img alt="Intersections between various domains" src=http://4.bp.blogspot.com/-is61jr8GyWY/UgkgXVRUvpI/AAAAAAAABV8/mZKaAlIszss/s1600/art-geek.png width=500 height=336></a></td></tr><tr><td class=tr-caption style="text-align: center">Intersections between various domains</td></tr></tbody></table><p>Playing fields are being <a href="http://www.informit.com/articles/article.aspx?p=1946001">shaken at their foundations</a> and traditions haven't had time to settle into a place. Disciplines don't know any more what exactly they can expect from each other. There is disruption, new industries are emerging, and old industries are continually developing new form. This form is as yet undecided.</p><p><strong>Multiple mindsets</strong><br>Another addition I would make is that there isn't just value in the wildcards that come from the intersection between disciplines. There is also power in multiple perspectives, and multiple approaches.</p><p>For example, an engineering, picture-building, problem-solving mindset is very different to an <a href=//blog/the-cascading-process>exploratory, irrational discovery mindset</a>. You can imagine other mixes and matches using different adjectives. For example the following, which are often attributed to people who work in various different domains: opportunistic, creative, and entrepreneurial mindsets - etc.</p><p>My feeling is that in uncertain and disruptive playing fields, being able to mix and match your mindset represents a powerful asset. The personal development required to achieve multiple mindsets comes from practice in crossing over skillsets, and so multiple mindsets is in fact another layer over this idea of intersections between domains.</p></div><a class=read-more href=/blog/the-artist-geek-hybrid>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/rgbdtoolkit-calibration-tutorial>RGBDToolkit Calibration Tutorial</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-08-10>Saturday 10 August, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=RGBDToolkit%20Calibration%20Tutorial&amp;url=http://jahya.net/blog/rgbdtoolkit-calibration-tutorial&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=RGBDToolkit%20Calibration%20Tutorial&amp;u=http://jahya.net/blog/rgbdtoolkit-calibration-tutorial"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=RGBDToolkit%20Calibration%20Tutorial&amp;u=http://jahya.net/blog/rgbdtoolkit-calibration-tutorial"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>The <a href=//blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit</a> is a fun piece of kit for tinkerers and artists, but it comes with a steep learning curve. This post will guide you through the hardest part - calibration.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-nM3cO6YZ_74/UfcwQJjDAdI/AAAAAAAABQ0/0rEqHT96_Fo/s1600/RGBDToolkit1.jpg><img alt="A mounted DSLR and Kinect, and volumetric video" src=http://4.bp.blogspot.com/-nM3cO6YZ_74/UfcwQJjDAdI/AAAAAAAABQ0/0rEqHT96_Fo/s500/RGBDToolkit1.jpg width=500 height=222></a></td></tr><tr><td class=tr-caption style="text-align: center">A mounted DSLR and Kinect, and volumetric video</td></tr></tbody></table><p>Ultimately calibration is about combining two commercial products which weren't designed to be combined - a depth sensor (either a Kinect or an Asus Xtion), and a DSLR (or some kind of RGB camera). Although it is undeniably difficult at first, the RGBDToolkit makes this process far easier than it would be if you were starting from scratch, and it gets easier and quicker with time.</p><p>This tutorial only covers calibration (pre-production). There is some great information on <a href=http://www.rgbdtoolkit.com/tutorials.html>capture and visualizing here</a>, and also <a href=//blog/rgbdtoolkit-visualizer-tutorial>here is a really good visualizer tutorial video from the workshop</a>.</p><p><strong>Why this tutorial?</strong><br>The RGBDToolkit guys write and maintain an <a href=http://www.rgbdtoolkit.com/tutorials.html>in-depth tutorial</a> on the website. It covers all bases and so is pretty dense. However, I found it easier to work from my own notes (made during an RGBD workshop). This post is just a write-up of those notes.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-scJLiauMJz8/UfcyAeHuRpI/AAAAAAAABRE/sVcqbkuX4nU/s1600/RGBDXtion2.jpg><img alt="An Asus Xtion Pro Live, the recommended alternative to a Kinect" src=http://4.bp.blogspot.com/-scJLiauMJz8/UfcyAeHuRpI/AAAAAAAABRE/sVcqbkuX4nU/s450/RGBDXtion2.jpg width=450 height=160></a></td></tr><tr><td class=tr-caption style="text-align: center">An Asus Xtion Pro Live, the recommended alternative to a Kinect</td></tr></tbody></table><p>This article talks in terms of the kit I am using (and I recommend this where possible):</p><ul><li>Mac / OSX</li><li><a href="http://www.asus.com/Multimedia/Xtion_PRO_LIVE/">Xtion Pro Live</a></li><li>Prefabricated mount purchased from the <a href="http://www.rgbdtoolkit.com/">RGBDToolkit store</a></li></ul><p>This post should still be pretty useful if you have a Kinect, or are using Windows, but may deviate in places. That said, it's mostly the same.</p><p>To follow along, you will also need:</p><ul><li>An <a href="https://github.com/obviousjim/RGBDToolkit/blob/master/chessboard_a3.pdf?raw=true">A3 checkerboard</a>, printed out and flatly glued or spray-mounted onto foam board or other flat surface</li></ul><p>Ok, let's get started!</p><p><strong>1. Install the software</strong><br>You'll need the software to get started. There is a <a href=http://www.rgbdtoolkit.com/downloads-confirmed.html>landing page for downloads here</a>, so you can check if there is a newer version. I used v005:</p><ul><li>If you are using an Asus Xtion, download and install the <a href=http://sensecast.com/files/Sensecast.and.Libs.Installer.v.0.02.dmg>Sensecast drivers</a><br><span style=font-style:italic>Otherwise, the RGBD software will crash on launch</span></li><li>Download the <a href=http://www.rgbdtoolkit.com/downloads/RGBDToolkit_preRelease_005_OSX.zip>beta v005 (application only)</a> <code>zip</code> file<br><span style=font-style:italic>- There are two application downloads for OSX, one with example data and one without example data<br>- The link above is for the <code>zip</code> without example data. If you want example data, you can download the <a href=http://www.rgbdtoolkit.com/downloads/RGBDToolkit_preRelease_005_OSX_example.zip>beta v005 (with example data)</a> <code>zip</code> file here instead</span></li><li>Unzip the <code>zip</code> file and take a look inside the resulting RGBDToolkit folder<br><span style=font-style:italic>The folder will be called something like <code>RGBDToolkit_preRelease_005_OSX</code></span></li><li>Among other things, you will find four <code>app</code> files inside:<br><span style=font-style:italic>- <code>RGBDCaptureKinect_1414.app</code><br>- <code>RGBDCaptureKinect_1473.app</code><br>- <code>RGBDCaptureXtionPro.app</code><br>- <code>RGBDVisualize.app</code></span></li><li>Go back outside the folder, and drag the entire folder into your Applications folder</li><li>Plug your depth sensor's USB cable into your computer</li><li>Launch the appropriate one of the three <code>Capture app</code>s for your depth sensor<br><span style=font-style:italic>- Which <code>app</code> to launch depends on which depth sensor you have - read about that below</span><br>- If all goes well you should see the depth sensor's live feed in the <code>Capture app</code> software's preview window<br>- If there is a problem, try quitting the <code>Capture app</code>, unplugging the depth sensor, replugging the depth sensor, and relaunching the <code>Capture app</code></li><li>When you have verified it works, quit the <code>Capture app</code> for now, and unplug the depth sensor's USB cable from the computer<br><span style=font-style:italic>In the next step we will be mounting the depth sensor to the camera, and that will be easier if the depth sensor is not plugged in to your computer!</span></li></ul><p>There may be four <code>app</code> files, but there are really only two <code>app</code> files you'll ever be working with. This is because there are separate Capture <code>app</code>s for three different models of depth sensor, so you'll choose the appropriate one for you and stick to it (I'm using the <code>RGBDCaptureXtionPro</code> <code>app</code> because I have an Asus Xtion Pro depth sensor). There is a single Visualize <code>app</code> regardless of which depth sensor you use.</p><p>As for the difference between Capture and Visualize, you can think of the stages of the RGBDToolkit process in cinematic terms:</p><ol><li>Pre-production<br><span style=font-style:italic>Getting ready to shoot, preparing</span></li><li>Production<br><span style=font-style:italic>Actually shooting footage</span></li><li>Post-production<br><span style=font-style:italic>Doing something with the footage you shot, editing</span></li></ol><p>The Capture <code>app</code> covers both pre-production and production, and the Visualize <code>app</code> covers post-production. The calibration process described in this tutorial is exclusively in the pre-production phase.</p><p><strong>2. Mount the camera to the depth sensor</strong><br>You can buy a mount from the makers of the RGBDToolkit. At the time of this writing there is a link at the top-right corner of <a href="http://www.rgbdtoolkit.com/">the official website</a>, and they deliver anywhere.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-H1NVA_Vh_HQ/UfwXgFensII/AAAAAAAABRg/KxdMffxkFV8/s1600/RGBDMount.jpg><img alt="A Kinect mount (left), and an Asus Xtion Pro mount (right)" src=http://4.bp.blogspot.com/-H1NVA_Vh_HQ/UfwXgFensII/AAAAAAAABRg/KxdMffxkFV8/s500/RGBDMount.jpg width=500 height=167></a></td></tr><tr><td class=tr-caption style="text-align: center">A Kinect mount (left), and an Asus Xtion Pro mount (right)</td></tr></tbody></table><p>There is a mount for the Asus Xtion and a mount for the Kinect, you need to make sure you buy the appropriate mount for your particular depth sensor.</p><p>Here you have a choice. You can watch the video, or continue along with the written steps. I'd recommend both, in that order. Watch the video to get a sense of what we are doing, then follow the steps to make sure you do things in the right order!</p><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/81937610?byline=0&amp;portrait=0"></iframe></div><p>Take a look at the image below. With the Xtion mount you need an extra bolt and three nuts (included in the package) to keep things in place. In case you ever need to replace these extras, it's just a 1.5" long 1/4"-20 bolt with three 1/4"-20 nuts.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-MF-2damKEBw/UfwYUt6l_9I/AAAAAAAABRs/YCHE5nUMADo/s1600/RGBDMount2.jpg><img alt="A mounted Asus Xtion Pro and Kinect" src=http://1.bp.blogspot.com/-MF-2damKEBw/UfwYUt6l_9I/AAAAAAAABRs/YCHE5nUMADo/s500/RGBDMount2.jpg width=500 height=167></a></td></tr><tr><td class=tr-caption style="text-align: center">A mounted Asus Xtion Pro and Kinect</td></tr></tbody></table><p>I found it handy to have a screwdriver, an adjustable wrench, and a pair of pliers for this part.</p><ul><li>Screw the orange RGBD mount wings to the metal box, with the four small screws<br><span style=font-style:italic>Don't tighten these screws too much, we are going to adjust these later</span></li><li>Remove the base plate from the Xtion<br><span style=font-style:italic>Just nudge the plastic covers with your fingernail, and then unscrew the screw you find inside</span></li><li>Place a nut in between the two screwholes of the Xtion base, hold it steady, and feed the long screw through by screwing it through the nut<br><span style=font-style:italic>- We are not touching the RGBD mount at this stage, it is to one side<br>- Use the screwdriver and pliers to tighten the screw to the nut</span></li><li>Once that is tight, add the next nut, followed by the RGBD mount, followed by the last nut<br><span style=font-style:italic>- Locate the depth sensor on the front of the Xtion - line up the mount so that the depth sensor is in the middle of the two orange mount wings<br>- Use the pliers to hold the middle nut still while you use the wrench to tighten the last nut - no movement should be possible on this once you are done</span></li></ul><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-I175LstcTgc/Uf1eElp53wI/AAAAAAAABTs/GWhu5LKr-EA/s1600/20130803_152731.jpg><img alt="The extra Xtion screw" src=http://1.bp.blogspot.com/-I175LstcTgc/Uf1eElp53wI/AAAAAAAABTs/GWhu5LKr-EA/s400/20130803_152731.jpg width=400 height=300></a></td></tr></tbody></table><ul><li>Place your DSLR on the top using the quick release plate provided<br><span style=font-style:italic>You now have a camera rig!</span></li><li>Place the camera rig on a tripod<br><span style=font-style:italic>This is only required during calibration, you can take it off the tripod when you want to film</span></li></ul><p><strong>3. Set up the DSLR</strong><br>Get your DSLR ready to be a movie camera.</p><ul><li>Fire up the camera</li><li>Adjust the framerate<br><span style=font-style:italic>The RGBD guys recommend setting a lower framerate if possible. For me this means PAL, which is 25fps</span></li><li>Adjust the shutter speed<br><span style=font-style:italic>- A good rule of thumb is for the shutter speed to be double the framerate<br>- The framerate of PAL is 25fps, so 50 shutter speed<br>- The framerate of NTSC is 30fps, so 60 shutter speed</span></li><li>Adjust the exposure<br><span style=font-style:italic>- Since you have just fixed the shutter speed you will need to change the aperture and ISO<br>- Make sure the viewfinder is giving you an honest representation, or go by the exposure guage</span></li><li>Choose the MJPG for the compression format<br><span style=font-style:italic>- If you have a choice between Motion JPG (MJPEG) or AVCHD, choose Motion JPG. This will make scrubbing possible in the Visualizer when you get to that stage. If you can only shoot in AVCHD or some keyframe-based format, you will have to convert your footage to MJPG before using it in the Visualizer</span></li></ul><p><strong>4. Align the two views</strong><br>We need to make sure the depth sensor and the DSLR see the same thing.</p><ul><li>Plug the depth sensor's USB cable into your computer, and launch the Capture app<br><span style=font-style:italic>For me this is <code>RGBDCaptureXtionPro.app</code></span></li><li>Look at both the camera viewfinder and the depth sensor viewfinder (in the Capture app). Point the camera rig at something complex enough that you can make out the edges of each of the two viewframes</li><li>Compare the two viewframes<br><span style=font-style:italic>Are they well aligned?</span></li><li>Adjust the mount until they match<br><span style=font-style:italic>- To move left and right, pan the camera on the quick release plate, and tighten it when you are done<br>- To tilt the camera up and down, note that the top two screws on the RGBD mount allow for movement</span></li><li>If you are using a zoom lens (not recommended), choose a zoom level that you are comfortable with. Then tape it down and make sure it can never move again!</li><li>When you are happy with the alignment, firm up the remaining screws<br><span style=font-style:italic>Make sure they are decently firm. From this point forward, the DSLR cannot be allowed to move in relation to the depth sensor</span></li></ul><p>When things are tight, you are ready for calibration.</p><p><strong>5. Calibration</strong><br>The calibration stage is the longest and most difficult. However if you invest a little time to learn exactly what it is we are doing, it will pay off and become easier to do. Hence, this little warm-up section will sprinkle a little context before we dive into Step 1.</p><p>Broadly speaking, the calibration stage is about getting and storing hard data about our physical rig. What size and shape are our lenses? Where exactly are they in relation to each other? This information is important. When we render the 3D scene, the Visualize app will use this information pre-warp the DSLR images so that they correctly overlay the depth images.</p><p>So, in the calibration stage we have to feed enough information into the Capture app so that it can record and correctly play back in the Visualize app. When we feed the data in, the Capture app checks to make sure the data is good enough to use, and then stores it in our Working Directory.</p><p>There are three steps in the calibration stage:</p><ol type=a><li>Step 1: Depth sensor lens intrinsics<br><span style=font-style:italic>What is the size and shape of the depth sensor lens?</span></li><li>Step 2: DSLR lens intrinsics<br><span style=font-style:italic>What is the size and shape of the DSLR lens?</span></li><li>Step 3: Correspondence (lens extrinsics)<br><span style=font-style:italic>What is the exact position and orientation of the two lenses in relation to each other?</span></li></ol><p>As you can see above, Steps 1 and 2 are pretty much doing the same thing - gathering information about each of the two lenses, so-called intrinsics. The reason we need to gather this information is that each individual lens out there in the world is unique, with it's own slight peculiarities of curvature and shape.</p><p>But as we'll discover when we actually go through the Steps, 1 &amp; 2 don't <span style=font-style:italic>feel</span> the same. This is because the way we go about doing them is different. In Step 1, depth sensors figure out their own intrinsics, we just hit a button. But in Step 2, our DSLR can't do this. So we have to take pictures of a checkerboard and the software figures out the DSLR lens intrinsics from that.</p><p>Finally, in Step 3, we again have to use a checkerboard to figure out where the two lenses are in relation to each other. So Step 3 <span style=font-style:italic>feels</span> like Step 2, but really they are doing very different things.</p><p>Think of it this way. Look at the words 'intrinsics' and 'extrinsics'. Intrinsics means inside, internal - the internal properties of each lens. Extrinsics means outside, external to any lens - information not about the lenses themselves but about the real-world environment in which they exist (where the two lenses physically are in relation to each other).</p><p>Finally, note that there is a menu bar at the top of the Capture app.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-XsO-c_PZp_c/UgCFPA5nZtI/AAAAAAAABUg/Bhk6LcTaTBg/s1600/RGBDMenu.png><img alt="Where each Step is in the top-menu" src=http://1.bp.blogspot.com/-XsO-c_PZp_c/UgCFPA5nZtI/AAAAAAAABUg/Bhk6LcTaTBg/s500/RGBDMenu.png width=500 height=137></a></td></tr><tr><td class=tr-caption style="text-align: center">Where each Step is in the top-menu</td></tr></tbody></table><p>As we progress through each calibration Step, we will progress through the menu as illustrated. The first two Steps are under <code>Calibrate Lenses</code> and the final Step is under <code>Calibrate Correspondence</code>.</p><p><strong>5a. Calibration Step 1: Depth Sensor Intrinsics</strong><br>In this step, we tell the software about the characteristics of the depth sensor lens, so-called lens intrinsics. Luckily, your depth sensor has built in functionality which can gather this information for you.</p><ul><li>First, set your Working Directory<br><span style=font-style:italic>This is where your files will be stored going forward, including the intrinsics information, your movie files - everything, so now is the time to set it</span></li><li>Press <code>Self Calibrate Depth Camera</code> at the bottom of the left-hand pane<br><span style=font-style:italic>Aim your rig at an area of space which has plenty of 'visible information' - featuring different colors, contrasts and depths</span></li><li>It should say <code>Depth Camera Self Calibrated!</code> in green</li><li>The <code>Principal Point</code> should read as <code>320 x 240</code><br><span style=font-style:italic>This is the center of the <code>640 x 480</code> depth sensor video stream. If you get any other numbers, repeat the process</span></li></ul><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://3.bp.blogspot.com/-ZHlurBxkOpg/UgB-evByiTI/AAAAAAAABUQ/v4kYbl-MdIU/s1600/RGBDCap2.png><img alt="A successful depth sensor lens intrinsics calibration" src=http://3.bp.blogspot.com/-ZHlurBxkOpg/UgB-evByiTI/AAAAAAAABUQ/v4kYbl-MdIU/s500/RGBDCap2.png width=500 height=290></a></td></tr><tr><td class=tr-caption style="text-align: center">A successful depth sensor lens intrinsics calibration</td></tr></tbody></table><p>When the box under the left pane goes green, you can proceed to Step 2. Good - that was easy!</p><p><strong>5b. Calibration Step 2: DSLR Intrinsics</strong><br>It's not quite as easy to get the DSLR intrinsics. This is where you first need your checkerboard. We will put aside the Capture app for the moment and focus on your DSLR.</p><p>This stage can be frustrating at first, as you need to develop a sense of the right light and the right shots. I will try to be as clear as possible.</p><ul><li>Place your checkerboard on a stand so that it occupies about 1/4 of the frame<br><span style=font-style:italic>Make sure it doesn't move, or sway in the breeze</span></li><li>Move the camera rig so that the checkerboard is in the top left quadrant<br><span style=font-style:italic>- Don't worry if the checkerboard is not exactly horizontal or vertical<br>- Do ensure that the entire checkerboard is in the frame, including the white border around the outside black squares<br>- Make sure the checkerboard is well exposed and evenly lit</span></li><li>Focus the DSLR onto the checkerboard<br><span style=font-style:italic>The corners of the checkerboard should be crisp</span></li><li>Record 1-3 seconds video<br><span style=font-style:italic>Ensure the camera remains still after you have pressed it. Later, the software will extract the middle frame from the sequence, and it will expect to see no movement blur</span></li><li>Repeat the steps above, so that you cover the 4 quadrants<br><span style=font-style:italic>(top left, top right, bottom left, bottom right)</span></li><li>Repeat the steps above again, but this time move the checkerboard back so that you can take shots from 9 quadrants<br><span style=font-style:italic>(top left, top center, top right, mid left, mid center, mid right, bottom left, bottom center, bottom right)</span></li></ul><p>The shots should look a bit like this:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-98ZOD2fBL_0/Ufw5gMPAe4I/AAAAAAAABR8/b1DtqexSzFo/s1600/RGBDCalib1.gif><img alt="The checkerboard positions cover the frame" src=http://1.bp.blogspot.com/-98ZOD2fBL_0/Ufw5gMPAe4I/AAAAAAAABR8/b1DtqexSzFo/s500/RGBDCalib1.gif width=500 height=281></a></td></tr><tr><td class=tr-caption style="text-align: center">The checkerboard positions cover the frame</td></tr></tbody></table><p>You should now have a total of 13 videos of the checkerboard in different positions. The idea is to have thoroughly covered all areas of the frame.</p><p>Now that you have the videos recorded, you need to get them into the Capture app so that they can be analyzed for intrinsics information.</p><ul><li>Pull out your SD Card from your DSLR, or connect your DSLR's USB cable so that you can see the videos in Finder</li><li>Select all 13 videos and drag them from Finder into the right pane in the Capture app, where it says <code>Drag &amp; Drop an RGB Checkerboard folder or files</code></li><li>The software will hang for a few seconds, and then display a <code>Total Error</code><br><span style=font-style:italic>This is the average error across all the calibration images. Alongside this, you can view the individual error margins for each image by scrubbing the mouse from left to right across the calibration image window.</span></li><li>A <code>Total Error</code> of &lt; 0.200 is desirable<br><span style=font-style:italic>If your calibration has resulted in a larger average error than this, scrub through your image set and look for any outlier images which have an error of &gt; 0.300. Note the filename of any outliers. You can re-perform the analysis at any time, simply by dragging the videos onto the window pane again - this time excluding the erroneous clips. This should improve your <code>Total Error</code>.</span></li></ul><p>When the <code>Total Error</code> is low enough, the box under the right pane will go green, and you can proceed to Step 3.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-Ri5VckR7MHE/UgB8HKp-ROI/AAAAAAAABUA/rzbXmClfhEU/s1600/RGBDCap1.png><img alt="A successful DSLR lens intrinsics calibration" src=http://4.bp.blogspot.com/-Ri5VckR7MHE/UgB8HKp-ROI/AAAAAAAABUA/rzbXmClfhEU/s500/RGBDCap1.png width=500 height=290></a></td></tr><tr><td class=tr-caption style="text-align: center">A successful DSLR lens intrinsics calibration</td></tr></tbody></table><p>The good news is, you only have to do this once for each lens. Once you have correctly gathered the intrinsics information, it is sitting safely in your current Working Directory for reuse (you can copy &amp; paste it to a new Working Directory). Even though you may have to recalibrate the correspondence (Step 3) each time you take apart your rig, you will be able to reuse the information gathered in Steps 1 and 2.</p><p><strong>5c. Calibration Step 3: Correspondence</strong><br>When you get to this stage, you've done the hardest part. Now we are doing the coolest part. We are telling the software where the two lenses are in relation to each other, which is how the overlay effect is constructed.</p><ul><li>Click the <code>Calibrate Correspondence</code> top-menu item in the Capture app<br><span class=font-style:italic>You should see a screen which looks like this:</span></li></ul><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://2.bp.blogspot.com/-b5LJj_olq8Y/UgCKHOS1stI/AAAAAAAABVA/O17nTkhJ_Pk/s1600/RGBDCap4.png><img alt="The calibrate correspondence screen" src=http://2.bp.blogspot.com/-b5LJj_olq8Y/UgCKHOS1stI/AAAAAAAABVA/O17nTkhJ_Pk/s500/RGBDCap4.png width=500 height=290></a></td></tr></tbody></table><p>To calibrate correspondence, we need to match up three images:</p><ul><li>The RGB image from the DSLR</li><li>The infra-red image from the depth sensor</li><li>The computed depth map from the depth sensor</li></ul><p>If you are not sure about these terms, particularly 'infra-red image' and 'computed depth map', check out this quick explanation of <a href=//blog/how-depth-sensor-works-in-5-minutes>how depth sensors work</a>. It will help you going forward to get a sense of what the depth sensor is actually doing.</p><p>We can see in the Capture app that there are slots for each of these three images, and they are organized into columns so that we can take images of the checkerboard at different distances from the rig. Taking images at different depths ensures that the overlay is correct when the items we shoot move from the near range to the far range.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="The three columns" src=http://2.bp.blogspot.com/-qLtYxVl7uP8/Uf0nexva9vI/AAAAAAAABTY/GqefofXrbCo/s1600/RGBDCalib3.png width=423 height=367></td></tr><tr><td class=tr-caption style="text-align: center">The three columns</td></tr></tbody></table><p>As we start taking pictures going forward, the rows will start to appear. If we ever want to delete a row, just use the <code>[x]</code> on it's right-hand side.</p><p>Before you start, make sure there are no other depth sensors or infra-red devices pointing in the direction of your rig.</p><ul><li>Place the checkerboard so that it fills up a large part of the frame</li><li>Focus your DSLR onto the checkerboard</li><li>Click the depth map column<br><span style=font-style:italic>There is a brief pause while the picture is taken</span></li><li>Place a piece of tissue over the infra-red projector on the depth sensor<br><span style=font-style:italic>The tissue diffuses the infra-red dots so that the software can make out the edges on the checkerboard</span></li></ul><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://3.bp.blogspot.com/-dCX9eiQ8oes/UgCQVlvMjkI/AAAAAAAABVY/wLYSinQ34zI/s1600/RGBDDiffuse.png><img alt="Dots should appear over the checkerboard" src=http://3.bp.blogspot.com/-dCX9eiQ8oes/UgCQVlvMjkI/AAAAAAAABVY/wLYSinQ34zI/s500/RGBDDiffuse.png width=500 height=261></a></td></tr></tbody></table><ul><li>Red dots should appear over the checkerboard<br><span style=font-style:italic>- If they don't, you may have an issue with glare, try re-angling the checkerboard<br>- It may also be that the tissue paper you have used is too thick or the light in the room not bright enough. Try variations.</span></li></ul><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-yYEtHXY5sRk/UgCQTOxnHqI/AAAAAAAABVQ/c4YZDhnqivI/s1600/RGBDDots.png><img alt="Dots should appear over the checkerboard" src=http://1.bp.blogspot.com/-yYEtHXY5sRk/UgCQTOxnHqI/AAAAAAAABVQ/c4YZDhnqivI/s500/RGBDDots.png width=500 height=208></a></td></tr></tbody></table><ul><li>While the red dots are showing, click the infra-red camera column<br><span style=font-style:italic>There is a brief pause while the picture is taken. A new row will appear in the software, and the DSLR column will remain blank. This is normal, later we will drag the captured DSLR shots into these empty spaces</span></li><li>Remove the tissue</li><li>Take a 1-3 second video with your DSLR<br><span style=font-style:italic>Don't forget this!</span></li></ul><p>Repeat the above process three more times, each time stepping the checkerboard back a little, so that you cover different depths.</p><p>Once you have four sets:</p><ul><li>Get the video files off your camera and drag them one-by-one into the corresponding empty slots in the software, so that all slots in all columns are now filled</li><li>Cross your fingers</li><li>Click <code>Regenerate RGB/Depth Correspondence</code></li></ul><p>If the calibration fails, you can try <code>IGNORE</code>ing one or two of the sets, or just try running again with all four (there is a little bit of randomness in the algorithm). If you can't get it to calibrate, you will need to improve the quality of your setup and try again.</p><p>If the calibration succeeds, the bottom of the right pane will turn green, and an image of your checkerboard will appear on the right. Using the keys A, S, D, W and the mouse you can navigate around the picture and see where the Capture app thinks the dots should be. The dots are color-coded against the colored frames of the rows on the left.</p><p>If you have a good set of dots in the places where you would expect them to be, congratulations! You have a good calibration.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-o_GKF3zJ_ys/UgCVL-4SQFI/AAAAAAAABVo/j860MuWdnp0/s1600/RGBDCap5.png><img alt="A successfully calibrated RGBDToolkit rig" src=http://1.bp.blogspot.com/-o_GKF3zJ_ys/UgCVL-4SQFI/AAAAAAAABVo/j860MuWdnp0/s500/RGBDCap5.png width=500 height=290></a></td></tr><tr><td class=tr-caption style="text-align: center">A successfully calibrated RGBDToolkit rig</td></tr></tbody></table><p>If your screen looks like something like this, you are now ready for the production phase - you are ready to <a href=https://vimeo.com/44240337>record</a>!</p></div><a class=read-more href=/blog/rgbdtoolkit-calibration-tutorial>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/how-depth-sensor-works-in-5-minutes>How a Depth Sensor Works - in 5 Minutes</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-08-06>Tuesday 6 August, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=How%20a%20Depth%20Sensor%20Works%20-%20in%205%20Minutes&amp;url=http://jahya.net/blog/how-depth-sensor-works-in-5-minutes&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=How%20a%20Depth%20Sensor%20Works%20-%20in%205%20Minutes&amp;u=http://jahya.net/blog/how-depth-sensor-works-in-5-minutes"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=How%20a%20Depth%20Sensor%20Works%20-%20in%205%20Minutes&amp;u=http://jahya.net/blog/how-depth-sensor-works-in-5-minutes"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>How does a Kinect, or Asus Xtion work? Take a look at the front:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://2.bp.blogspot.com/-k7oVxPflMco/Uf0aQcQ1jSI/AAAAAAAABSg/KWWBtTNnq80/s1600/RGBDXtion3.png><img alt="The elements of a depth sensor" src=http://2.bp.blogspot.com/-k7oVxPflMco/Uf0aQcQ1jSI/AAAAAAAABSg/KWWBtTNnq80/s500/RGBDXtion3.png width=500 height=231></a></td></tr><tr><td class=tr-caption style="text-align: center">The elements of a depth sensor</td></tr></tbody></table><p>Some depth sensors have an RGB (Red Green Blue) camera, some don't. But that's fairly unremarkable and let's ignore it for now.</p><p>There are two other elements which must always be present for depth sense: An IR (Infra-Red) projector, and an IR camera.</p><p>The IR projector projects a pattern of IR light which falls on objects around it like a sea of dots. We can't see the dots because the light is projected in the Infra-Red color range:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-QkGEsyZVRrU/Uf0TufF07LI/AAAAAAAABSQ/NtnT06hO3E4/s1600/RGBDDots.png><img alt="Depth sensors work by projecting a pattern of dots in infra-red" src=http://4.bp.blogspot.com/-QkGEsyZVRrU/Uf0TufF07LI/AAAAAAAABSQ/NtnT06hO3E4/s500/RGBDDots.png width=500 height=273></a></td></tr><tr><td class=tr-caption style="text-align: center">Depth sensors work by projecting a pattern of dots in infra-red<br>Credit (and more info): <a href=http://graphics.stanford.edu/~mdfisher/Kinect.html>Matthew Fisher</a></td></tr></tbody></table><p>But the IR camera <span style=font-style:italic>can</span> see the dots. An IR camera is essentially the same as a regular RGB camera except that the images it captures are in the Infra-Red color range. So nothing too fancy going on there, still no actual depth sense.</p><p>The camera sends it's video feed of this distorted dot pattern into the depth sensor's processor, and the processor <span style=font-style:italic>works out</span> depth from the displacement of the dots. On near objects the pattern is spread out, on far objects the pattern is dense.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="The sensor internally builds a depth map" src=http://3.bp.blogspot.com/-dMUX5KyMCWY/Uf0kNnPn8-I/AAAAAAAABTI/u-cqcPRWHkg/s1600/RGBDepthMap.jpg width=450 height=340></td></tr><tr><td class=tr-caption style="text-align: center">The sensor internally builds a depth map</td></tr></tbody></table><p>This 'worked out' depth map can be read from the depth sensor into your computer, or you can just take the feed directly from the IR camera, it's up to you. When calibrating the <a href=//blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit</a>, during the correspondence calibration phase we must take a feed from both the depth map and the IR camera feed.</p></div><a class=read-more href=/blog/how-depth-sensor-works-in-5-minutes>Read more &gt;</a><hr class=separator></article><ul id=paging class=cta><li><a href="/blog/page2/">&lt; Previous page</a></li><li><a href="/blog/page4/">Next page &gt;</a></li></ul></main><aside class=right><nav><header><h1>All posts</h1></header><h2 id=2015-ref>2015</h2><ul><li><a href=/blog/new-site-for-hardware-hack-lab>New Site for Hardware Hack Lab</a></li></ul><h2 id=2014-ref>2014</h2><ul><li><a href=/blog/sound-control-at-future-interfaces>Sound Control at Future Interfaces</a></li><li><a href=/blog/projection-masking-not-projection>Projection Masking, not Projection Mapping</a></li><li><a href=/blog/addon-for-openframeworks-kinect-v2-and>Addon for openFrameworks, Kinect V2 and Mac</a></li><li><a href=/blog/john-cleese-on-creativity>John Cleese on Creativity</a></li><li><a href=/blog/takeaways-from-eyeo-2014>Takeaways from Eyeo 2014</a></li><li><a href=/blog/hardware-hacker-culture-of-new-york>Hardware Hacker Culture of New York</a></li><li><a href=/blog/future-visions-for-human-interaction>Future Visions for Human Interaction</a></li><li><a href=/blog/openbci-nears-its-kickstarter-goal>OpenBCI Nears it's Kickstarter Goal</a></li></ul><h2 id=2013-ref>2013</h2><ul><li><a href=/blog/sound-chamber-2013>Sound Chamber (2013)</a></li><li><a href=/blog/openbci-hackathon-at-thoughtworks>OpenBCI Hackathon at ThoughtWorks</a></li><li><a href=/blog/exploring-depth-video-at-culturehub>Exploring Depth Video at CultureHub</a></li><li><a href=/blog/introduction-to-ibeacons>Introduction to iBeacons</a></li><li><a href=/blog/volumetric-lab-at-culturehub-nyc>Volumetric Lab at CultureHub NYC</a></li><li><a href=/blog/randomness-in-algorithm>Randomness in the Algorithm</a></li><li><a href=/blog/study-existential>Video: Existential</a></li><li><a href=/blog/the-visual-art-of-brian-eno>The Visual Art of Brian Eno</a></li><li><a href=/blog/rgbdtoolkit-sketch-at-sampler>RGBDToolkit Sketch at The Sampler</a></li><li><a href=/blog/the-artist-geek-hybrid>The Artist-Geek Hybrid</a></li><li><a href=/blog/rgbdtoolkit-calibration-tutorial>RGBDToolkit Calibration Tutorial</a></li><li><a href=/blog/how-depth-sensor-works-in-5-minutes>How a Depth Sensor Works - in 5 Minutes</a></li><li><a href=/blog/focal-lengths-and-camera-sensors>Focal Lengths and Camera Sensors</a></li><li><a href=/blog/rgbdtoolkit-visualizer-tutorial>RGBDToolkit Visualizer Tutorial</a></li><li><a href=/blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit Workshop at Eyebeam</a></li><li><a href=/blog/nosql-distilled-to-keynote>NoSQL Distilled to a Keynote!</a></li><li><a href=/blog/paper-prototyping>Paper Prototyping</a></li><li><a href=/blog/git-vs-github>Is Git the Same Thing as Github!?</a></li></ul><h2 id=2012-ref>2012</h2><ul><li><a href=/blog/counterpoint-to-remote>Counterpoint to the Remote</a></li><li><a href=/blog/the-cascading-process>The Cascading Process</a></li><li><a href=/blog/working-with-total-space>Working with Total Space</a></li><li><a href=/blog/residency-begins-at-cac-troy>Residency Begins at CAC Troy</a></li><li><a href=/blog/installation-sketch-at-open-studios>Installation Sketch at Open Studios</a></li><li><a href=/blog/roman-moshenskys-mirror-world>Roman Moshensky's Mirror World</a></li><li><a href=/blog/open-studios-at-i-park>Open Studios at I-Park</a></li><li><a href=/blog/perception-as-creative-process>Perception as a Creative Process</a></li><li><a href=/blog/the-i-park-graveyard>The I-Park Graveyard</a></li><li><a href=/blog/scoping-out-land>Scoping Out the Land</a></li><li><a href=/blog/residency-begins-at-i-park>Residency Begins at I-Park</a></li><li><a href=/blog/residency-at-contemporary-artists-center>Residency at Contemporary Artists Center</a></li><li><a href=/blog/the-shopping-list-for-projection-bombing>First Shopping List for Projection-Bombing</a></li><li><a href=/blog/portable-projection-in-rural-context>Portable Projection in a Rural Context</a></li><li><a href=/blog/stephen-lumentas-sc-textmate-bundle>Stephen Lumenta's SC TextMate Bundle</a></li><li><a href=/blog/adding-openframeworks-addons>Adding OF Addons (ofxSuperCollider)</a></li><li><a href=/blog/setting-up-supercollider-with-textmate>Setting up SuperCollider with TextMate</a></li><li><a href=/blog/switching-to-macbook-pro>Switching to MacBook Pro</a></li><li><a href=/blog/quickref-for-supercollider>QuickRef for SuperCollider</a></li><li><a href=/blog/getting-started-with-supercollider>Getting Started with SuperCollider</a></li><li><a href=/blog/getting-started-with-openframeworks-in>Getting Started with OpenFrameworks</a></li><li><a href=/blog/overtones-harmonics-and-additive>Overtones, Harmonics and Additive Synthesis</a></li><li><a href=/blog/visit-to-cold-spring>Visit to Cold Spring</a></li><li><a href=/blog/i-park-residency>Residency at I-Park</a></li><li><a href=/blog/light-waves>Light Waves</a></li></ul><h2 id=2011-ref>2011</h2><ul><li><a href=/blog/final-exhibition>The Final Exhibition</a></li><li><a href=/blog/playing-with-particles>Playing with Particles</a></li><li><a href=/blog/responsive-granular-sound>Responsive Granular Sound</a></li><li><a href=/blog/kinecting-to-network>Kinecting to the Network</a></li><li><a href=/blog/first-working-day>First Working Day</a></li><li><a href=/blog/designs-for-freemote>Designs for Freemote</a></li><li><a href=/blog/freemote-utrecht>Freemote Utrecht</a></li><li><a href=/blog/untitled-picture-this-2011>Untitled - Picture This (2011)</a></li><li><a href=/blog/wider-context>The Wider Context?</a></li><li><a href=/blog/trading-time-for-space>Trading Time for Space</a></li><li><a href=/blog/talk-at-goldsmiths-digital-studios>Talk at Goldsmiths Digital Studios</a></li><li><a href=/blog/intro-to-marius-watz>Intro to Marius Watz</a></li><li><a href=/blog/practical-guide-to-generative-art>Practical Guide to Generative Art</a></li><li><a href=/blog/installation-at-alpha-ville>Installation at Alpha-Ville</a></li><li><a href=/blog/simple-harmonic-motion>Simple Harmonic Motion</a></li><li><a href=/blog/jaaga-journal-features>Jaaga Journal Features</a></li><li><a href=/blog/gravity-2011>Gravity (2011)</a></li><li><a href=/blog/reflections-2011>Reflections (2011)</a></li><li><a href=/blog/jaaga-sound-lights>Jaaga Sound & Lights</a></li><li><a href=/blog/two-works-for-jaaga-gravity-and-memory>Two Works for Jaaga: Gravity and Reflections</a></li><li><a href=/blog/cosm-collision-detection-and-volume>Cosm, Collision Detection and Volume</a></li><li><a href=/blog/vector-base-amplitude-panning>Vector-Base Amplitude Panning</a></li><li><a href=/blog/intuition-and-direction-of-project>Intuition, and Direction of the Project</a></li><li><a href=/blog/reflections-what-is-jaaga>Reflections: What is Jaaga?</a></li><li><a href=/blog/going-further-with-ambisonics>Going Further with Ambisonics</a></li><li><a href=/blog/introduction-to-ambisonics>Introduction to Ambisonics</a></li><li><a href=/blog/surface-light-sound-installation>Surface (2010)</a></li><li><a href=/blog/running-servo-motor>Servo Motors and Transistors</a></li><li><a href=/blog/spinning-12v-dc-motor>Spinning a 12V DC Motor</a></li><li><a href=/blog/spinning-dc-motor>Spinning a 5V DC Motor</a></li><li><a href=/blog/first-week-at-jaaga>First Week at Jaaga</a></li><li><a href=/blog/presentation-style>Presentation Style</a></li><li><a href=/blog/beginning-jaaga-fellowship>Beginning the Jaaga Fellowship</a></li><li><a href=/blog/brian-eno-role-models-and-direction>Brian Eno, Role Models and Direction</a></li><li><a href=/blog/hype-cycle_17>The Hype Cycle</a></li></ul><h2 id=2010-ref>2010</h2><ul><li><a href=/blog/working-with-3d-space>Working with 3D Space</a></li><li><a href=/blog/technology-and-luck>Technology and Luck</a></li><li><a href=/blog/gallery-types-and-commercial-gallery>Different Types of Gallery</a></li><li><a href=/blog/jason-bruges-studio>Jason Bruges Studio</a></li><li><a href=/blog/unstable-empathy-trust-and>Unstable Empathy & Gaining Trust</a></li><li><a href=/blog/chris-o-two-creative-cultures>Chris O'Shea & Two Creative Cultures</a></li></ul></nav></aside></div><script src=/javascript/script.e2be.js></script></body></html>