<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1.0"><meta name=author content="Andrew McWilliams"><title>Blog - Andrew McWilliams</title><link href=http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css rel=stylesheet><link href="http://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic" rel=stylesheet type=text/css><link rel=stylesheet href=/stylesheets/style.83cf.css><!--[if lt IE 9]>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv-printshiv.min.js"></script>
    <![endif]--><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.4f09.png><link rel="shortcut icon" href=/images/favicon.6537.png></head><body id=blog class=content><header role=banner><h1><a href="/"><span class=shift>Andrew</span> M<span class=sub>c</span>Williams</a></h1><nav role=navigation><h2>Navigation</h2><a id=menu-button href=#><i class="fa fa-bars"></i> <i class="fa fa-times"></i></a><ul class=cta><li><a href="/about/">About</a></li><li><a href="/works/">Works</a></li><li><a href="/blog/">Blog</a></li></ul></nav></header><div class=flex-row><main class=left><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/focal-lengths-and-camera-sensors>Focal Lengths and Camera Sensors</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-07-28>Sunday 28 July, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Focal%20Lengths%20and%20Camera%20Sensors&amp;url=http://jahya.net/blog/focal-lengths-and-camera-sensors&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Focal%20Lengths%20and%20Camera%20Sensors&amp;u=http://jahya.net/blog/focal-lengths-and-camera-sensors"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Focal%20Lengths%20and%20Camera%20Sensors&amp;u=http://jahya.net/blog/focal-lengths-and-camera-sensors"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>This post explains some of the hidden secrets of camera lenses. It will help you compare lenses against one another, explaining focal length and the <span style=font-style:italic>35mm equivalent</span> (more on that later). It's very important to understand this concept given a camera market in which there are so many different sensor sizes.</p><p>Note that this post is based on research I did for working with <a href=//blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit</a> - but anyway it applies to all cameras. At the end of this post I'll double back and explain how this helps us choose a lens for use specifically with the RGBDToolkit.</p><p><strong>Narrow vs. Wide</strong><br>We've all see images like the one below (taken from Vimeo's <a href=https://vimeo.com/videoschool/lesson/114/behind-the-glass-part-1-an-introduction-to-lenses>Behind the Glass series</a>), showing the effect that different 'focal lengths' have on the crop of the image we see:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=https://vimeo.com/videoschool/lesson/114/behind-the-glass-part-1-an-introduction-to-lenses><img alt="Focal lengths comparison" src=http://4.bp.blogspot.com/-g3p4iA0jlm0/UfUeSv1HArI/AAAAAAAABPg/CjhqZ7iTr8o/s500/Camera4.jpg width=500 height=281></a></td></tr><tr><td class=tr-caption style="text-align: center">Focal lengths comparison</td></tr></tbody></table><p>Using different focal lengths, we can zoom right in and get great detailed shots of faces (200mm), or take a really wide angle shot that captures the entire scene (16mm). For reference, the human eye focal length is about the equivalent of the 50mm focal length in the picture above.</p><p>(Ofcourse, this image doesn't mean that zoom lenses are best - you will <a href=http://digital-photography-school.com/prime-vs-zoom-lenses-which-are-best>always get the best quality</a> with lenses made specifically for a given Focal Length).</p><p>Here's a quick explanation of some common Focal Lengths and their general uses (again, from <a href=https://vimeo.com/videoschool/lesson/114/behind-the-glass-part-1-an-introduction-to-lenses>Behind the Glass</a>):</p><ul><li style=color:gray><strong>16mm</strong><br>"An ultra wide lens, this bad boy distorts heavily, emphasizing objects in the foreground by making them look a lot larger than the background. Dynamic, but use with caution!"</li><li style=color:gray><strong>28mm</strong><br>"Standard for documentary and photojournalism to shoot cowboy shots, otherwise known as medium shots."</li><li style=color:gray><strong>35mm</strong><br>"Another standard for documentary filming, also tight enough to shoot portraits."</li><li style=color:gray><strong>50mm</strong><br>"Standard for cinema/video, it approximates the human eye's typical focal length."</li><li style=color:gray><strong>85mm</strong><br>"A popular portrait, or 'beauty' lens. Capable of making everyone look lovely!"</li><li style=color:gray><strong>200mm</strong><br>"The top of the scale for most people, this is a telephoto lens. Their inherent shallow depth of field makes them useful in eliminating unwanted foreground and background objects by simply throwing them out of focus. Great for sports photography!"</li></ul><p>And that's great - but what really <span style=font-style:italic>is</span> Focal Length? What do these numbers mean? What does 200mm and 16mm refer to?</p><p><strong>What is Focal Length?</strong><br>It should start to become clear if you take a look at the image below. We can see that focal length is the distance between the lens and the Camera Sensor.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="What is focal length?" src=http://1.bp.blogspot.com/-phSfibMAiBs/UfUeSaVV-DI/AAAAAAAABPc/0rthwTiZlj0/s470/Camera3.gif width=470 height=350></td></tr><tr><td class=tr-caption style="text-align: center">What is focal length?</td></tr></tbody></table><p>We can imagine that when the lens is further away from the Camera Sensor (say, 200mm) the picture angle and therefore Field of View (FOV) becomes narrower - a narrow angle lens. In the picture above, the bird's wings would be cropped off and we'd have a tight close-up.</p><p>We can equally imagine that if the lens is closer to the Camera Sensor (say, 16mm) the FOV will be wider and the the picture will be of a smaller bird with much more sky - a wide angle lens.</p><p>So what does the 35mm mentioned at the top of this post refer to?</p><p><strong>Everything revolves around 35mm Camera Sensors</strong><br>There is a different measurement in millimeters which does not describe the Focal Length. Instead, when we say '35mm sensor' we are describing the size of the Camera Sensor (look back at the diagram above).</p><p>It's a hangover from the days of film. The standard, canonical size of Camera Sensor is based on the standard, canonical size of film - 35mm.</p><p>Now, a 50mm Focal Length has a certain angle of view associated with it - as illustrated in the first picture at the top of this blog post - and a 200mm Focal Length also has a certain view angle associated with it. But when we use the numbers 50mm and 200mm to describe an <span style=font-style:italic>angle of view</span>, we do so <span style=font-style:italic>assuming</span> a 35mm Camera Sensor.</p><p>In fact all of the angles shown in the image at the top of this post are labelled with the commonly used language of Focal Lengths - 50mm, 200mm etc, and this language is ubiquitous. But the actual millimeter Focal Lengths used to achieve those shots depend on also using a 35mm Camera Sensor.</p><p><strong>For example</strong><br>So what happens if we use a smaller sensor? With the same lens?</p><p>Let's imagine we use exactly the same 50mm lens on both a 35mm sensor, and then on a sensor half that size (17.5mm).</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Using a smaller camera sensor" src=http://2.bp.blogspot.com/-3vAsnVMvJWQ/UfUeSe0z5CI/AAAAAAAABPw/GFzsIRymgbk/s500/Camera1.png width=500 height=370></td></tr><tr><td class=tr-caption style="text-align: center">Using a smaller camera sensor</td></tr></tbody></table><p>The net effect is something like this - the smaller sensor has a narrower FOV for the same Focal Length:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Using a smaller camera sensor" src=http://3.bp.blogspot.com/-7Zx8ZLbKqRM/UfUeScLYduI/AAAAAAAABPk/sw12oONW9PM/s260/Camera2.JPG width=260 height=260></td></tr><tr><td class=tr-caption style="text-align: center">Using a smaller camera sensor</td></tr></tbody></table><p>To achieve the same FOV, you need to shrink down the entire lens, Focal Length and all, by half:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="A smaller lens for a smaller sensor" src=http://2.bp.blogspot.com/-zce9g3Coft8/UfWIVF2DWkI/AAAAAAAABQU/oNi_Q0C6YQU/s500/Camera7.png width=500 height=154></td></tr><tr><td class=tr-caption style="text-align: center">A smaller lens for a smaller sensor</td></tr></tbody></table><p>In fact, this is one of the selling points of the standards described below. The smaller sensors mean you can have smaller, lighter and less imposing-looking camera lenses.</p><p><strong>Common standards</strong><br>Three common standard size formats have grown up for three different sensor sizes:</p><ul><li>Micro Four-Thirds (2 x focal length)</li><li>APS (1.5 x focal length) - most mirrorless DSLRs</li><li>Full Frame - same size as a 35mm camera</li></ul><p>The Micro Four Thirds standard is exactly half the size of the Full Frame (35mm), and so whatever focal length you have for a Micro Four Thirds lens, you can double it to get the 35mm equivalent.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Full Frame, APS and Micro Four Thirds sensors" src=http://1.bp.blogspot.com/-PeLZIZ2eZ3Y/UfWCTFxmGmI/AAAAAAAABQI/1Lrq2tA2_xQ/s493/Camera5.jpg width=493 height=370></td></tr><tr><td class=tr-caption style="text-align: center">Full Frame, APS and Micro Four Thirds sensors (*)</td></tr></tbody></table><p>The APS standard is midway between the Micro Four Thirds and the Full Frame, and so whatever focal length you have for an APS lens, you can times it by 1.5 to get the 35mm equivalent.</p><p>For example, the angle of a 14mm Micro Four Thirds is roughly equivalent to a Full Frame 28mm.</p><p>So the next time someone tells you they have a 50mm lens, ask them what size sensor they have and scale up before building a picture in your mind of the FOV it will produce!</p><p><strong>Which lens for the RGBDToolkit?</strong><br>When finding a lens to use with the RGBDToolkit, we are looking to find a lens with a similar, but if anything slightly wider FOV. The makers of RGBD <a href=http://www.rgbdtoolkit.com/tutorials.html>recommend a ~24mm lens</a> on a Full Frame sensor, so a 12mm lens on a Micro Four Thirds. They also recommend a ~16mm lens for an APS-C sensor like an entry-level Canon DSLR.</p><p>So it's pretty simple really, but now you have some context as to why those settings are appropriate relative to each other, and some sense of the FOV you can expect at the end. And as the RGBDToolkit tutorial says: "When in doubt, set your zoom lens to its widest setting. Use a wide prime lens for best results."</p><hr><p>(*) -- I know this isn't really a Micro Four Thirds sensor on the right of the image. It's a slightly smaller sensor. But it's a really nice image, and the article just flows a little better if we just pretend it is, don't you think?</p></div><a class=read-more href=/blog/focal-lengths-and-camera-sensors>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/rgbdtoolkit-visualizer-tutorial>RGBDToolkit Visualizer Tutorial</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-07-25>Thursday 25 July, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=RGBDToolkit%20Visualizer%20Tutorial&amp;url=http://jahya.net/blog/rgbdtoolkit-visualizer-tutorial&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=RGBDToolkit%20Visualizer%20Tutorial&amp;u=http://jahya.net/blog/rgbdtoolkit-visualizer-tutorial"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=RGBDToolkit%20Visualizer%20Tutorial&amp;u=http://jahya.net/blog/rgbdtoolkit-visualizer-tutorial"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>This is a video of James George showing us how to us to use the RGBDToolkit visualizer. I recorded it during <a href=//blog/rgbdtoolkit-workshop-at-eyebeam>last week's workshop at Eyebeam</a>:</p><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/70924774?byline=0&amp;portrait=0"></iframe></div><p>In the video, James is demoing a new pre-release of the visualizer software, v006. You can check for the <a href="http://www.rgbdtoolkit.com/downloads/">latest releases here</a>, or download the exact software <a href=http://www.rgbdtoolkit.com/downloads/RGBDVisualize_006_preRelease.zip>we are seeing in the video here</a>.</p><p>A <a href=http://www.jahya.net/dls/docs/RGBDToolkit-visualizer-tutorial-transcript.pdf>full transcript of this recording</a> was taken down in realtime by Ellen Pearlman. The transcript contains a little extra that didn't make it into the video, on exporting from the visualizer.</p><p>Note that <a href=https://vimeo.com/album/1977644>there are older tutorials here</a> - one of those is a visualizer tutorial. It may be a little out of date (it is an older version of the software), but may also contain some useful information.</p></div><a class=read-more href=/blog/rgbdtoolkit-visualizer-tutorial>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit Workshop at Eyebeam</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-07-24>Wednesday 24 July, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=RGBDToolkit%20Workshop%20at%20Eyebeam&amp;url=http://jahya.net/blog/rgbdtoolkit-workshop-at-eyebeam&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=RGBDToolkit%20Workshop%20at%20Eyebeam&amp;u=http://jahya.net/blog/rgbdtoolkit-workshop-at-eyebeam"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=RGBDToolkit%20Workshop%20at%20Eyebeam&amp;u=http://jahya.net/blog/rgbdtoolkit-workshop-at-eyebeam"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>Last weekend I attended an <a href="http://www.rgbdtoolkit.com/">RGBDToolkit</a> workshop at <a href="http://www.eyebeam.org/">Eyebeam</a> NYC. I'm posting about it now just because it reminded me how much I enjoy hacking hardware, playing with digital media and exploring new technologies.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="RGBDToolkit workshop at Eyebeam" src=http://1.bp.blogspot.com/-szNOXqOKnfQ/Ue9b9ZTdWUI/AAAAAAAABO0/pzkJfxR6llM/s500/20130714_130200.jpg width=500 height=375></td></tr><tr><td class=tr-caption style="text-align: center">RGBDToolkit workshop at Eyebeam</td></tr></tbody></table><p>The Toolkit is about combining HD video from DSLRs with depth data from <a href="http://www.primesense.com/">PrimeSense</a>-based sensors such as Xtion or Kinect - effectively an early technique for creating 3D cinema. Due to the implementation details there is a particular aesthetic associated with RGBD, but that is true of all media and I'm looking forward to pushing against this aesthetic.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/39505902?byline=0&amp;portrait=0"></iframe></div></td></tr><tr><td class=tr-caption style="text-align: center">What is RGBDToolkit?</td></tr></tbody></table><p>At this stage I just want to note the impact of the workshop. It was great to learn the technique, but more broadly it's re-engaging the artist-geek hybrid within me. And to say thanks to the organizers <a href="http://jamesgeorge.org/">James George</a> and <a href="http://www.alexanderporter.net/">Alexander Porter</a>, who put together a really good, informative, creative workshop experience.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="A DSLR bound to a Kinect with an RGBDToolkit mount" src=http://2.bp.blogspot.com/-4f9VgzQkfqA/Ue9dJ8coRwI/AAAAAAAABPE/psxEjtInV1E/s667/20130713_122838.jpg width=500 height=667></td></tr><tr><td class=tr-caption style="text-align: center">A DSLR bound to a Kinect with an RGBDToolkit mount</td></tr></tbody></table><p>I have some more material on this that I will come back and post in the next few days, as well as the seeds of a potential collaboration.</p></div><a class=read-more href=/blog/rgbdtoolkit-workshop-at-eyebeam>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/nosql-distilled-to-keynote>NoSQL Distilled to a Keynote!</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-06-17>Monday 17 June, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=NoSQL%20Distilled%20to%20a%20Keynote!&amp;url=http://jahya.net/blog/nosql-distilled-to-keynote&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=NoSQL%20Distilled%20to%20a%20Keynote!&amp;u=http://jahya.net/blog/nosql-distilled-to-keynote"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=NoSQL%20Distilled%20to%20a%20Keynote!&amp;u=http://jahya.net/blog/nosql-distilled-to-keynote"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>This presentation by Martin Fowler gives a broad overview of NoSQL. It's perfect for newcomers to NoSQL, and also for those seeking a perspective from which to view the myriad recent developments under the nebulous umbrella 'NoSQL':</p><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/66052102?byline=0&amp;portrait=0"></iframe></div><p>As always, I'll briefly summarize what I think are some of the salient points in the video, to make it easier to digest at a glance. For the purposes of this article, you will need to have a working knowledge of Relational SQL databases, and OO code.</p><p><strong>Object databases</strong><br>First, Martin places NoSQL in a DB popularity timeline:</p><ul><li>80's - rise of Relational</li><li>90's - critical success but <span style=font-style:italic>commercial</span> failure of <a href=http://en.wikipedia.org/wiki/Object_database>Object databases</a></li><li>00's - rise of NoSQL <span style=font-style:italic>alongside</span> Relational</li></ul><p>Object DBs make a huge amount of sense for OO application development. The Object DB disk storage paradigm matches the in-memory paradigm of hierarchical object relationships, making the <a href="http://impedancemismatch.com/">impedance mismatch</a> disappear. The security, transaction and querying features are comparable to Relational. So why the commercial failure?</p><p>Martin puts it down to the larger ecosystem - it's a common requirement that the new DB you are creating be accessible by existing reporting tools, or multiple other pre-existing applications. Different departments will want to access your data, and they are loaded with Relational/SQL experts.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Commonly, a DB will need to be directly accesible from multiple applications" src=http://4.bp.blogspot.com/-azsqFssHSio/Ub5LQ7XGNhI/AAAAAAAABNI/ArnYiaU9AvA/s500/nosql-impedence1.png width=500 height=281></td></tr><tr><td class=tr-caption style="text-align: center">Commonly, a DB will need to be directly accesible from multiple applications</td></tr></tbody></table><p>Object DBs use <a href=http://en.wikipedia.org/wiki/Object_Query_Language>much simpler SQL</a> for data access, and it is incompatible with the <code>JOIN</code>-heavy SQL used in Relational DBs.</p><p>In this scenario it makes sense to stick with a single DB paradigm - the existing, trusted, proven Relational - and just let the OO application developers deal with impedance mismatch via <a href=https://en.wikipedia.org/wiki/Object-relational_mapping>ORM</a>.</p><p><strong>Lots of data =&gt; distributed data =&gt; NoSQL</strong><br>Martin goes on to say that in the early 2000's when Google and Amazon were considering how to deal efficiently with massive scale, it became clear that there wasn't a computer big enough in the world to house all that data. The only option was to store it on multiple computers, and large distributed networks. <a href=http://en.wikipedia.org/wiki/BigTable>Bigtable</a> and <a href=http://en.wikipedia.org/wiki/Dynamo_(storage_system)>Dynamo</a> were born.</p><p>The term 'NoSQL' is almost an accidental term and doesn't really tell you anything about the characteristics of the technologies. Loosely clustered within this label are the following:</p><ul><li><strong>Non-relational</strong> - not all, but generally</li><li><strong>Open source</strong> - not all, but generally</li><li><strong>Cluster friendly</strong> - the initial driver, but now just one aspect</li><li><strong>21st Century web</strong> - are DBs with similar characteristics from pre-2000 part of this group? - no.</li><li><strong>Schema-less</strong> - again, not all, but generally. And note that in reality there is always a schema - it just means there is an implicit schema, rather than an explicit one.</li></ul><p>The label 'NoSQL' is terrible, because the characteristics described above don't lead you to it. NoSQL is just a catch-all, and when you look at the different things that people mean when they say it, you end up something like with the characteristics above.</p><p><strong>Commonly discussed types</strong><br>Here we have an outline of the most often-discussed types, and some sample implementations:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Common NoSQL implementations" src=http://1.bp.blogspot.com/-TEe81HD9tkQ/Ub5eLpEwBSI/AAAAAAAABNY/52SO5d04rAI/s500/nosql-ecosystem.png width=500 height=281></td></tr><tr><td class=tr-caption style="text-align: center">Common NoSQL implementations</td></tr></tbody></table><p>Things are missing here, and at the end one of the audience members asks about different types. Martin gives a very interesting answer illustrating what he thinks might happen with respect to the future of DBs. But for the purposes of this introductory presentation, these are the types we are discussing.</p><p><strong>Aggregate-oriented data models</strong><br>When using an ORM to access a traditional relational DB we are declaring that we want some <span style=font-style:italic>aggregate</span> of data. That aggregate may involve a lookup from some <code>CountryID</code> to the name of an actual country, or to all sorts of other data types in other tables, perhaps several Foreign Keys down the chain.</p><p>The concept of <span style=font-style:italic>aggregate</span> here is taken from Eric Evans' <a href=http://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215>Domain-Driven Design</a>. The insight added is that in Key-Value, Document and Column-family DBs, a key is used to access an aggregate of data. The given DB is 'aware' of the aggregate in a way that Relational DBs are just not aware, and therefore the aggregate can be distributed as a unit, improving performance over massive datasets.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="They all use keys" src=http://3.bp.blogspot.com/-gPGwyBPrYWY/Ub5kfkepB8I/AAAAAAAABNo/Eyl0mD0BMk8/s600/nosql-keys.png width=500 height=600></td></tr><tr><td class=tr-caption style="text-align: center">They all use keys to access an aggregate</td></tr></tbody></table><p>Note the trade-off - what if you don't want to access that data as an aggregate? What if you want to know the average age across all <code>Customer</code> records? Now you have to access loads of aggregates across loads of distributed nodes.</p><p>But where is the mission-critical part of your business? Is it in running a report like that? Or is it in quickly retrieving aggregated records?</p><p><strong>Graph DBs are different</strong><br>The next section illustrates how Graph DBs are different, and affirms the oddness of grouping all of these DBs under the term 'NoSQL'.</p><p>Check out this explanation if you are not familiar with the acronyms <a href="http://www.johndcook.com/blog/2009/07/06/brewer-cap-theorem-base/">ACID and BASE</a>. Martin points out that in many respects, Relational DBs are not ACID anyway. For example, transactions are not designed to remain open across entire internet browsing sessions where two people are considering purchasing the same item.</p><p>However, within the framework of ACID and BASE, the three Aggregate-oriented DBs are BASE where the Graph DB is ACID:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Graph databases are ACID" src=http://4.bp.blogspot.com/-awTaJlHeHVQ/Ub5t1J42qEI/AAAAAAAABN4/j6uNQzLtBxA/s500/nosql-base-acid.png width=500 height=281></td></tr><tr><td class=tr-caption style="text-align: center">Graph databases are ACID</td></tr></tbody></table><p>Part of that BASE acronym may be unfamiliar to you - what is Eventual Consistency? To get an idea, this article contains a great <a href="http://ksat.me/a-plain-english-introduction-to-cap-theorem/">Introduction to the CAP theorem, in Plain English</a>. Explanation by analogy is one of my favorites!</p><p><strong>Problems with CAP</strong><br>Understanding the CAP theorem is important, because it tells you about the trade-offs you are making when you consider a NoSQL DB.</p><p>However, it should be noted that for the vast majority of projects, CAP need not apply. Massive, distributed data may have been one of the initial drivers for NoSQL's existence, but that was just the 'crack'. Now there are all kinds of other reasons, for example the ease of development, the reduction of impedance mismatch, the alignment with particular types of data storage paradigms.</p><p>But unless, like the very few, you really do have to have massively distributed data, then CAP is meaningless. You can forego the Partition Tolerance and be happy with the Availability and Consistency.</p><strong>When to use (and NOT use) NoSQL</strong><br>There are two main drivers (and one more - the middle one below - is implied):<ul><li><strong>Large scale data</strong> - the thing that kicked off NoSQL in the first place</li><li><strong>The paradigm works</strong> - as in the case of Graph DBs, if the data storage paradigm better mirrors your purposes</li><li><strong>Easier development</strong> - simpler data access, just like Object DBs promised in the 90's</li></ul><p>For the mostpart now, the reason people are using NoSQL DBs is the last one - easier development. So again, in most cases, where you don't have large volumes of data, the CAP issue doesn't really apply. Which begs one more question...</p><p><strong>Why NoSQL now?</strong><br>So why is NoSQL taking off where Object DBs failed? Martin posits that the original blocker has faded due to a happy coincidence.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="RESTful Web services make all the difference" src=http://2.bp.blogspot.com/-W-LEPtVImwk/Ub5zrUpJPTI/AAAAAAAABOI/D4h6-lEMnnE/s320/nosql-webservices.png width=500 height=281></td></tr><tr><td class=tr-caption style="text-align: center">Graph databases are ACID</td></tr></tbody></table><p>The rise of the popularity of RESTful web services has meant that there is no longer a need for systems to integrate with databases directly. A datastore can be exposed via a standardized REST interface, abstracting the underlying storage implementation.</p><p>As ever, when I see an interesting video I sometimes like to summarize it for my own sake. All of the arguments put here derive from the video and I have included no original research. I just sometimes like to write up a few notes to look back on later, and share in case you find it useful too!</p></div><a class=read-more href=/blog/nosql-distilled-to-keynote>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/paper-prototyping>Paper Prototyping</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-05-20>Monday 20 May, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Paper%20Prototyping&amp;url=http://jahya.net/blog/paper-prototyping&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Paper%20Prototyping&amp;u=http://jahya.net/blog/paper-prototyping"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Paper%20Prototyping&amp;u=http://jahya.net/blog/paper-prototyping"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>What is the quickest way to try out a complicated tech idea on real people? How do you get real, valuable feedback from users without going to the expense of writing code?</p><div style="text-align: center"><iframe frameborder=0 height=375 width=500 src="http://player.vimeo.com/video/65234919?byline=0&amp;portrait=0"></iframe></div><p>Paper prototyping is an idea I've recently been introduced to at ThoughtWorks. You draft up the interface on pieces of paper, and have a human act as the computer would, swapping out bits of paper in response to a real user.</p><p><strong>Advantages and limitations</strong><br>Ben O'Hear explains the concept in the above video. As he describes, the advantages are:</p><ul><li>It costs far less than developing even basic software</li><li>You can get very early feedback, even before writing any code - and that feedback could have significant impact on what code you write (or don't write)</li><li>If a user does something unexpected, you can respond ad-hoc and get feedback on your improvised response</li><li>You can work collaboratively and iterate rapidly - i.e. a group of you can discuss the user's interactions and sketch out a whole new UI there and then, and test it out again on users</li></ul><p>Some of the disadvantages are:</p><ul><li>It's easy to test out big ideas, but hard to zero in on small details</li><li>You are constrained by location - you can't test remote responses to user actions. However, on this point, it seems to me that you could simulate some types of remote testing using Skype</li><li>You are constrained by paper - you can't have detailed real-time responsiveness, e.g. Kinect interactions, or sound synthesis. However, again you could simulate this using human voice or recorded sound</li><li>As in all usability testing, this can tell you whether users understand your app, and can use it - it doesn't tell you whether users <span style="font-style: italic">want</span> to use your app or are willing to <span style="font-style: italic">pay</span> for it</li></ul><p>This is a very <a href=http://en.wikipedia.org/wiki/Lean_Startup>lean startup</a> way of thinking, in which <a href=http://en.wikipedia.org/wiki/Validated_learning>learning about your users</a> is the preferred unit of measure - rather than completing features.</p></div><a class=read-more href=/blog/paper-prototyping>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/git-vs-github>Is Git the Same Thing as Github!?</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-05-14>Tuesday 14 May, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Is%20Git%20the%20Same%20Thing%20as%20Github!?&amp;url=http://jahya.net/blog/git-vs-github&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Is%20Git%20the%20Same%20Thing%20as%20Github!?&amp;u=http://jahya.net/blog/git-vs-github"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Is%20Git%20the%20Same%20Thing%20as%20Github!?&amp;u=http://jahya.net/blog/git-vs-github"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>I was trying to tell a friend the other day how Git and Github are related. Where does Git end and Github start? What do they each even do for you?</p><p>I realised that it's actually one of those fundamental things that is worth stepping through and getting a clear understanding. If you've been using the terms interchangeably, the distinction makes a good backdrop to learn more, and getting clarity will enable you to steer past a whole bunch of confusion later on.</p><p><strong>What is Git?</strong><br>Well, Git is not <a href="https://github.com/">Github</a>. Git is a piece of software that you install locally on your computer which handles 'version control' for you.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-WY2YpNr3W6g/UY6tZAc-H3I/AAAAAAAABLY/xJ9x3wIY8V8/s1600/Github2.png><img alt="Git is not Github" src=http://1.bp.blogspot.com/-WY2YpNr3W6g/UY6tZAc-H3I/AAAAAAAABLY/xJ9x3wIY8V8/s440/Github2.png width=440 height=208></a></td></tr></tbody></table><p>So to learn about Git, you have to learn about version control.</p><p><strong>What is Version Control?</strong><br>Let's say you have some new project, and you are planning to store all the files for that project in some new directory. You know that as time goes on, the files in this project will change - a lot. Things could get messy, and who knows when you might need to revert back to a previous working version of what you had?</p><p>So, you install Git on your computer. Then, you have Git create the new project directory for you. You also tell Git that you would like to keep a history of the changes you make within that directory.</p><p>Then, you add some files to kick off your project. The files you just added represent the first incremental step on the journey of your project. So you tell Git to take a snapshot.</p><p>Then you make a small change - your next incremental step. So you take another snapshot.</p><p>And that's about it for version control - make a small change, take a snapshot, make another small change, take a snapshot. You can then use Git to step back and forth whenever necessary through each snapshot (snapshot aka version) of your project directory. Hence, version control.</p><p>And Git is just one of the many version control systems out there, that you can download and install on your machine. Hence, Git.</p><p><strong>Collaborating with Git</strong><br>That's great for you as an individual. But what if you are working on a team, and you want to share your project directory? And you want to make changes on your machine, send those changes to your collaborators, and also have changes they make appear in your machine's project directory?</p><p>Git is a so called <span style="font-style: italic">distributed</span> version control system. All that means is that Git has commands that allow you to <code>push</code> and <code>pull</code> your changes to other people's machines:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://3.bp.blogspot.com/-BuqB89_c7Jo/UY62W0naffI/AAAAAAAABLw/vcssorOCelA/s1600/Github3.png><img alt="Collaborating using Git" src=http://3.bp.blogspot.com/-BuqB89_c7Jo/UY62W0naffI/AAAAAAAABLw/vcssorOCelA/s440/Github3.png width=440 height=186></a></td></tr></tbody></table><p>Neither copy of the project directory is any better or 'greater' than any other - you are both collaborating on identical copies. This is a good thing, and Git gives you the power to work on your own copy as-is until you are ready to <code>pull</code> in your collaborator's changes, and <code>push</code> back your own changes.</p><p>But unless you are working right next to each other every day, you can't be sure exactly when your collaborator will have their machine on and plugged into a network. Wouldn't it be great if there were a third identical copy you could both <code>push</code> and <code>pull</code> from?</p><p><strong>Collaborating with Git and GitHub</strong><br>Well, that's what Github is! At it's core, it's just a place to store your identical working directories - aka <span class="style: font-style:italic">repositories</span>, or <span class="style: font-style:italic">repo's</span> for short. That's the service that Github provides - it's literally a hub for Git repositories.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://3.bp.blogspot.com/-SnWr9oa-G30/UY6tZKwGZPI/AAAAAAAABLc/dyQGoX_i3E8/s1600/Github.png><img alt="Collaborating using Git and GitHub" src=http://3.bp.blogspot.com/-SnWr9oa-G30/UY6tZKwGZPI/AAAAAAAABLc/dyQGoX_i3E8/s500/Github.png width=500 height=339></a></td></tr></tbody></table><p>Github gives you a bunch more features, like a nice website to allow you to compare changes and administrate user accounts. But it's <a href="http://en.wiktionary.org/wiki/raison_d'%C3%AAtre">raison d'&#234;tre</a> is to host your repos, and to make it easier for you to <code>push</code> and <code>pull</code> from your collaborators.</p><p><strong>*Not* just a hosting service!!</strong><br>One common mistake people make is thinking that because Github repo's are public by default, that it is essentially just somewhere to host and share your code when it's done. This is one thing you can do, but if that's all you are doing, you are missing the real power of Git.</p><p>Where Git really excels is as a <em>collaboration tool</em>. A place for you to <a href=http://www.furtherfield.org/features/articles/diwo-do-it-others-%E2%80%93-no-ecology-without-social-ecology>Do It With Others</a>. If you are doing all your coding on your local machine and then just uploading it in one snapshot (aka commit) at the end, you are missing out on a huge amount of value.</p><p>Git allows you to snapshot/commit incrementally, after each little change you do. I regularly have 10 commits per day, and I or anyone can cycle back and forth through those snapshots any time we like. People can see how my thinking evolved - the early commits are experimental and the project has barely begun to address it's aims, and the later commits are more mature and the project is getting close.</p><p><strong>Commit early, commit often</strong><br>But the larger benefits of <a href=http://www.codinghorror.com/blog/2008/08/check-in-early-check-in-often.html>commit early/commit often</a> are that other people can see and comment on what you are doing. You are being <em>collaborative</em> and <em>open</em>, and the feedback, suggestions or help you get along the way might just alter the entire course of the project for the greater good. It might well save you a whole bunch of time, help you discover some previously unconsidered potential, or even identify an awesome collaborator who will help you drive your project forward.</p><p>Opening out your half-baked thoughts sounds scary to some, but we all go through those stages - and those are the times when feedback and engagement is most critical. If you don't want the world to see your project, you can always create a private repo and pull in collaborators by invitation only.</p><p><strong>Alternatives to Github</strong><br>Since Git and Github aren't really linked - Github is just another place to store identical repos - you could use any Git hosting service. One alternative is <a href="https://bitbucket.org/">Bitbucket</a>. This service gives you free private repos (unlike Github), in case you aren't ready to share your work with the world.</p><p>However Github is the most widely used Git hosting service, and has a broad community of users sharing code and interacting.</p><p><strong>How to Learn Git</strong><br>So in any case, the real challenge when you are starting out isn't learning Github, which is just an interchangeable service which allows you to host the thing of real value - your Git repository. Your attention is better spent learning Git.</p><p>The best way to learn Git in my opinion is this free online book: <a href=http://git-scm.com/book>git-scm.com/book</a>. It walks you through step by step and assumes no particular knowledge. There is an online, PDF and mobi version available, and it uses Github for hosting when you get to that stage.</p><p>There are a lot of topics to cover but for most users interacting on a fairly small scale, the first two chapters should suffice. You can pick up the harder stuff as and when necessary.</p><p>Another good place to go if you want to try out a few commands without going through the hassle of installing Git, is <a href=http://try.github.com>Try Git</a>. Expect some commercial ad tie-ins, and it won't answer your questions like the book does. But it does let you give things a try and learn by doing.</p><p>Good luck!</p></div><a class=read-more href=/blog/git-vs-github>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/counterpoint-to-remote>Counterpoint to the Remote</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2012-10-24>Wednesday 24 October, 2012</time><ul class=social><li><a href="http://twitter.com/share?text=Counterpoint%20to%20the%20Remote&amp;url=http://jahya.net/blog/counterpoint-to-remote&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Counterpoint%20to%20the%20Remote&amp;u=http://jahya.net/blog/counterpoint-to-remote"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Counterpoint%20to%20the%20Remote&amp;u=http://jahya.net/blog/counterpoint-to-remote"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>In my recent work, I have been exploring the <a href=http://jahyawords.blogspot.co.uk/2012/09/perception-as-creative-process.html>creativity inherent in perception</a>, and the sense of value we seamlessly inject into our everyday experience.</p><p>To do this, I have been looking, within an installation context, at the phenomenal experience of encounters that are both remote and somehow also close. The approach I have taken is to use light and sound to augment the surface of rocks.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Rock with white projected light #3" height=375 width=500 src=http://4.bp.blogspot.com/-EZ4NZ2C4FvY/UIHWWyrlOMI/AAAAAAAABHg/yHK4rix798U/s500/P1000384-resized.jpg></td></tr><tr><td class=tr-caption style="text-align: center">Rock with white projected light #3</td></tr></tbody></table><p>Soon I will upload video documentation of new installation work, but in this post I want to share some of the photographs that have been part of the outcome of this process.</p><p>The images shown here are all photographs of rock surfaces taken in the studio at <a href="http://www.cactroy.org/">CAC</a>. The surfaces have been mapped with light from a projector. The photographs have not been edited or digitally processed.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Rock surface with white projected light #1" height=375 width=500 src=http://3.bp.blogspot.com/-MMYT4XoIjIs/UIHVuvFVHeI/AAAAAAAABGw/bgrOIKBxoPM/s500/P1000363-resized.jpg></td></tr><tr><td class=tr-caption style="text-align: center">Rock surface with white projected light #1</td></tr></tbody></table><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Two rocks with white projected light #1" height=667 width=500 src=http://4.bp.blogspot.com/-PB9RR871ZKU/UIHWYLgghzI/AAAAAAAABH4/jkcirTzPFCk/s667/P1000402-resized.jpg></td></tr><tr><td class=tr-caption style="text-align: center">Two rocks with white projected light #1</td></tr></tbody></table><p>The image below is one of a series of detail shots taken to capture the texture of the rock surface in incandescent lighting, for use in the developed installation work. The angled light creates a relief and a resulting contrast.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Rock texture photographed with incandescent relief lighting #1" height=375 width=500 src=http://4.bp.blogspot.com/-ZRSREBuNJWc/UIHKTbYXobI/AAAAAAAABF4/Okf-jDOmY1s/s500/P1000047.JPG></td></tr><tr><td class=tr-caption style="text-align: center">Rock texture photographed with incandescent relief lighting #1</td></tr></tbody></table><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Rock surface with white projected light #2" height=375 width=500 src=http://1.bp.blogspot.com/-KtJxngY_45o/UIHVvKrlqgI/AAAAAAAABG8/jCXd6kT3SS0/s500/P1000366-resized.jpg></td></tr><tr><td class=tr-caption style="text-align: center">Rock surface with white projected light #2</td></tr></tbody></table><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Rock texture photographed with incandescent relief lighting #3" height=375 width=500 src=http://4.bp.blogspot.com/-QcJ3os3Y8Dc/UIG7Zhbaj-I/AAAAAAAABFM/3B24k73l-MY/s500/P1000063.JPG></td></tr><tr><td class=tr-caption style="text-align: center">Rock texture photographed with incandescent relief lighting #3</td></tr></tbody></table><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><img alt="Two rocks with white projected light #2" height=667 width=500 src=http://4.bp.blogspot.com/-Sk7mrHz1gck/UIHWXf1zvxI/AAAAAAAABHs/WT29W_YEcMM/s667/P1000397-resized.jpg></td></tr><tr><td class=tr-caption style="text-align: center">Two rocks with white projected light #2</td></tr></tbody></table><p>The projector's light texture is visible in shots at a distance from the light source, as above.</p></div><a class=read-more href=/blog/counterpoint-to-remote>Read more &gt;</a><hr class=separator></article><ul id=paging class=cta><li><a href="/blog/page3/">&lt; Previous page</a></li><li><a href="/blog/page5/">Next page &gt;</a></li></ul></main><aside class=right><nav><header><h1>All posts</h1></header><h2 id=2015-ref>2015</h2><ul><li><a href=/blog/new-site-for-hardware-hack-lab>New Site for Hardware Hack Lab</a></li></ul><h2 id=2014-ref>2014</h2><ul><li><a href=/blog/sound-control-at-future-interfaces>Sound Control at Future Interfaces</a></li><li><a href=/blog/projection-masking-not-projection>Projection Masking, not Projection Mapping</a></li><li><a href=/blog/addon-for-openframeworks-kinect-v2-and>Addon for openFrameworks, Kinect V2 and Mac</a></li><li><a href=/blog/john-cleese-on-creativity>John Cleese on Creativity</a></li><li><a href=/blog/takeaways-from-eyeo-2014>Takeaways from Eyeo 2014</a></li><li><a href=/blog/hardware-hacker-culture-of-new-york>Hardware Hacker Culture of New York</a></li><li><a href=/blog/future-visions-for-human-interaction>Future Visions for Human Interaction</a></li><li><a href=/blog/openbci-nears-its-kickstarter-goal>OpenBCI Nears it's Kickstarter Goal</a></li></ul><h2 id=2013-ref>2013</h2><ul><li><a href=/blog/sound-chamber-2013>Sound Chamber (2013)</a></li><li><a href=/blog/openbci-hackathon-at-thoughtworks>OpenBCI Hackathon at ThoughtWorks</a></li><li><a href=/blog/exploring-depth-video-at-culturehub>Exploring Depth Video at CultureHub</a></li><li><a href=/blog/introduction-to-ibeacons>Introduction to iBeacons</a></li><li><a href=/blog/volumetric-lab-at-culturehub-nyc>Volumetric Lab at CultureHub NYC</a></li><li><a href=/blog/randomness-in-algorithm>Randomness in the Algorithm</a></li><li><a href=/blog/study-existential>Video: Existential</a></li><li><a href=/blog/the-visual-art-of-brian-eno>The Visual Art of Brian Eno</a></li><li><a href=/blog/rgbdtoolkit-sketch-at-sampler>RGBDToolkit Sketch at The Sampler</a></li><li><a href=/blog/the-artist-geek-hybrid>The Artist-Geek Hybrid</a></li><li><a href=/blog/rgbdtoolkit-calibration-tutorial>RGBDToolkit Calibration Tutorial</a></li><li><a href=/blog/how-depth-sensor-works-in-5-minutes>How a Depth Sensor Works - in 5 Minutes</a></li><li><a href=/blog/focal-lengths-and-camera-sensors>Focal Lengths and Camera Sensors</a></li><li><a href=/blog/rgbdtoolkit-visualizer-tutorial>RGBDToolkit Visualizer Tutorial</a></li><li><a href=/blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit Workshop at Eyebeam</a></li><li><a href=/blog/nosql-distilled-to-keynote>NoSQL Distilled to a Keynote!</a></li><li><a href=/blog/paper-prototyping>Paper Prototyping</a></li><li><a href=/blog/git-vs-github>Is Git the Same Thing as Github!?</a></li></ul><h2 id=2012-ref>2012</h2><ul><li><a href=/blog/counterpoint-to-remote>Counterpoint to the Remote</a></li><li><a href=/blog/the-cascading-process>The Cascading Process</a></li><li><a href=/blog/working-with-total-space>Working with Total Space</a></li><li><a href=/blog/residency-begins-at-cac-troy>Residency Begins at CAC Troy</a></li><li><a href=/blog/installation-sketch-at-open-studios>Installation Sketch at Open Studios</a></li><li><a href=/blog/roman-moshenskys-mirror-world>Roman Moshensky's Mirror World</a></li><li><a href=/blog/open-studios-at-i-park>Open Studios at I-Park</a></li><li><a href=/blog/perception-as-creative-process>Perception as a Creative Process</a></li><li><a href=/blog/the-i-park-graveyard>The I-Park Graveyard</a></li><li><a href=/blog/scoping-out-land>Scoping Out the Land</a></li><li><a href=/blog/residency-begins-at-i-park>Residency Begins at I-Park</a></li><li><a href=/blog/residency-at-contemporary-artists-center>Residency at Contemporary Artists Center</a></li><li><a href=/blog/the-shopping-list-for-projection-bombing>First Shopping List for Projection-Bombing</a></li><li><a href=/blog/portable-projection-in-rural-context>Portable Projection in a Rural Context</a></li><li><a href=/blog/stephen-lumentas-sc-textmate-bundle>Stephen Lumenta's SC TextMate Bundle</a></li><li><a href=/blog/adding-openframeworks-addons>Adding OF Addons (ofxSuperCollider)</a></li><li><a href=/blog/setting-up-supercollider-with-textmate>Setting up SuperCollider with TextMate</a></li><li><a href=/blog/switching-to-macbook-pro>Switching to MacBook Pro</a></li><li><a href=/blog/quickref-for-supercollider>QuickRef for SuperCollider</a></li><li><a href=/blog/getting-started-with-supercollider>Getting Started with SuperCollider</a></li><li><a href=/blog/getting-started-with-openframeworks-in>Getting Started with OpenFrameworks</a></li><li><a href=/blog/overtones-harmonics-and-additive>Overtones, Harmonics and Additive Synthesis</a></li><li><a href=/blog/visit-to-cold-spring>Visit to Cold Spring</a></li><li><a href=/blog/i-park-residency>Residency at I-Park</a></li><li><a href=/blog/light-waves>Light Waves</a></li></ul><h2 id=2011-ref>2011</h2><ul><li><a href=/blog/final-exhibition>The Final Exhibition</a></li><li><a href=/blog/playing-with-particles>Playing with Particles</a></li><li><a href=/blog/responsive-granular-sound>Responsive Granular Sound</a></li><li><a href=/blog/kinecting-to-network>Kinecting to the Network</a></li><li><a href=/blog/first-working-day>First Working Day</a></li><li><a href=/blog/designs-for-freemote>Designs for Freemote</a></li><li><a href=/blog/freemote-utrecht>Freemote Utrecht</a></li><li><a href=/blog/untitled-picture-this-2011>Untitled - Picture This (2011)</a></li><li><a href=/blog/wider-context>The Wider Context?</a></li><li><a href=/blog/trading-time-for-space>Trading Time for Space</a></li><li><a href=/blog/talk-at-goldsmiths-digital-studios>Talk at Goldsmiths Digital Studios</a></li><li><a href=/blog/intro-to-marius-watz>Intro to Marius Watz</a></li><li><a href=/blog/practical-guide-to-generative-art>Practical Guide to Generative Art</a></li><li><a href=/blog/installation-at-alpha-ville>Installation at Alpha-Ville</a></li><li><a href=/blog/simple-harmonic-motion>Simple Harmonic Motion</a></li><li><a href=/blog/jaaga-journal-features>Jaaga Journal Features</a></li><li><a href=/blog/gravity-2011>Gravity (2011)</a></li><li><a href=/blog/reflections-2011>Reflections (2011)</a></li><li><a href=/blog/jaaga-sound-lights>Jaaga Sound & Lights</a></li><li><a href=/blog/two-works-for-jaaga-gravity-and-memory>Two Works for Jaaga: Gravity and Reflections</a></li><li><a href=/blog/cosm-collision-detection-and-volume>Cosm, Collision Detection and Volume</a></li><li><a href=/blog/vector-base-amplitude-panning>Vector-Base Amplitude Panning</a></li><li><a href=/blog/intuition-and-direction-of-project>Intuition, and Direction of the Project</a></li><li><a href=/blog/reflections-what-is-jaaga>Reflections: What is Jaaga?</a></li><li><a href=/blog/going-further-with-ambisonics>Going Further with Ambisonics</a></li><li><a href=/blog/introduction-to-ambisonics>Introduction to Ambisonics</a></li><li><a href=/blog/surface-light-sound-installation>Surface (2010)</a></li><li><a href=/blog/running-servo-motor>Servo Motors and Transistors</a></li><li><a href=/blog/spinning-12v-dc-motor>Spinning a 12V DC Motor</a></li><li><a href=/blog/spinning-dc-motor>Spinning a 5V DC Motor</a></li><li><a href=/blog/first-week-at-jaaga>First Week at Jaaga</a></li><li><a href=/blog/presentation-style>Presentation Style</a></li><li><a href=/blog/beginning-jaaga-fellowship>Beginning the Jaaga Fellowship</a></li><li><a href=/blog/brian-eno-role-models-and-direction>Brian Eno, Role Models and Direction</a></li><li><a href=/blog/hype-cycle_17>The Hype Cycle</a></li></ul><h2 id=2010-ref>2010</h2><ul><li><a href=/blog/working-with-3d-space>Working with 3D Space</a></li><li><a href=/blog/technology-and-luck>Technology and Luck</a></li><li><a href=/blog/gallery-types-and-commercial-gallery>Different Types of Gallery</a></li><li><a href=/blog/jason-bruges-studio>Jason Bruges Studio</a></li><li><a href=/blog/unstable-empathy-trust-and>Unstable Empathy & Gaining Trust</a></li><li><a href=/blog/chris-o-two-creative-cultures>Chris O'Shea & Two Creative Cultures</a></li></ul></nav></aside></div><script src=/javascript/script.e2be.js></script></body></html>