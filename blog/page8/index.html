<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1.0"><meta name=author content="Andrew McWilliams"><title>Blog - Andrew McWilliams</title><link href=http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css rel=stylesheet><link href="http://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic" rel=stylesheet type=text/css><link rel=stylesheet href=/stylesheets/style.83cf.css><!--[if lt IE 9]>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv-printshiv.min.js"></script>
    <![endif]--><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.4f09.png><link rel="shortcut icon" href=/images/favicon.6537.png></head><body id=blog class=content><header role=banner><h1><a href="/"><span class=shift>Andrew</span> M<span class=sub>c</span>Williams</a></h1><nav role=navigation><h2>Navigation</h2><a id=menu-button href=#><i class="fa fa-bars"></i> <i class="fa fa-times"></i></a><ul class=cta><li><a href="/about/">About</a></li><li><a href="/works/">Works</a></li><li><a href="/blog/">Blog</a></li></ul></nav></header><div class=flex-row><main class=left><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/visit-to-cold-spring>Visit to Cold Spring</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2012-05-01>Tuesday 1 May, 2012</time><ul class=social><li><a href="http://twitter.com/share?text=Visit%20to%20Cold%20Spring&amp;url=http://jahya.net/blog/visit-to-cold-spring&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Visit%20to%20Cold%20Spring&amp;u=http://jahya.net/blog/visit-to-cold-spring"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Visit%20to%20Cold%20Spring&amp;u=http://jahya.net/blog/visit-to-cold-spring"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>New York state has some outstanding areas of natural beauty. On Sunday we hiked up a trail in Cold Spring, and brought back some photographic evidence.</p><p>I want to talk in this post about some of the discussions that came up as we walked, and the way they are affecting how I'm thinking about my upcoming projects. I'll illustrate the text with some shots of our hike.</p><a href=http://3.bp.blogspot.com/-o7FTo_mMBME/T59NisAt0VI/AAAAAAAAAlQ/DCfWlktToq4/s1600/P4309591.JPG style="margin-left: 1em; margin-right: 1em; display: block; text-align: center"><img alt="An early view from the trail" height=375 src=http://3.bp.blogspot.com/-o7FTo_mMBME/T59NisAt0VI/AAAAAAAAAlQ/DCfWlktToq4/s500/P4309591.JPG width=500></a><p>I met Hannah Gould, and as we hiked she told me about her father's <a href="http://www.lizardskinstudios.com/">studio out in San Jose, California</a> - and their mutual admiration of Andy Goldsworthy. Bill Gould creates various types of physical sculpture, signage, fencing and gates for public locations.</p><p>It seems possible that I could arrange a trip to see him and spend some time in the studio creating work inspired by <a href=http://en.wikipedia.org/wiki/Land_art>land art</a>.</p><a href=http://3.bp.blogspot.com/-haYF6Fg9O8I/T59NnPnSWII/AAAAAAAAAlY/u9VYlQBOg_Y/s1600/P4309599.JPG style="margin-left: 1em; margin-right: 1em; display: block; text-align: center"><img alt="Climbing the trail" height=667 src=http://3.bp.blogspot.com/-haYF6Fg9O8I/T59NnPnSWII/AAAAAAAAAlY/u9VYlQBOg_Y/s667/P4309599.JPG width=500></a><p>It all seemed to tie in very nicely with the <a href=//blog/i-park-residency>plans I have been making</a> for I-Park. It feels like a cloud is forming around this idea of 'land art', portable projection, and visual-sound correlation.</p><p>I'm beginning to see several disparate concepts converge. All of which I can come back to, flesh out and consider over the coming months.</p><p><strong>Spatial augmentation and exploration</strong><br>I'm really interested in thinking about the way that we consciously and unconsciously project value onto objects and environments around us. 'Nature' and 'natural objects', rocks, trees and rivers, are perceived with a given context based on the historic demographic context of the observer. In the work I produce I want to think about the ways my augmentations of these objects affect this sense of value.</p><a href=http://1.bp.blogspot.com/-ZOjS2afjWDc/T59NrU24DRI/AAAAAAAAAlg/I0VOqaSaV18/s1600/P4309606.JPG style="margin-left: 1em; margin-right: 1em; display: block; text-align: center"><img alt="Ascending the trail" height=375 src=http://1.bp.blogspot.com/-ZOjS2afjWDc/T59NrU24DRI/AAAAAAAAAlg/I0VOqaSaV18/s500/P4309606.JPG width=500></a><p><strong>Land art</strong><br>For example, in the arrangement of natural objects in the traditional sense of land art, do we 'humanize' the object, in making it more relevant to us? Or are we moving closer to nature, in the sense that we are augmenting natural objects by revealing natural patterns?</p><p>And one step further from that, does it really make sense to demarcate 'natural' and 'human' and to posit them on either end of a scale?</p><a href=http://3.bp.blogspot.com/-AdNqvHBakzE/T59N1lYaxwI/AAAAAAAAAlw/hue4G5CqKPY/s1600/P4309627.JPG style="margin-left: 1em; margin-right: 1em; display: block; text-align: center"><img alt="A land art performance" height=375 src=http://3.bp.blogspot.com/-AdNqvHBakzE/T59N1lYaxwI/AAAAAAAAAlw/hue4G5CqKPY/s500/P4309627.JPG width=500></a><p><strong>Working with natural textures</strong><br>And what happens to this conversation when we introduce digital elements?</p><p>If I project algorithmically-constructed patterns as light onto a naturally textured surface, does this do anything more to the surface than can be achieved with the use of natural materials alone?</p><p>Will the strictness of the algorithmic geometry work against the endless unpredictability of the natural texture? Will this form a disconnect, and what might this disconnect say about the demarcation between 'natural' and 'human'?</p><p>And does this disconnect occur in 'pure' land art?</p><a href=http://4.bp.blogspot.com/-GFa7P2u5TCk/T59OaJTJlBI/AAAAAAAAAmo/iB87A3w5F1A/s1600/P4309659.JPG style="margin-left: 1em; margin-right: 1em; display: block; text-align: center"><img alt="Augmenting a natural texture" height=667 src=http://4.bp.blogspot.com/-GFa7P2u5TCk/T59OaJTJlBI/AAAAAAAAAmo/iB87A3w5F1A/s667/P4309659.JPG width=500></a><p>I've always preferred 'natural' texture to algorithmically-generated ones (i.e. Perlin noise), and so I'm fascinated by the idea of projecting onto them. So much projection mapping is designed to attack as geometrically 'perfect' surfaces as possible, it will be really liberating to explore with the purpose of highlighting the 'flaws' in the surface rather than concealing them.</p><p><strong>The synaesthetic effect</strong><br>The aspect which excites me most about digital projection right now is the way it can be linked in real-time to other perceptible events - in particular simultaneously generated sound. Some people call this the 'synaesthetic' effect, though synaesthesia is about much more than just vision and sound.</p><p>Walking with Gene we discussed potential experiments we could work on with SuperCollider, and combining it with VVVV. Gene is passionate about SuperCollider at the moment, and we have already agreed to collaborate over the next few months before he goes to India.</p><a href=http://2.bp.blogspot.com/-l7uO7wuuipM/T59OQLRhl8I/AAAAAAAAAmY/4yYtSfNIjcw/s1600/P4309657.JPG style="margin-left: 1em; margin-right: 1em; text-align: center; display: block"><img alt="Standing on a rock in Cold Spring" height=375 src=http://2.bp.blogspot.com/-l7uO7wuuipM/T59OQLRhl8I/AAAAAAAAAmY/4yYtSfNIjcw/s500/P4309657.JPG width=500></a><p>These experiments, along with the land art training in Bill Gould's studio could provide a really solid base for the work I produce at the I-Park residency.</p><p><strong>The technical part of portable projection</strong><br>The one and only reason I am interested in 'portable projection', is so that I can walk off with a projector to a remote location and not have to worry about a power source. The way I've seen this achieved is by connecting a car battery to a projector. There are ofcourse <a href=http://en.wikipedia.org/wiki/Handheld_projector>pico projectors</a>, but these have nowhere enough lumens to make a convincing 'coat of light'.</p><p>The car battery approach means that each projection will have a very ephemeral nature. The projector will need to be switched off shortly before the power runs out. This gives only about an hour, including set-up and mapping time.</p><a href=http://1.bp.blogspot.com/-byk8aUnu4Wc/T59NwbZOysI/AAAAAAAAAlo/lSSxEOhphuI/s1600/P4309611.JPG style="margin-left: 1em; margin-right: 1em; text-align: center; display: block"><img alt="Descending the trail" height=667 src=http://1.bp.blogspot.com/-byk8aUnu4Wc/T59NwbZOysI/AAAAAAAAAlo/lSSxEOhphuI/s667/P4309611.JPG width=500></a><p><strong>Computer Vision and quick mapping</strong><br>Gene is also very interested in projection mapping, and he is particularly interested in using Computer Vision libraries to create a real-time automapper using a webcam. This could mean that software could continuously scan a webcam feed, and use edge detection and other algorithms to continuously redraw it's internal representation of projected surfaces. This in turn would mean that as you move the projector around the object, the 'coat of light' applied would continuously update (a considerably lower-tech version of <a href="http://createdigitalmotion.com/2012/04/projection-mapping-with-robotics-goes-further-to-augmenting-reality-mps-demo-touchdesigner/">this</a>).</p><p>This is lofty stuff, but I am interested in exploring this because it could speed up the mapping process. Which, when you are time-limited by battery life, would be a really positive thing.</p><a href=http://3.bp.blogspot.com/-mldt3ZtTWKs/T59Ota1o7JI/AAAAAAAAAnI/YEc0GHU5xPE/s1600/P4309707.JPG style="margin-left: 1em; margin-right: 1em; text-align: center; display: block"><img alt="Coming to the end of the trail" height=667 src=http://3.bp.blogspot.com/-mldt3ZtTWKs/T59Ota1o7JI/AAAAAAAAAnI/YEc0GHU5xPE/s667/P4309707.JPG width=500></a><p>I will come back and look at these subjects more in the coming weeks, to see where a little fleshing out takes them.</p></div><a class=read-more href=/blog/visit-to-cold-spring>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/i-park-residency>Residency at I-Park</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2012-04-28>Saturday 28 April, 2012</time><ul class=social><li><a href="http://twitter.com/share?text=Residency%20at%20I-Park&amp;url=http://jahya.net/blog/i-park-residency&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Residency%20at%20I-Park&amp;u=http://jahya.net/blog/i-park-residency"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Residency%20at%20I-Park&amp;u=http://jahya.net/blog/i-park-residency"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>I have just accepted a residency at <a href="http://www.i-park.org/">I-Park</a>, Connecticut starting in mid-August 2012!</p><table class=tr-caption-container style="margin-left: auto; margin-right:auto"><tbody><tr><td style="text-align: center"><a href=http://www.i-park.org/index.html style="clear:right; float:right; margin-left:1em; margin-right:1em"><img height=210 width=283 src=http://3.bp.blogspot.com/-3Dv80fPuUvw/T5x6-8st3zI/AAAAAAAAAlA/qlMrMaOj5zo/s1600/NNProjectOverview.jpg></a></td></tr><tr><td class=tr-caption style="text-align: center">The I-Park house</td></tr></tbody></table><p>I-Park is a unique space - it is a woodland retreat, and the work I do will be embedded in the natural environment. This is a first for me, having always exhibited in cities (although <a href=http://jahya.net/blog/?tag:Jaaga_Residency>not always in a whitewall gallery</a>!)</p><p>From the I-Park website:</p><blockquote>"I-Park is a 450-acre woodland retreat in rural East Haddam, Connecticut. The property consists of ponds, hills, streams, stone outcroppings and sheer cliffs. It has wild fields and new growth forest, as well as miles of stonewalls and walking trails. It is bisected by the Eight Mile River and adjoins the Devil's Hopyard State Park and other preserved tracts. The land has a wild, gnarly character that suits I-Park's role as a refuge from and recourse to the safe routines and subtle compromises of the workaday world."</blockquote><p>I will be experimenting with the intersection between portable projection and land art, inspired by the work of <a href="http://prickimage.com/">Shaun / PRICKIMAGE</a>, and my friend <a href="http://www.flickr.com/photos/brainflakes/3036022224/">Andrew Crowe</a>. I will keep this blog updated with my preperations and progress. So watch this space!</p><p>My friend and fellow resident at Jaaga, <a href="http://deweyhagborg.com/">Heather Dewey-Hagborg</a> created a piece for I-Park in 2010, entitled <a href="http://deweyhagborg.wordpress.com/2010/09/08/buried-sound-series-1-bower/">Bower</a>. And there is a <a href=http://www.i-park.org/FellowsbyYear.html>long list of previous fellows</a> who have created work at I-Park since 2001.</p><p>This will be a great chance to get out of city life and produce something unique and different!</p></div><a class=read-more href=/blog/i-park-residency>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/light-waves>Light Waves</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2012-01-19>Thursday 19 January, 2012</time><ul class=social><li><a href="http://twitter.com/share?text=Light%20Waves&amp;url=http://jahya.net/blog/light-waves&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Light%20Waves&amp;u=http://jahya.net/blog/light-waves"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Light%20Waves&amp;u=http://jahya.net/blog/light-waves"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>I've been invited to produce an artwork for Light Waves, an interactive lighting installation spanning several buildings in Ipswich. My work will feature as one of a series of works using the lighting infrastructure installed by <a href="http://www.creatmosphere.com/">Creatmosphere</a> at the end of last year.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tr><td style="text-align: center"><div class=separator style="clear: both; text-align: center"><a href=http://4.bp.blogspot.com/-yh00z0V830E/TxhQOyUYzEI/AAAAAAAAAU0/rJd-c_0eUEw/s1600/Lightwaves_by_Creatmosphere_01.jpg style="margin-left: 1em; margin-right: 1em"><img height=225 src=http://4.bp.blogspot.com/-yh00z0V830E/TxhQOyUYzEI/AAAAAAAAAU0/rJd-c_0eUEw/s320/Lightwaves_by_Creatmosphere_01.jpg width=320 alt="Landscape shot of the building and space"></a></div></td></tr><tr><td class=tr-caption style="text-align: center">A view of the theatre and derelict buildings from across the waterfront<br>(Photograph by <a href="http://www.jnphotographs.co.uk/">James Newton</a>)</td></tr></table><p>We had a kick-off meeting last night to discuss the project, at the Electric Matchbox in Hackney Wick. Hayden and Denise talked us through the infrastructure.</p><p>The installation is based out of the Jerwood DanceHouse (we call it 'the theatre'), an international dance centre for training and performance. It's the brightly-lit building on the bottom-right in the picture above. Those lights are part of a DMX-controlled 109-part lighting installation which can be seen clearly from across the waterfront - from where the photo was taken.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tr><td style="text-align: center"><div class=separator style="clear: both; text-align: center"><a href=http://2.bp.blogspot.com/-d8_AK1yJk4w/TxhQWtpdgcI/AAAAAAAAAWQ/1jPu4EN7T8Q/s1600/Lightwaves_by_Creatmosphere_13.jpg style="margin-left: 1em; margin-right: 1em; text-align: center"><img alt="A close-up of a window" height=140 src=http://2.bp.blogspot.com/-d8_AK1yJk4w/TxhQWtpdgcI/AAAAAAAAAWQ/1jPu4EN7T8Q/s200/Lightwaves_by_Creatmosphere_13.jpg width=200></a><a href=http://3.bp.blogspot.com/-dJ7UEMxEew8/TxhQQYOk_wI/AAAAAAAAAVI/iv2KFnS3Iq8/s1600/Lightwaves_by_Creatmosphere_04.jpg style="margin-left: 1em; margin-right: 1em"><img alt="The larger building (old grain mill)" height=200 src=http://3.bp.blogspot.com/-dJ7UEMxEew8/TxhQQYOk_wI/AAAAAAAAAVI/iv2KFnS3Iq8/s200/Lightwaves_by_Creatmosphere_04.jpg width=140></a></div></td></tr><tr><td class=tr-caption style="text-align: center">Close-ups of the old grain mill<br>(Photographs by <a href="http://www.jnphotographs.co.uk/">James Newton</a>)</td></tr></table><p>Most of the 109 lights are housed on the fa&#231;ade of the theatre building, hence it's brightness. But a large number of lights are also installed in the adjacent buildings. The three tall buildings rising out directly above and behind the theatre aren't involved; the two wider buildings to the left of the theatre are part of the installation and filled with lights.</p><p>Those two buildings have been derelict for years. Seemingly the larger of the two housed an industrial grain mill, but now it's a home mainly to pigeons. Here <a href=http://www.edp24.co.uk/news/blaze_at_ipswich_waterfront_1_163401>is a report of it being on fire a couple of years ago</a>, and here <a href=http://www.derelicte.co.uk/ipswich-waterfront-mills>are some very nice photos of the interior</a>.<table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tr><td style="text-align: center"><div class=separator style="clear: both; text-align: center"><a href=http://1.bp.blogspot.com/-5Jbhc57qksg/TxhQPorSEVI/AAAAAAAAAVA/e-wXs-5pKbU/s1600/Lightwaves_by_Creatmosphere_03.jpg style="margin-left: 1em; margin-right: 1em"><img alt="People using the space" height=225 src=http://1.bp.blogspot.com/-5Jbhc57qksg/TxhQPorSEVI/AAAAAAAAAVA/e-wXs-5pKbU/s320/Lightwaves_by_Creatmosphere_03.jpg width=320></a></div></td></tr><tr><td class=tr-caption style="text-align: center">The projection and sounds follow human movement<br>(Photograph by <a href="http://www.jnphotographs.co.uk/">James Newton</a>)</td></tr></table></p><p>The photo above shows a close-up of the entranceway to the theatre, which is accessed via the waterfront entrance and receives most of the footfall. This is the other dimension to Light Waves - a ground-level projection area which responds to human movement, along with a 2-channel elevated loudspeaker arrangement above.</p><p>The lighting for the buildings and the lighting in the projection are all controlled from central software - lovingly crafted by Hayden and <a href=http://www.v4wednesday.com>available on the V4W website</a> (click Digital Platform Preview).</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tr><td style="text-align: center"><div class=separator style="clear: both; text-align: center"><a href=http://4.bp.blogspot.com/-C6nF7v38z2Y/TxhQRv1EvTI/AAAAAAAAAVc/kCdKiF1h0B0/s1600/Lightwaves_by_Creatmosphere_06.jpg style="margin-left: 1em; margin-right: 1em"><img alt="People using the space" height=225 src=http://4.bp.blogspot.com/-C6nF7v38z2Y/TxhQRv1EvTI/AAAAAAAAAVc/kCdKiF1h0B0/s320/Lightwaves_by_Creatmosphere_06.jpg width=320></a></div></td></tr><tr><td class=tr-caption style="text-align: center">Light strips decorate the horizontal colonnade<br>(Photograph by <a href="http://www.jnphotographs.co.uk/">James Newton</a>)</td></tr></table><p>The infrastructure is planned to remain installed for two years, and during that time a selection of artists will be invited to come up with concepts (and software) for how to use it. Each artist will have their work installed for one month.</p><p>The lights can be individually controlled, and the response time should be very fast (once Hayden installs some software to get around the bloated out-of-the-box DMX controller software). The building lights can respond to the movements of people under the projection, or the lights under the projection can respond to the light of the building. Similarly for the use of audio - sound can be used to control lighting or lighting can be used to control sound.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tr><td style="text-align: center"><div class=separator style="clear: both; text-align: center"><a href=http://4.bp.blogspot.com/-RdiUNtrEZAw/TxhQV1TpzCI/AAAAAAAAAWI/iAbwZ0zsrh4/s1600/Lightwaves_by_Creatmosphere_12.jpg style="margin-left: 1em; margin-right: 1em"><img alt="The back of the building" height=225 src=http://4.bp.blogspot.com/-RdiUNtrEZAw/TxhQV1TpzCI/AAAAAAAAAWI/iAbwZ0zsrh4/s320/Lightwaves_by_Creatmosphere_12.jpg width=320></a></div></td></tr><tr><td class=tr-caption style="text-align: center">The view from a steeper angle<br>(Photograph by <a href="http://www.jnphotographs.co.uk/">James Newton</a>)</td></tr></table><p>This gives each artist considerable freedom - all they have to do to utilise the infrastructure is design some software to slot into Hayden's infrastructure patch.</p><p>Above is the view of the back of the disused mill. The position of the windows seems arbitrary, but it serves the building's original function. Inside each window there is not a room, rather a some kind of terrace. I'm sure I could describe that better if I had actually been to the space! But those odd arrangements on each side make this quite a peculiar show. It will be interesting to see what everyone comes up with.</p></div><a class=read-more href=/blog/light-waves>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/final-exhibition>The Final Exhibition</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2011-12-14>Wednesday 14 December, 2011</time><ul class=social><li><a href="http://twitter.com/share?text=The%20Final%20Exhibition&amp;url=http://jahya.net/blog/final-exhibition&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=The%20Final%20Exhibition&amp;u=http://jahya.net/blog/final-exhibition"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=The%20Final%20Exhibition&amp;u=http://jahya.net/blog/final-exhibition"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>A few nights ago, after 3 intense days and nights of working, we installed our collaborative installation at Freemote Utrecht. After all the work, it was up for about 6 hours.</p><div style="text-align: center"><iframe frameborder=0 height=300 width=534 src="http://player.vimeo.com/video/33682783?byline=0&amp;portrait=0"></iframe></div><p>The group I've been working with, <a href="http://www.v4wednesday.com/">V4W</a>, do this a lot - it's all part of the process for them. Turn up with a bunch of equipment, create an artwork onsite during opening hours, and display it on the final night. While we were working, people came over and observed, made comments (and jokes), and asked questions.</p><table align=center class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-w055yxKaSdE/Tujzre5Z8SI/AAAAAAAAATs/dXnJwqTHyw0/s1600/_1010255.JPG style="margin-left: auto; margin-right: auto"><img height=213 src=http://1.bp.blogspot.com/-w055yxKaSdE/Tujzre5Z8SI/AAAAAAAAATs/dXnJwqTHyw0/s320/_1010255.JPG width=320></a></td></tr><tr><td class=tr-caption style="text-align: center">We were our exhibit (on the left)</td></tr></tbody></table><p>Speaking to Gareth and Hayden about it, they say they like the exposure it gives to the subculture of digital artists, musicians and programmers, i.e. us. It's difficult for people to get a handle on what exactly it is we do, unlike say in film, music or painting. Don't get me wrong, there is a world of hidden esoteric knowledge and in-culture in those media, but the difference is that audiences have had a lot longer to figure out their relationship to it.</p><table align=center class=tr-caption-container style="margin-left: 1.5em; text-align: center; float:right"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-ceUHoHdk8g8/Tuj0mtYFa8I/AAAAAAAAAUM/dCGreAw3c04/s1600/P1010377.JPG style="margin-left: auto; margin-right: auto"><img height=112 src=http://4.bp.blogspot.com/-ceUHoHdk8g8/Tuj0mtYFa8I/AAAAAAAAAUM/dCGreAw3c04/s200/P1010377.JPG width=200></a></td></tr><tr><td class=tr-caption style="text-align: center">Discussing options</td></tr></tbody></table><p>We did a similar thing at AlphaVille, so this is my second run with V4W. For me personally, I like the challenge. But I'm not sure how convinced I am about working that way regularly - it's hectic. Quick choices have to be made, and with eight people on a short timescale, people can pull in different directions.</p><p>In any case, what results is quite nice simply for that reason. It's a Frankenstein of different creative impulses thrown together, each relatively uncensored and forced to mix on equal terms. No hierarchy seemed to develop, and no-one was precious or held the group to ransom. It's probably because they pretty much all know each other and have worked together before, and are an open-minded group.</p><table align=center class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-_8ndHNhhsEE/Tuj0CC8TCVI/AAAAAAAAAT0/PIRxDeIi3dc/s1600/_1010636.JPG style="margin-left: auto; margin-right: auto"><img height=213 src=http://4.bp.blogspot.com/-_8ndHNhhsEE/Tuj0CC8TCVI/AAAAAAAAAT0/PIRxDeIi3dc/s320/_1010636.JPG width=320></a></td></tr><tr><td class=tr-caption style="text-align: center">Doing Yoga... no not really</td></tr></tbody></table><p>Actually somebody in the group said that it's because we're all British (and so we have a stereotype to live up to). I'm not sure about that, but with a different mix of people, I could easily see more tension.</p><p>So in effect Gareth was more a coordinator than a director. As for my role, I became more a technical consultant and facilitator. Due to some tight deadlines for proposals in the run-up to the trip, I wasn't really able to get into the design discussion until after the concept was concretised. I arrived looking for something to do and found an open niche as Max/MSP developer, helping to create the interactive sound based somewhere between Barney's sound design (mostly musical) and Alex's audio manipulation ideas (mostly noise).</p><table align=center class=tr-caption-container style="margin-left: 1.5em; text-align: center; float:right"><tbody><tr><td style="text-align: center"><a href=http://2.bp.blogspot.com/--q0qJyjcQDw/Tuj0Uwddk1I/AAAAAAAAAT8/zr336GRSbpA/s1600/_1010647.JPG style="margin-left: 1em; margin-right: 1em"><img height=133 src=http://2.bp.blogspot.com/--q0qJyjcQDw/Tuj0Uwddk1I/AAAAAAAAAT8/zr336GRSbpA/s200/_1010647.JPG width=200></a><br><br><a href=http://2.bp.blogspot.com/-vMclMtqm7ew/Tuj0jcM0vLI/AAAAAAAAAUE/9xAsywqNxMo/s1600/_1010651.JPG style="margin-left: 1em; margin-right: 1em; text-align: center"><img height=133 src=http://2.bp.blogspot.com/-vMclMtqm7ew/Tuj0jcM0vLI/AAAAAAAAAUE/9xAsywqNxMo/s200/_1010651.JPG width=200></a></td></tr><tr><td class=tr-caption style="text-align: center">Some of the other installations</td></tr></tbody></table><p>I didn't feel like what we needed was another voice on top of these two, pushing yet another creative direction, so instead I looked for synthesis between them. The two streams proved divergent, and I think in the end we just allowed that to be. There was a music section and an interactive sound section, running at different times, and honestly each were far better without the interference of the other.</p><p>The three aspects of the video - the particles, the floor and the balls - all seemed to come together on equal terms, but we'll see how people feel about that in the retrospective we have coming up.</p><p>So again, it all points to what it was - a 3 day, open, creative experiment. We created a piece for exhibition, but in the end the exhibition was us, programming, composing, designing and setting up hardware. At least, I think that's what 70% of visitors to the festival will remember from it. But it seems that's what V4W are about - exposing the subculture.</p><p>I'm looking forward to (and slightly afraid of) the next time we work together.</p></div><a class=read-more href=/blog/final-exhibition>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/playing-with-particles>Playing with Particles</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2011-12-09>Friday 9 December, 2011</time><ul class=social><li><a href="http://twitter.com/share?text=Playing%20with%20Particles&amp;url=http://jahya.net/blog/playing-with-particles&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Playing%20with%20Particles&amp;u=http://jahya.net/blog/playing-with-particles"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Playing%20with%20Particles&amp;u=http://jahya.net/blog/playing-with-particles"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>Mike, Gareth and Joe have been working on a 3D scene which will be projected up on the wall, and which will respond in real-time to the Kinect data. The environment is built and runs in VVVV.</p><table align=center class=tr-caption-container style="margin-left: 1.5em; text-align: center; float:right"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-6uGv7wVCmjA/TuFVYLN6pVI/AAAAAAAAATk/Nyl3EukpJGc/s1600/SL372438.JPG imageanchor=1 style="margin-left: auto; margin-right: auto"><img alt="A first dry run" height=320 src=http://1.bp.blogspot.com/-6uGv7wVCmjA/TuFVYLN6pVI/AAAAAAAAATk/Nyl3EukpJGc/s320/SL372438.JPG width=240></a></td></tr><tr><td class=tr-caption style="text-align: center">A first dry run</td></tr></tbody></table><p>Mike has created the particle stream (the spheres <a href=//blog/designs-for-freemote>mentioned in the design</a> which flow to / from the threshold). He found the <a href=http://vvvv.org/contribution/ciantparticles-3d-dynamic-16000000p-gpu-particle-system>CiantParticles patch</a> early, because it pretty much just does what we wanted to do. CiantParticles move according to forces you specify with parameters, and they respond to other objects in the scene - in this case our Kinect skeletons.</p><p>We are still experimenting with exactly how the particles should respond to people - should they gravitate towards and fall away? Or bounce off, or avoid them altogether? Mike has created a threshold (the vertical line in the image), and how they react will depend on which side of the threshold the person is on.</p><p>Joe has created a floor that our Kinect people will be walking on. The floor is a 2D grid distorted by waves. The wave system is <a href=http://vvvv.org/forum/another-particles-and-fluids-pluging>supplied by the Fluid plugin</a>. You feed into the plugin the size of the grid, and the coordinates of people's footfalls, and it sends out a new grid with the wave values.</p><p>As the footfall interaction changes, the internal state of the Fluid plugin updates to reflect the wave patterns and outputs the changes in realtime. We then use these output values to affect the 2D floor grid by supplying them as arguments to a vertexshader operating on the grid.</p><table align=center class=tr-caption-container style="margin-left: auto; margin-right:auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://3.bp.blogspot.com/-w3PQCe9EiGg/TuFNc6POW0I/AAAAAAAAATU/l7Z73pIRbn8/s1600/SL372378.JPG style="margin-left: 1em; margin-right: 1em"><img height=150 src=http://3.bp.blogspot.com/-w3PQCe9EiGg/TuFNc6POW0I/AAAAAAAAATU/l7Z73pIRbn8/s200/SL372378.JPG width=200 alt="Joe's broken laptop screen"></a>&nbsp;&nbsp;&nbsp;&nbsp;<a href=http://4.bp.blogspot.com/-VWoX7sjxtn0/TuFNnXRMqqI/AAAAAAAAATc/9YcTiCHaVrs/s1600/SL372383.JPG style="margin-left: 1em; margin-right: 1em"><img height=150 src=http://4.bp.blogspot.com/-VWoX7sjxtn0/TuFNnXRMqqI/AAAAAAAAATc/9YcTiCHaVrs/s200/SL372383.JPG width=200 alt="A render of the watery floor"></a></td></tr><tr><td class=tr-caption style="text-align: center">Joe's working environment</td></tr></tbody></table><p>Gareth has been working <a href=http://vvvv.org/contribution/bullet-physics-nodes-beta>with the Bullet physics system</a> to allow Kinect skeletons to interact with floating objects inside Mike's particle stream. The idea is that when you are inside the stream and looking at yourself in the projection, you will notice floating objects near your Kinect alter-ego.</p><p>You can then reach out and interact with them - we are still experimenting as to how. But this provides a more immediate objective for people inside the stream, because the other interactions with the sound environment will probably take a little more time to get used to.</p><table align=center class=tr-caption-container style="margin-left: auto; margin-right:auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-cemm7WGfJNw/TuFNVmn_AOI/AAAAAAAAATM/rCsNP5N15zs/s1600/SL372377.JPG style="margin-left: 1em; margin-right: 1em"><img alt="Mike's screen rendering particles" height=240 src=http://1.bp.blogspot.com/-cemm7WGfJNw/TuFNVmn_AOI/AAAAAAAAATM/rCsNP5N15zs/s320/SL372377.JPG width=320></a></td></tr><tr><td class=tr-caption style="text-align: center">Mike's screen rendering particles</td></tr></tbody></table><p>Data from this interaction with objects inside the stream will be sent out to the sound environment so that the interaction has a sonic element to it too, <a href=//blog/responsive-granular-sound>as discussed in this post</a>.</p><p>We got as far as a dry-run by the end of the night tonight - not bad for three days work. Tomorrow we'll be looking to tighten everything up, and to experiment with how the interaction feels now that we have a running environment.</p></div><a class=read-more href=/blog/playing-with-particles>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/responsive-granular-sound>Responsive Granular Sound</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2011-12-08>Thursday 8 December, 2011</time><ul class=social><li><a href="http://twitter.com/share?text=Responsive%20Granular%20Sound&amp;url=http://jahya.net/blog/responsive-granular-sound&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Responsive%20Granular%20Sound&amp;u=http://jahya.net/blog/responsive-granular-sound"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Responsive%20Granular%20Sound&amp;u=http://jahya.net/blog/responsive-granular-sound"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>Alex, Barney and I have been making an responsive sound environment. The idea is that when people walk into the installation space, their 'skeleton' is assigned an instrument from the currently playing music track.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-T3AkMdtp10g/TuExlH5fGnI/AAAAAAAAAS0/6oyY-VNC-Ig/s1600/SL372362.JPG style="margin-left: 1em; margin-right: 1em"><img height=240 src=http://4.bp.blogspot.com/-T3AkMdtp10g/TuExlH5fGnI/AAAAAAAAAS0/6oyY-VNC-Ig/s320/SL372362.JPG width=320 alt="Setting up the speakers"></a></td></tr><tr><td class=tr-caption style="text-align: center">Setting up the space</td></tr></tbody></table><p>As they move around, their movements control effects on that instrument only. When another person enters the space, they are assigned another instrument and each can continue to control their own instrument independently of each other, each person contributing to the overall sound.</p><p>With multiple people controlling different aspects of the sound, it could easily get a bit chaotic. So we are keeping some tracks fixed, playing back a blanket of sound against which the other controlled instruments can springboard.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://2.bp.blogspot.com/-qKUKLR_fdqM/TuEyPTSf34I/AAAAAAAAAS8/R1jGKKc6DFk/s1600/barney.png style="margin-left: 1em; margin-right: 1em"><img height=235 src=http://2.bp.blogspot.com/-qKUKLR_fdqM/TuEyPTSf34I/AAAAAAAAAS8/R1jGKKc6DFk/s320/barney.png width=320 alt="Barney composing in Reason"></a></td></tr><tr><td class=tr-caption style="text-align: center">Composing with Reason - roughly speaking each instrument above will be controlled by one Kinect skeleton</td></tr></tbody></table><p>Barney has made some mellow electronic tracks, with looping melodies. The idea is that there will be some pace and a connection with the electronic music taking place in the rest of the venue. He has used Reason to compose the music and render out individual tracks for us to cut apart in realtime in Max/MSP.</p><p>Alex and I have created a Max/MSP patch to handle interaction with the sound environment. It's kind of the go-between for the music against the interaction values received over the network from the graphics server.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://2.bp.blogspot.com/-K_tCgGCDNbY/TuEyRBx__zI/AAAAAAAAATE/_dKiQN55JcY/s1600/maxmsp.png style="margin-left: 1em; margin-right: 1em"><img height=189 src=http://2.bp.blogspot.com/-K_tCgGCDNbY/TuEyRBx__zI/AAAAAAAAATE/_dKiQN55JcY/s320/maxmsp.png width=320 alt="The Max/MSP patch"></a></td></tr><tr><td class=tr-caption style="text-align: center">Cutting it up with a modular Max/MSP patch</td></tr></tbody></table><p>The Max/MSP patch operates on up to 8 rendered tracks (instruments) for each of Barney's compositions - each track is controlled by one Kinect skeleton. Each track has up to 9 controllable attributes (reverb / granular synthesis / VSTs). So each Kinect skeleton controls 9 attributes on a single track of the currently playing composition.</p><p>One of the key parts of building this environment has been modularising Alex's original granular synth and reverb patches, so that they can be re-used with different settings and against different buffers (above).</p><p>We also created a 4-way panning system based on VBAP to allow us to pan sounds around the four speakers in our installation environment. Each rendered track can be panned individually. We'll be experimenting later today to figure out whether this panning should also be controlled by OSC data or some built-in pattern related to the sounds.</p></div><a class=read-more href=/blog/responsive-granular-sound>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/kinecting-to-network>Kinecting to the Network</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2011-12-08>Thursday 8 December, 2011</time><ul class=social><li><a href="http://twitter.com/share?text=Kinecting%20to%20the%20Network&amp;url=http://jahya.net/blog/kinecting-to-network&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Kinecting%20to%20the%20Network&amp;u=http://jahya.net/blog/kinecting-to-network"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Kinecting%20to%20the%20Network&amp;u=http://jahya.net/blog/kinecting-to-network"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>Tom and Hayden have been working on grabbing Kinect data, which is basically data about human movement, and sending it out across the network. The graphics server (an Alienware laptop) reads the data from the network and uses it to render an Augmented Reality scene, which is projected up on the wall.</p><table class=tr-caption-container style="margin-left: 1.5em; text-align: center; float: right"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-5YL2sKF3lBg/TuD-ODcJXKI/AAAAAAAAASE/wfOdaM9fFFc/s1600/SL372350.JPG style="clear:left; float:left;margin-right:1em; margin-right:1em"><img alt="Barney interacting with a Kinect" height=320 width=240 src=http://4.bp.blogspot.com/-5YL2sKF3lBg/TuD-ODcJXKI/AAAAAAAAASE/wfOdaM9fFFc/s320/SL372350.JPG></a></td></tr><tr><td class=tr-caption style="text-align: center">Barney interacting with a Kinect</td></tr></tbody></table><p>The Kinect is a very cool Microsoft device which was designed for the Xbox 360. It's great for mapping realistic human activity into a 3D model in realtime. It has two cameras onboard - one infrared camera for reading depth data, one webcam-style camera for getting a regular video image matrix.</p><p>Tom downloaded the <a href="http://kinectforwindows.org/">Microsoft SDK</a> (there are others, for example <a href=http://www.codeproject.com/Articles/148251/How-to-Successfully-Install-Kinect-on-Windows-Open.aspx>OpenNI/NITE</a>), which you can use to transpose the depth data onto the video. The result is a video image with identifiable humans overlaid (see the image below).</p><p>The Microsoft SDK also contains software to take this further. By analysing data from the camera streams, and using algorithms designed to find human body parts (such as limbs and torsos), the SDK can build an overlay of 3D 'skeletal' data. This means that it works out the points where it thinks the human body parts extend to and draws lines between them to make up a stick man ('skeleton').</p><p>Tom has written a small C# program which uses the SDK to grab skeletal data points and broadcast them across the network. We are using 3 Kinects, each of which can handle 2 skeletons reliably, making a total of up to 6 actors that can interact with the installation.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-Df7wJ0_tk2o/TuD-On2k_VI/AAAAAAAAASQ/4ZnqMRkhLYU/s1600/SL372315.JPG><img alt="Kinect depth data" height=150 width=200 src=http://1.bp.blogspot.com/-Df7wJ0_tk2o/TuD-On2k_VI/AAAAAAAAASQ/4ZnqMRkhLYU/s200/SL372315.JPG></a>&nbsp;&nbsp;&nbsp;&nbsp; <a href=http://1.bp.blogspot.com/-qrhIHD_Wjy0/TuEOF5_YaRI/AAAAAAAAASk/rCyh4yqJ2Po/s1600/SL372409_flipped.jpg><img alt="Kinect skeletal data" height=150 src=http://1.bp.blogspot.com/-qrhIHD_Wjy0/TuEOF5_YaRI/AAAAAAAAASk/rCyh4yqJ2Po/s200/SL372409_flipped.jpg width=200></a></td></tr><tr><td class=tr-caption style="text-align: center">The Kinect SDK offers depth data and skeletal data</td></tr></tbody></table><p>Hayden has created an OSC server on the graphics server to pick up the skeletal data points from the network. He then set up a test person renderer (the stick man on the wall, above) in VVVV so that we can fine-tune the Kinect interaction before passing data on to the real 3D patches.</p><p>Data from the 3D scene are in turn formatted and broadcast across the network as more OSC messages, for the sound environment to pick up. The result will be an audio/visual environment which can be controlled solely by moving in front of the Kinects.</p></div><a class=read-more href=/blog/kinecting-to-network>Read more &gt;</a><hr class=separator></article><ul id=paging class=cta><li><a href="/blog/page7/">&lt; Previous page</a></li><li><a href="/blog/page9/">Next page &gt;</a></li></ul></main><aside class=right><nav><header><h1>All posts</h1></header><h2 id=2015-ref>2015</h2><ul><li><a href=/blog/new-site-for-hardware-hack-lab>New Site for Hardware Hack Lab</a></li></ul><h2 id=2014-ref>2014</h2><ul><li><a href=/blog/sound-control-at-future-interfaces>Sound Control at Future Interfaces</a></li><li><a href=/blog/projection-masking-not-projection>Projection Masking, not Projection Mapping</a></li><li><a href=/blog/addon-for-openframeworks-kinect-v2-and>Addon for openFrameworks, Kinect V2 and Mac</a></li><li><a href=/blog/john-cleese-on-creativity>John Cleese on Creativity</a></li><li><a href=/blog/takeaways-from-eyeo-2014>Takeaways from Eyeo 2014</a></li><li><a href=/blog/hardware-hacker-culture-of-new-york>Hardware Hacker Culture of New York</a></li><li><a href=/blog/future-visions-for-human-interaction>Future Visions for Human Interaction</a></li><li><a href=/blog/openbci-nears-its-kickstarter-goal>OpenBCI Nears it's Kickstarter Goal</a></li></ul><h2 id=2013-ref>2013</h2><ul><li><a href=/blog/sound-chamber-2013>Sound Chamber (2013)</a></li><li><a href=/blog/openbci-hackathon-at-thoughtworks>OpenBCI Hackathon at ThoughtWorks</a></li><li><a href=/blog/exploring-depth-video-at-culturehub>Exploring Depth Video at CultureHub</a></li><li><a href=/blog/introduction-to-ibeacons>Introduction to iBeacons</a></li><li><a href=/blog/volumetric-lab-at-culturehub-nyc>Volumetric Lab at CultureHub NYC</a></li><li><a href=/blog/randomness-in-algorithm>Randomness in the Algorithm</a></li><li><a href=/blog/study-existential>Video: Existential</a></li><li><a href=/blog/the-visual-art-of-brian-eno>The Visual Art of Brian Eno</a></li><li><a href=/blog/rgbdtoolkit-sketch-at-sampler>RGBDToolkit Sketch at The Sampler</a></li><li><a href=/blog/the-artist-geek-hybrid>The Artist-Geek Hybrid</a></li><li><a href=/blog/rgbdtoolkit-calibration-tutorial>RGBDToolkit Calibration Tutorial</a></li><li><a href=/blog/how-depth-sensor-works-in-5-minutes>How a Depth Sensor Works - in 5 Minutes</a></li><li><a href=/blog/focal-lengths-and-camera-sensors>Focal Lengths and Camera Sensors</a></li><li><a href=/blog/rgbdtoolkit-visualizer-tutorial>RGBDToolkit Visualizer Tutorial</a></li><li><a href=/blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit Workshop at Eyebeam</a></li><li><a href=/blog/nosql-distilled-to-keynote>NoSQL Distilled to a Keynote!</a></li><li><a href=/blog/paper-prototyping>Paper Prototyping</a></li><li><a href=/blog/git-vs-github>Is Git the Same Thing as Github!?</a></li></ul><h2 id=2012-ref>2012</h2><ul><li><a href=/blog/counterpoint-to-remote>Counterpoint to the Remote</a></li><li><a href=/blog/the-cascading-process>The Cascading Process</a></li><li><a href=/blog/working-with-total-space>Working with Total Space</a></li><li><a href=/blog/residency-begins-at-cac-troy>Residency Begins at CAC Troy</a></li><li><a href=/blog/installation-sketch-at-open-studios>Installation Sketch at Open Studios</a></li><li><a href=/blog/roman-moshenskys-mirror-world>Roman Moshensky's Mirror World</a></li><li><a href=/blog/open-studios-at-i-park>Open Studios at I-Park</a></li><li><a href=/blog/perception-as-creative-process>Perception as a Creative Process</a></li><li><a href=/blog/the-i-park-graveyard>The I-Park Graveyard</a></li><li><a href=/blog/scoping-out-land>Scoping Out the Land</a></li><li><a href=/blog/residency-begins-at-i-park>Residency Begins at I-Park</a></li><li><a href=/blog/residency-at-contemporary-artists-center>Residency at Contemporary Artists Center</a></li><li><a href=/blog/the-shopping-list-for-projection-bombing>First Shopping List for Projection-Bombing</a></li><li><a href=/blog/portable-projection-in-rural-context>Portable Projection in a Rural Context</a></li><li><a href=/blog/stephen-lumentas-sc-textmate-bundle>Stephen Lumenta's SC TextMate Bundle</a></li><li><a href=/blog/adding-openframeworks-addons>Adding OF Addons (ofxSuperCollider)</a></li><li><a href=/blog/setting-up-supercollider-with-textmate>Setting up SuperCollider with TextMate</a></li><li><a href=/blog/switching-to-macbook-pro>Switching to MacBook Pro</a></li><li><a href=/blog/quickref-for-supercollider>QuickRef for SuperCollider</a></li><li><a href=/blog/getting-started-with-supercollider>Getting Started with SuperCollider</a></li><li><a href=/blog/getting-started-with-openframeworks-in>Getting Started with OpenFrameworks</a></li><li><a href=/blog/overtones-harmonics-and-additive>Overtones, Harmonics and Additive Synthesis</a></li><li><a href=/blog/visit-to-cold-spring>Visit to Cold Spring</a></li><li><a href=/blog/i-park-residency>Residency at I-Park</a></li><li><a href=/blog/light-waves>Light Waves</a></li></ul><h2 id=2011-ref>2011</h2><ul><li><a href=/blog/final-exhibition>The Final Exhibition</a></li><li><a href=/blog/playing-with-particles>Playing with Particles</a></li><li><a href=/blog/responsive-granular-sound>Responsive Granular Sound</a></li><li><a href=/blog/kinecting-to-network>Kinecting to the Network</a></li><li><a href=/blog/first-working-day>First Working Day</a></li><li><a href=/blog/designs-for-freemote>Designs for Freemote</a></li><li><a href=/blog/freemote-utrecht>Freemote Utrecht</a></li><li><a href=/blog/untitled-picture-this-2011>Untitled - Picture This (2011)</a></li><li><a href=/blog/wider-context>The Wider Context?</a></li><li><a href=/blog/trading-time-for-space>Trading Time for Space</a></li><li><a href=/blog/talk-at-goldsmiths-digital-studios>Talk at Goldsmiths Digital Studios</a></li><li><a href=/blog/intro-to-marius-watz>Intro to Marius Watz</a></li><li><a href=/blog/practical-guide-to-generative-art>Practical Guide to Generative Art</a></li><li><a href=/blog/installation-at-alpha-ville>Installation at Alpha-Ville</a></li><li><a href=/blog/simple-harmonic-motion>Simple Harmonic Motion</a></li><li><a href=/blog/jaaga-journal-features>Jaaga Journal Features</a></li><li><a href=/blog/gravity-2011>Gravity (2011)</a></li><li><a href=/blog/reflections-2011>Reflections (2011)</a></li><li><a href=/blog/jaaga-sound-lights>Jaaga Sound & Lights</a></li><li><a href=/blog/two-works-for-jaaga-gravity-and-memory>Two Works for Jaaga: Gravity and Reflections</a></li><li><a href=/blog/cosm-collision-detection-and-volume>Cosm, Collision Detection and Volume</a></li><li><a href=/blog/vector-base-amplitude-panning>Vector-Base Amplitude Panning</a></li><li><a href=/blog/intuition-and-direction-of-project>Intuition, and Direction of the Project</a></li><li><a href=/blog/reflections-what-is-jaaga>Reflections: What is Jaaga?</a></li><li><a href=/blog/going-further-with-ambisonics>Going Further with Ambisonics</a></li><li><a href=/blog/introduction-to-ambisonics>Introduction to Ambisonics</a></li><li><a href=/blog/surface-light-sound-installation>Surface (2010)</a></li><li><a href=/blog/running-servo-motor>Servo Motors and Transistors</a></li><li><a href=/blog/spinning-12v-dc-motor>Spinning a 12V DC Motor</a></li><li><a href=/blog/spinning-dc-motor>Spinning a 5V DC Motor</a></li><li><a href=/blog/first-week-at-jaaga>First Week at Jaaga</a></li><li><a href=/blog/presentation-style>Presentation Style</a></li><li><a href=/blog/beginning-jaaga-fellowship>Beginning the Jaaga Fellowship</a></li><li><a href=/blog/brian-eno-role-models-and-direction>Brian Eno, Role Models and Direction</a></li><li><a href=/blog/hype-cycle_17>The Hype Cycle</a></li></ul><h2 id=2010-ref>2010</h2><ul><li><a href=/blog/working-with-3d-space>Working with 3D Space</a></li><li><a href=/blog/technology-and-luck>Technology and Luck</a></li><li><a href=/blog/gallery-types-and-commercial-gallery>Different Types of Gallery</a></li><li><a href=/blog/jason-bruges-studio>Jason Bruges Studio</a></li><li><a href=/blog/unstable-empathy-trust-and>Unstable Empathy & Gaining Trust</a></li><li><a href=/blog/chris-o-two-creative-cultures>Chris O'Shea & Two Creative Cultures</a></li></ul></nav></aside></div><script src=/javascript/script.e2be.js></script></body></html>