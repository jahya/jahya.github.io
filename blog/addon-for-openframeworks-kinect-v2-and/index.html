<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1.0"><meta name=author content="Andrew McWilliams"><title>Addon for openFrameworks, Kinect V2 and Mac</title><link href=http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css rel=stylesheet><link href="http://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic" rel=stylesheet type=text/css><link rel=stylesheet href=/stylesheets/style.83cf.css><!--[if lt IE 9]>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv-printshiv.min.js"></script>
    <![endif]--><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.4f09.png><link rel="shortcut icon" href=/images/favicon.6537.png></head><body id=blog class=content><header role=banner><h1><a href="/"><span class=shift>Andrew</span> M<span class=sub>c</span>Williams</a></h1><nav role=navigation><h2>Navigation</h2><a id=menu-button href=#><i class="fa fa-bars"></i> <i class="fa fa-times"></i></a><ul class=cta><li><a href="/about/">About</a></li><li><a href="/works/">Works</a></li><li><a href="/blog/">Blog</a></li></ul></nav></header><div class=flex-row><main class=left><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1>Addon for openFrameworks, Kinect V2 and Mac</h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2014-10-04>Saturday 4 October, 2014</time><ul class=social><li><a href="http://twitter.com/share?text=Addon%20for%20openFrameworks,%20Kinect%20V2%20and%20Mac&amp;url=http://jahya.net/blog/addon-for-openframeworks-kinect-v2-and&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Addon%20for%20openFrameworks,%20Kinect%20V2%20and%20Mac&amp;u=http://jahya.net/blog/addon-for-openframeworks-kinect-v2-and"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Addon%20for%20openFrameworks,%20Kinect%20V2%20and%20Mac&amp;u=http://jahya.net/blog/addon-for-openframeworks-kinect-v2-and"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>The <a href=http://www.microsoft.com/en-us/kinectforwindows/purchase/default.aspx>Kinect V2 depth sensor</a> is now widely available, but bound to Windows. I've <a href=https://github.com/microcosm/ofxKinectV2-OSC>written an addon</a> which simplifies getting Kinect V2 skeletal data onto a Mac, so you can manipulate it in openFrameworks.</p><figure><a href=http://1.bp.blogspot.com/-kZMUmluha3o/VDA8aUxGWfI/AAAAAAAAFqk/xDq3R5AEPPc/s1600/screenshot-win.png><img alt="A new Kinect V2 sensor" src=http://1.bp.blogspot.com/-a8mcD9T1I6s/VDA0QwTjYVI/AAAAAAAAFqU/ni-nXPCDOwU/s800/kinect.jpg></a></figure><p>You'll need two computers - a Windows and a Mac - and a network connection between them. You won't have to write any code in Windows, just run a utility.</p><!--excerpt-ends--><p>Then just launch an openFrameworks project on your Mac, include the addon, and you will have access to a simple template for rendering the skeletal data in realtime.</p><p><strong>A bit of background</strong><br>The problem this addon tries to solve is that the Kinect V2 really needs to run on Windows 8.1 (and using a USB3 port). If you develop on a Mac, you have to <a href=http://blogs.msdn.com/b/kinectforwindows/archive/2014/07/28/developing-with-kinect-v2-on-a-mac.aspx>switch your operating system</a> - it's the only way.</p><p>But it's not even that simple - let's say you switch to Windows, and want to use openFrameworks. You then have to learn <a href="http://www.visualstudio.com/">Visual Studio</a>, which is really is <a href=//blog/adding-openframeworks-addons>not the most appealing</a> to the openFrameworks community.</p><p>Even if you do that, the solutions available to get sensor data into openFrameworks are patchy and immature at best. So you might be better off learning <a href="http://msdn.microsoft.com/en-us/library/aa970268(v=vs.110).aspx">WPF / C#</a>, or <a href=http://en.wikipedia.org/wiki/DirectX>DirectX</a> / C++ so you can work with the well-supported managed APIs.</p><p>That's a lot to learn. And it ties you even further into Windows-only code.</p><p><strong>A simpler solution</strong><br>If you only need skeletal and gesture data, there's a simpler way. The addon <a href=https://github.com/microcosm/ofxKinectV2-OSC>ofxKinectV2-OSC</a> helps you get realtime skeletal data from the V2 sensor into openFrameworks on your Mac.</p><p>The way it works is this. You have a Windows machine and a Mac, running side-by-side on the same network. On the Windows machine you download and run <a href=https://github.com/microcosm/KinectV2-OSC>this simple utility</a> (written in WPF). It broadcasts all the skeletal information over the network using <a href=http://opensoundcontrol.org/introduction-osc>OSC</a>.</p><p>It sends all information every frame, so you don't have to configure it or edit code.</p><figure><a href=http://1.bp.blogspot.com/-kZMUmluha3o/VDA8aUxGWfI/AAAAAAAAFqk/xDq3R5AEPPc/s1600/screenshot-win.png><img alt="The broadcasting utility running on Windows" src=http://1.bp.blogspot.com/-kZMUmluha3o/VDA8aUxGWfI/AAAAAAAAFqk/xDq3R5AEPPc/s500/screenshot-win.png></a><figcaption class=caption>The <a href=https://github.com/microcosm/KinectV2-OSC>broadcasting utility</a> running on Windows</figcaption></figure><p>Over on the Mac, you can clone <a href=https://github.com/microcosm/ofxKinectV2-OSC>ofxKinectV2-OSC</a> into your <code>addons</code> directory, and it will read in all the OSC data and use it to populate an object model. You can then query that object model as you wish:</p><p class=code><code>&nbsp;&nbsp;ofxKinectV2OSC kinect;<br><br>&nbsp;&nbsp;void setup() {<br>&nbsp;&nbsp;&nbsp;&nbsp;kinect.setup(PORT_NUMBER);<br>&nbsp;&nbsp;}<br><br>&nbsp;&nbsp;//This draws the left hand of each skeleton to the screen<br>&nbsp;&nbsp;void draw() {<br>&nbsp;&nbsp;&nbsp;&nbsp;vector&lt;Skeleton&gt;&#42; skeletons = kinect.getSkeletons();<br><br>&nbsp;&nbsp;&nbsp;&nbsp;for(int i = 0; i &lt; skeletons-&gt;size(); i++) {<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Skeleton* skeleton = &skeletons-&gt;at(i);<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Joint handLeft = skeleton-&gt;getHandLeft();<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ofCircle(handLeft.getPoint(), 25);<br>&nbsp;&nbsp;&nbsp;&nbsp;}<br>&nbsp;&nbsp;}</code></p><p>That's it. You can get the 3-dimensional position of all joints, the tracking state of each joint (Tracked, NotTracked or Inferred), and the hand open/closed status of each hand.</p><p><strong>Using the included example code</strong><br>The addon comes with <a href=https://github.com/microcosm/ofxKinectV2-OSC/tree/master/example/src>example code</a>, and a <code>BodyRenderer</code> class you can edit to draw the skeleton how you like:</p><figure><a href=http://1.bp.blogspot.com/-Zs8VVJEEBAI/VDA8aWy4WiI/AAAAAAAAFqo/5E59eKoPwZs/s1600/screenshot-mac.png><img alt="The addon example code running on Mac" src=http://1.bp.blogspot.com/-Zs8VVJEEBAI/VDA8aWy4WiI/AAAAAAAAFqo/5E59eKoPwZs/s500/screenshot-mac.png></a><figcaption class=caption>The addon example code running on Mac</figcaption></figure><p>This screengrab shows the skeleton being rendered in openFrameworks, with confident bones in thick white and less-confident bones in thin gray. One hand is open (green), and the other is closed (red).</p><p><strong>Going forward</strong><br>There are a huge number of gestures the Kinect V2 SDK supports, plus vendor addons, and these can all be added going forward. I'm open to collaborations / pull requests to develop the addon and utility further.</p><p>For those of you in New York, we will be experimenting with this further <a href=//blog/hardware-hacker-culture-of-new-york>over in the lab</a> on Wednesdays. Join <a href="http://www.meetup.com/volumetric/">the meetup</a> if you want to come and try out the gear for yourself.</p></div></article><ul id=paging class=cta><li><a href=/blog/john-cleese-on-creativity>&lt; Previous post</a></li><li><a href=/blog/projection-masking-not-projection>Next post &gt;</a></li></ul></main><aside class=right><nav><header><h1>All posts</h1></header><h2 id=2015-ref>2015</h2><ul><li><a href=/blog/new-site-for-hardware-hack-lab>New Site for Hardware Hack Lab</a></li></ul><h2 id=2014-ref>2014</h2><ul><li><a href=/blog/sound-control-at-future-interfaces>Sound Control at Future Interfaces</a></li><li><a href=/blog/projection-masking-not-projection>Projection Masking, not Projection Mapping</a></li><li><a href=/blog/addon-for-openframeworks-kinect-v2-and>Addon for openFrameworks, Kinect V2 and Mac</a></li><li><a href=/blog/john-cleese-on-creativity>John Cleese on Creativity</a></li><li><a href=/blog/takeaways-from-eyeo-2014>Takeaways from Eyeo 2014</a></li><li><a href=/blog/hardware-hacker-culture-of-new-york>Hardware Hacker Culture of New York</a></li><li><a href=/blog/future-visions-for-human-interaction>Future Visions for Human Interaction</a></li><li><a href=/blog/openbci-nears-its-kickstarter-goal>OpenBCI Nears it's Kickstarter Goal</a></li></ul><h2 id=2013-ref>2013</h2><ul><li><a href=/blog/sound-chamber-2013>Sound Chamber (2013)</a></li><li><a href=/blog/openbci-hackathon-at-thoughtworks>OpenBCI Hackathon at ThoughtWorks</a></li><li><a href=/blog/exploring-depth-video-at-culturehub>Exploring Depth Video at CultureHub</a></li><li><a href=/blog/introduction-to-ibeacons>Introduction to iBeacons</a></li><li><a href=/blog/volumetric-lab-at-culturehub-nyc>Volumetric Lab at CultureHub NYC</a></li><li><a href=/blog/randomness-in-algorithm>Randomness in the Algorithm</a></li><li><a href=/blog/study-existential>Video: Existential</a></li><li><a href=/blog/the-visual-art-of-brian-eno>The Visual Art of Brian Eno</a></li><li><a href=/blog/rgbdtoolkit-sketch-at-sampler>RGBDToolkit Sketch at The Sampler</a></li><li><a href=/blog/the-artist-geek-hybrid>The Artist-Geek Hybrid</a></li><li><a href=/blog/rgbdtoolkit-calibration-tutorial>RGBDToolkit Calibration Tutorial</a></li><li><a href=/blog/how-depth-sensor-works-in-5-minutes>How a Depth Sensor Works - in 5 Minutes</a></li><li><a href=/blog/focal-lengths-and-camera-sensors>Focal Lengths and Camera Sensors</a></li><li><a href=/blog/rgbdtoolkit-visualizer-tutorial>RGBDToolkit Visualizer Tutorial</a></li><li><a href=/blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit Workshop at Eyebeam</a></li><li><a href=/blog/nosql-distilled-to-keynote>NoSQL Distilled to a Keynote!</a></li><li><a href=/blog/paper-prototyping>Paper Prototyping</a></li><li><a href=/blog/git-vs-github>Is Git the Same Thing as Github!?</a></li></ul><h2 id=2012-ref>2012</h2><ul><li><a href=/blog/counterpoint-to-remote>Counterpoint to the Remote</a></li><li><a href=/blog/the-cascading-process>The Cascading Process</a></li><li><a href=/blog/working-with-total-space>Working with Total Space</a></li><li><a href=/blog/residency-begins-at-cac-troy>Residency Begins at CAC Troy</a></li><li><a href=/blog/installation-sketch-at-open-studios>Installation Sketch at Open Studios</a></li><li><a href=/blog/roman-moshenskys-mirror-world>Roman Moshensky's Mirror World</a></li><li><a href=/blog/open-studios-at-i-park>Open Studios at I-Park</a></li><li><a href=/blog/perception-as-creative-process>Perception as a Creative Process</a></li><li><a href=/blog/the-i-park-graveyard>The I-Park Graveyard</a></li><li><a href=/blog/scoping-out-land>Scoping Out the Land</a></li><li><a href=/blog/residency-begins-at-i-park>Residency Begins at I-Park</a></li><li><a href=/blog/residency-at-contemporary-artists-center>Residency at Contemporary Artists Center</a></li><li><a href=/blog/the-shopping-list-for-projection-bombing>First Shopping List for Projection-Bombing</a></li><li><a href=/blog/portable-projection-in-rural-context>Portable Projection in a Rural Context</a></li><li><a href=/blog/stephen-lumentas-sc-textmate-bundle>Stephen Lumenta's SC TextMate Bundle</a></li><li><a href=/blog/adding-openframeworks-addons>Adding OF Addons (ofxSuperCollider)</a></li><li><a href=/blog/setting-up-supercollider-with-textmate>Setting up SuperCollider with TextMate</a></li><li><a href=/blog/switching-to-macbook-pro>Switching to MacBook Pro</a></li><li><a href=/blog/quickref-for-supercollider>QuickRef for SuperCollider</a></li><li><a href=/blog/getting-started-with-supercollider>Getting Started with SuperCollider</a></li><li><a href=/blog/getting-started-with-openframeworks-in>Getting Started with OpenFrameworks</a></li><li><a href=/blog/overtones-harmonics-and-additive>Overtones, Harmonics and Additive Synthesis</a></li><li><a href=/blog/visit-to-cold-spring>Visit to Cold Spring</a></li><li><a href=/blog/i-park-residency>Residency at I-Park</a></li><li><a href=/blog/light-waves>Light Waves</a></li></ul><h2 id=2011-ref>2011</h2><ul><li><a href=/blog/final-exhibition>The Final Exhibition</a></li><li><a href=/blog/playing-with-particles>Playing with Particles</a></li><li><a href=/blog/responsive-granular-sound>Responsive Granular Sound</a></li><li><a href=/blog/kinecting-to-network>Kinecting to the Network</a></li><li><a href=/blog/first-working-day>First Working Day</a></li><li><a href=/blog/designs-for-freemote>Designs for Freemote</a></li><li><a href=/blog/freemote-utrecht>Freemote Utrecht</a></li><li><a href=/blog/untitled-picture-this-2011>Untitled - Picture This (2011)</a></li><li><a href=/blog/wider-context>The Wider Context?</a></li><li><a href=/blog/trading-time-for-space>Trading Time for Space</a></li><li><a href=/blog/talk-at-goldsmiths-digital-studios>Talk at Goldsmiths Digital Studios</a></li><li><a href=/blog/intro-to-marius-watz>Intro to Marius Watz</a></li><li><a href=/blog/practical-guide-to-generative-art>Practical Guide to Generative Art</a></li><li><a href=/blog/installation-at-alpha-ville>Installation at Alpha-Ville</a></li><li><a href=/blog/simple-harmonic-motion>Simple Harmonic Motion</a></li><li><a href=/blog/jaaga-journal-features>Jaaga Journal Features</a></li><li><a href=/blog/gravity-2011>Gravity (2011)</a></li><li><a href=/blog/reflections-2011>Reflections (2011)</a></li><li><a href=/blog/jaaga-sound-lights>Jaaga Sound & Lights</a></li><li><a href=/blog/two-works-for-jaaga-gravity-and-memory>Two Works for Jaaga: Gravity and Reflections</a></li><li><a href=/blog/cosm-collision-detection-and-volume>Cosm, Collision Detection and Volume</a></li><li><a href=/blog/vector-base-amplitude-panning>Vector-Base Amplitude Panning</a></li><li><a href=/blog/intuition-and-direction-of-project>Intuition, and Direction of the Project</a></li><li><a href=/blog/reflections-what-is-jaaga>Reflections: What is Jaaga?</a></li><li><a href=/blog/going-further-with-ambisonics>Going Further with Ambisonics</a></li><li><a href=/blog/introduction-to-ambisonics>Introduction to Ambisonics</a></li><li><a href=/blog/surface-light-sound-installation>Surface (2010)</a></li><li><a href=/blog/running-servo-motor>Servo Motors and Transistors</a></li><li><a href=/blog/spinning-12v-dc-motor>Spinning a 12V DC Motor</a></li><li><a href=/blog/spinning-dc-motor>Spinning a 5V DC Motor</a></li><li><a href=/blog/first-week-at-jaaga>First Week at Jaaga</a></li><li><a href=/blog/presentation-style>Presentation Style</a></li><li><a href=/blog/beginning-jaaga-fellowship>Beginning the Jaaga Fellowship</a></li><li><a href=/blog/brian-eno-role-models-and-direction>Brian Eno, Role Models and Direction</a></li><li><a href=/blog/hype-cycle_17>The Hype Cycle</a></li></ul><h2 id=2010-ref>2010</h2><ul><li><a href=/blog/working-with-3d-space>Working with 3D Space</a></li><li><a href=/blog/technology-and-luck>Technology and Luck</a></li><li><a href=/blog/gallery-types-and-commercial-gallery>Different Types of Gallery</a></li><li><a href=/blog/jason-bruges-studio>Jason Bruges Studio</a></li><li><a href=/blog/unstable-empathy-trust-and>Unstable Empathy & Gaining Trust</a></li><li><a href=/blog/chris-o-two-creative-cultures>Chris O'Shea & Two Creative Cultures</a></li></ul></nav></aside></div><script src=/javascript/script.e2be.js></script></body></html>