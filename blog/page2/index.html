<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1.0"><meta name=author content="Andrew McWilliams"><title>Blog - Andrew McWilliams</title><link href=http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css rel=stylesheet><link href="http://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic" rel=stylesheet type=text/css><link rel=stylesheet href=/stylesheets/style.83cf.css><!--[if lt IE 9]>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv-printshiv.min.js"></script>
    <![endif]--><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.4f09.png><link rel="shortcut icon" href=/images/favicon.6537.png></head><body id=blog class=content><header role=banner><h1><a href="/"><span class=shift>Andrew</span> M<span class=sub>c</span>Williams</a></h1><nav role=navigation><h2>Navigation</h2><a id=menu-button href=#><i class="fa fa-bars"></i> <i class="fa fa-times"></i></a><ul class=cta><li><a href="/about/">About</a></li><li><a href="/works/">Works</a></li><li><a href="/blog/">Blog</a></li></ul></nav></header><div class=flex-row><main class=left><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/future-visions-for-human-interaction>Future Visions for Human Interaction</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2014-02-10>Monday 10 February, 2014</time><ul class=social><li><a href="http://twitter.com/share?text=Future%20Visions%20for%20Human%20Interaction&amp;url=http://jahya.net/blog/future-visions-for-human-interaction&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Future%20Visions%20for%20Human%20Interaction&amp;u=http://jahya.net/blog/future-visions-for-human-interaction"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Future%20Visions%20for%20Human%20Interaction&amp;u=http://jahya.net/blog/future-visions-for-human-interaction"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>Over the course of an hour at ThoughtWorks last week Ken Perlin described a vision for the future of immersive human interaction.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><iframe width=500 height=281 src=//www.youtube.com/embed/nHBAGke3eCU></iframe></td></tr></table><p>It was rich and varied subject matter, and it drew a line from Ken's early inspirations right through current research and beyond.</p><p>The Internet Society's Joly MacFie was on hand to film it (above), but I'll summarize the thrust of the argument here so you can get a sense. I also want to sprinkle in a few of my own comments and reactions.</p><p><strong>What this talk is really about</strong><br>Over the course of the hour, Ken weaves through a range of subjects including narrative, immersion, imagination, creativity, shorthand pop culture references to the 'future', and human nature - already a lot for one hour.</p><p>Add to that a range of technology subjects - wearables, implants, depth &amp; holography, virtual &amp; augmented reality, machine learning, kinematics and software programming - and you find yourself with plenty of rabbit holes to go down.</p><p>However, the real vision is all here in this 1-minute clip. In the video below, Professor Whoopee helpfully explains the functioning of a CRT using his 3DBB - his 3-dimensional blackboard (ofcourse). Check it out, and particularly watch how Professor Whoopee uses the 3DBB to communicate and interact with the other characters around him:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><iframe width=420 height=315 src=//www.youtube.com/embed/D3OSTflMO80></iframe></td></tr></table><p>The 3DBB is like an immersive environment, in which Professor Whoopee can create and operate a virtual, functioning CRT, which he and the other characters around him can all have a shared, volumetric experience of.</p><p>Bearing in mind that context, have a look through a transcript of Ken's closing statements to the audience last Tuesday:</p><p><blockquote>"Eventually, when you and I are face-to-face in an augmented version of reality, and we have nothing but our bodies and our eyes and our hands and each other... then we'll be able to use these very very simple techniques, because we've understood the semantics of how I create something for you in realtime. And what I can offer is this library [of intuitively instantiated, intelligent and directable, interactive yet autonomous virtual objects]..."</blockquote></p><p><blockquote>"Then the virtual world we have between us becomes something that is not just a replication of our physical reality, but actually the kind of reality we'd really like to be in"</blockquote></p><p>In this vision, we are able to spontaneously create and manipulate logically-consistent virtual objects, or we might say 'directable actors' since they are semi-autonomous. These object/actors surrounding us are not impositions on physical reality. As a consequence of immersion, they are indistinguishable, an effectively inseparable component of reality itself.</p><p>The actor/objects are viewable and interoperable in the space between multiple people, similar to how we currently imagine future holographic interfaces.</p><p>But we don't interact with these objects by using a portion of space which we would identifiably define as being an interface. The net effect of immersion combined with shared experience is that the virtual-physical reality we inhabit precludes the need for an interface.</p><p>All of it, or rather none of it is the interface.</p><p><strong>This versus other visions</strong><br>I think a lot of people will find that hard to imagine. One way to try and imagine it is to contrast it with other visions of the future.</p><p>In his <a href=http://worrydream.com/#!/ABriefRantOnTheFutureOfInteractionDesign>Brief Rant on the Future of Interaction Design</a>, Bret Victor takes a decent swipe at the vision presented in this video:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><iframe width=500 height=281 src=//www.youtube.com/embed/a6cNdhOKwi0></iframe></td></tr></table><p>Bret's criticism is to note that the characters in such visions are immersed in an experience which can take on any shape or form (so long as that form blends the possible characteristics of virtual and physical worlds).</p><p>And yet to interact with this immersive reality, they turn to their hands and manipulate a little virtual 'phone'.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><a href=""><img alt="" src=http://2.bp.blogspot.com/-PeGlFGKC1s4/Uva55r4cSoI/AAAAAAAAE-8/EvD86sJcLGU/s250/interaction1.jpg width=250 height=187></a><tr><td class=tr-caption style="text-align: center">Interacting with a contrived interface (<a href=http://worrydream.com/#!/ABriefRantOnTheFutureOfInteractionDesign>credit</a>)</td></tr></td></tr></table><p>It doesn't make sense.</p><p>A real, modern-day phone is like a glass window which you can swipe and prod, and there isn't a great deal of immersive haptic feedback.</p><p>Compare that to your intuitive sense of your place in a book by the relative density of pages in each hand. Or the amount of water by the shift in weight distribution as you tilt a glass.</p><table align=center class=tr-caption-container style="margin-left: auto; margin-right:auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://3.bp.blogspot.com/-w3PQCe9EiGg/TuFNc6POW0I/AAAAAAAAATU/l7Z73pIRbn8/s1600/SL372378.JPG style="margin-left: 1em; margin-right: 1em"><img height=150 src=http://3.bp.blogspot.com/-KuU9iPMN7O8/Uva55pfjr8I/AAAAAAAAE_E/dgqbmKN-tJU/s200/interaction3.jpg width=200 alt="Interacting with a book"></a>&nbsp;&nbsp;&nbsp;&nbsp;<a href=http://4.bp.blogspot.com/-VWoX7sjxtn0/TuFNnXRMqqI/AAAAAAAAATc/9YcTiCHaVrs/s1600/SL372383.JPG style="margin-left: 1em; margin-right: 1em"><img height=150 src=http://4.bp.blogspot.com/-CmM-VzIYxE0/Uva55uV8p5I/AAAAAAAAE_A/WIRcuZ5nKoQ/s200/interaction2.jpg width=200 alt="Interacting with a glass"></a></td></tr><tr><td class=tr-caption style="text-align: center">Interacting with seamless, integral interfaces (<a href=http://worrydream.com/#!/ABriefRantOnTheFutureOfInteractionDesign>credit</a>)</td></tr></tbody></table><p>Natural human interactions are richly physical, and both Ken and Victor point out that we are currently going through a very odd transitional stage - walking around with our heads facing down, glued by our eyes and fingers to screens. Visions for future human interaction should allow that as soon as our dependency on physical devices for virtual interaction goes, so too goes that framework of interaction.</p><p><strong>If no interface, then what?</strong><br>Ken's answer is that the spontaneous creation of shared virtual actors will become a new staple of human communication, in much the same way that the ability to instantly communicate by video with people the other side of the world became a staple of human communication.</p><p>These actors will be scriptable and responsive to their environment, much like real actors on a stage. But hang on - we've already seen this type of thing recently haven't we?</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><iframe width=500 height=281 src=//www.youtube.com/embed/vv5uI2vlXE8></iframe></td></tr></table><p>The big difference between this and Ken's vision is that in the PS4 you enter into a contrived interaction with a specific subset of actors and scenarios. Because the hardware is not a seamless, integral element of your everyday experience, the user-experience case for being able to instantiate your own actors at will is much weaker.</p><p>This begs another question. If the interactions in Ken's vision are not to be contrived, doesn't this rely on each individual user to craft and nurture their own individual libraries of symbolic 'actors'? Or do most people subscribe to commercially available 'packs' of actors, and combine them to curate their own unique library?</p><p>Questions start flooding in. In what way does a library contribute to, or become reflective of, a person's personality? And tangentially, if these actors can't provide haptic feedback, are they really so different from the Minority Report interface?</p><p>Perhaps a further modification. What if it is the ability to transfer virtual characteristics to physical objects, and have those physical objects respond in a haptically-meaningful way that will provide the most engaging experience?</p><p><strong>Yes, great conjecture - but is any of this even possible?</strong><br>Ken spends most of his talk explaining how far he and his students have come, and how he expects technology to develop in the near future. I'd recommend watching if any of this interests you.</p><p>We will keep an eye on Ken's work over at Volumetric and catch up with him again to explore these questions in the future.</p></div><a class=read-more href=/blog/future-visions-for-human-interaction>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/openbci-nears-its-kickstarter-goal>OpenBCI Nears it's Kickstarter Goal</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2014-01-07>Tuesday 7 January, 2014</time><ul class=social><li><a href="http://twitter.com/share?text=OpenBCI%20Nears%20it's%20Kickstarter%20Goal&amp;url=http://jahya.net/blog/openbci-nears-its-kickstarter-goal&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=OpenBCI%20Nears%20it's%20Kickstarter%20Goal&amp;u=http://jahya.net/blog/openbci-nears-its-kickstarter-goal"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=OpenBCI%20Nears%20it's%20Kickstarter%20Goal&amp;u=http://jahya.net/blog/openbci-nears-its-kickstarter-goal"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>With just two weeks to go, <a href="http://www.openbci.com/">OpenBCI</a> is closing in on it's $100,000 <a href=http://www.kickstarter.com/projects/openbci/openbci-an-open-source-brain-computer-interface-fo>Kickstarter</a> target.</p><div style="text-align: center"><iframe frameborder=0 height=280 src=http://www.youtube.com/embed/a55_EfFtysc title="YouTube video player" width=500></iframe></div><p>They've been featured in <a href="http://www.forbes.com/sites/reuvencohen/2014/01/03/new-open-source-platform-allows-anyone-to-hack-brain-waves/">Forbes</a>, <a href=http://www.fastcolabs.com/3023498/mind-controlled-computing-goes-open-source>Fast Company</a>, <a href=http://m.cnet.com.au/diy-mind-control-interface-on-kickstarter-339346215.htm>CNet</a>, and ofcourse the ultimate accolade - they've worked with us over at <a href="http://hardwarehacklab.tumblr.com/">Hardware Hack Lab</a>!</p><p>In the run-up to the hackathon we hosted at ThoughtWorks I quizzed Conor on the purpose of OpenBCI. I think his answers bear repeating here.</p><p>First, what is OpenBCI?</p><blockquote>"OpenBCI is a new open-source initiative. Our mission is to promote brain-computer interface (BCI) research in a transparent atmosphere, by putting the technology in the hands of the people. We have built a BCI prototyping platform that is entirely open and supported by a growing community of hardware and software engineers and makers."</blockquote><p>Why OpenBCI, why now?</p><blockquote>"OpenBCI has no proprietary algorithms. The barrier of entry is slightly higher than existing commercial BCIs because the software frameworks are still in their infancy. That said, as the open-source community adopts OpenBCI, we hope that the barrier of entry rapidly broadens, allowing makers of all skill levels to begin doing research and development surrounding the human brain and body."</blockquote><p>Can you explain the differences between what you are doing and what <a href=http://en.wikipedia.org/wiki/Comparison_of_consumer_brain%E2%80%93computer_interfaces#Open-source_projects>other open source players</a> are doing, for example <a href="http://openeeg.sourceforge.net/">OpenEEG</a>?</p><blockquote>"Primarily, we are trying to lower the barrier of entry into research-grade EEG. OpenEEG is an amazing initiative, but the platform can be a bit intimidating to newcomers. Most contributors to the OpenEEG project are well-versed in electrical engineering and have the aptitude and/or training to get down and dirty with datasheets and circuit diagrams. With OpenBCI we are looking to provide the open-source community with more flexibility than commercial devices like NeuroSky &amp; Emotiv (which have fixed electrode configurations), while at the same time keeping the barrier of entry low."</blockquote><p>Go on.</p><blockquote>"In a sense, we want to create the Arduino of BCIs. Arduino made electronics prototyping easy and accessible for everyone from electrical engineers to grade school kids. It's our hope that OpenBCI does the same thing, but for synthesizing the digital signals of the human body. The OpenBCI hardware that is going to be kickstarted will most likely have an integrated microchip, making OpenBCI a programmable BCI with no set electrode configuration - a perfect tool to let the open-source community figure out what non-invasive BCIs are capable of."<br><br>"I hope that clarifies our mission a bit."</blockquote><p>Yes, it does, thank you!</p></div><a class=read-more href=/blog/openbci-nears-its-kickstarter-goal>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/sound-chamber-2013>Sound Chamber (2013)</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-12-27>Friday 27 December, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Sound%20Chamber%20(2013)&amp;url=http://jahya.net/blog/sound-chamber-2013&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Sound%20Chamber%20(2013)&amp;u=http://jahya.net/blog/sound-chamber-2013"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Sound%20Chamber%20(2013)&amp;u=http://jahya.net/blog/sound-chamber-2013"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>In what ways can a human body control continuously-generated synthetic sound?</p><p>That's the question that drove <a href="http://alex.hornbake.com/">Alex Hornbake</a> and I to create Sound Chamber, an exploratory audio-visual installation presented at <a href="http://info.thoughtworks.com/new-york/">ThoughtWorks New York</a> in 2013.</p><div style="text-align: center"><iframe style=border:0 height=280 width=500 src=//www.youtube.com/embed/78NCrNWdUdM title="YouTube video player"></iframe></div><p>This project was one of the outcomes of <a href="http://hardwarehacklab.tumblr.com/">Hardware Hack Lab</a>, a co-working space for technologists and artists run every Wednesday here in New York.</p><p>In it we took a close look at full-body interaction using commercial depth devices such as the Kinect or Asus Xtion. We tied these devices up to a custom-designed sound synthesis environment built with SuperCollider, and out also to video screens placed around the space.</p><p>Alex &amp; I considered this project a creative exploration, and the installation an unfinished demo - an outcome of the exploration. As such, all the code is <a href=https://github.com/alexhornbake/soundChamber>up on Github</a> along with instructions to replicate. We feel pretty good about adding this mode of interaction to our toolbox and plan to experiment more in the new year.</p></div><a class=read-more href=/blog/sound-chamber-2013>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/openbci-hackathon-at-thoughtworks>OpenBCI Hackathon at ThoughtWorks</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-11-20>Wednesday 20 November, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=OpenBCI%20Hackathon%20at%20ThoughtWorks&amp;url=http://jahya.net/blog/openbci-hackathon-at-thoughtworks&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=OpenBCI%20Hackathon%20at%20ThoughtWorks&amp;u=http://jahya.net/blog/openbci-hackathon-at-thoughtworks"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=OpenBCI%20Hackathon%20at%20ThoughtWorks&amp;u=http://jahya.net/blog/openbci-hackathon-at-thoughtworks"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>This weekend we held the first OpenBCI (Brain Computer Interface) hackathon at ThoughtWorks. We went from Saturday morning to Sunday night working on challenges of design, community, hardware and software.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-Ld0-Jmvwnao/UownNoD7QaI/AAAAAAAAC5g/B1_eq8jdnQY/s1600/20131116_124128.jpg><img alt="Conor Russomanno gives a crash course in EEG" src=http://4.bp.blogspot.com/-Ld0-Jmvwnao/UownNoD7QaI/AAAAAAAAC5g/B1_eq8jdnQY/s500/20131116_124128.jpg width=500 height=375></a></td></tr><tr><td class=tr-caption style="text-align: center">Conor Russomanno gives a crash course in <a href=http://en.wikipedia.org/wiki/Electroencephalography>EEG</a> at the hackathon</td></tr></tbody></table><p>We have been working with the <a href="http://www.openbci.com/">OpenBCI</a> guys since they started attending our weekly <a href="http://hardwarehacklab.tumblr.com/">Hardware Hack Lab</a>, also out of the ThoughtWorks New York office.</p><p>OpenBCI are <a href="http://conorrussomanno.me/">Conor Russomanno</a> and <a href="http://www.joelmurphy.net/">Joel Murphy</a>. These guys are serious about lowering the barrier of entry to research-grade EEG. Commercial sets like Neurosky do this but tie your hands in the process. OpenBCI is about making a viable open source alternative.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://3.bp.blogspot.com/-0DSVUpet_kU/Uowo1izSUHI/AAAAAAAAC50/9KX0a3MSqAQ/s1600/20131116_134918.jpg><img alt="Aisen Caro poses with a headcap" src=http://3.bp.blogspot.com/-0DSVUpet_kU/Uowo1izSUHI/AAAAAAAAC50/9KX0a3MSqAQ/s500/20131116_134918.jpg width=500 height=375></a></td></tr><tr><td class=tr-caption style="text-align: center"><a href="http://www.aisencaro.com/">Aisen Caro</a> poses with a headcap</td></tr></tbody></table><p>We had four sets of caps and hardware, and over the course of the two days different volunteers wore the gear so that we could test on real people.</p><p>Here's a short video of Joel demoing the application of a cap.</p><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/79853193?byline=0&amp;portrait=0"></iframe></div><p>There was a good buzz of excitement and energy and we would definitely host an OpenBCI hackathon again. Looking forward to the next one.</p></div><a class=read-more href=/blog/openbci-hackathon-at-thoughtworks>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/exploring-depth-video-at-culturehub>Exploring Depth Video at CultureHub</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-11-08>Friday 8 November, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Exploring%20Depth%20Video%20at%20CultureHub&amp;url=http://jahya.net/blog/exploring-depth-video-at-culturehub&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Exploring%20Depth%20Video%20at%20CultureHub&amp;u=http://jahya.net/blog/exploring-depth-video-at-culturehub"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Exploring%20Depth%20Video%20at%20CultureHub&amp;u=http://jahya.net/blog/exploring-depth-video-at-culturehub"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>A short video of me speaking at <a href="http://www.culturehub.org/">CultureHub</a>. If you are interested in the <a href=http://www.culturehub.org/volumetric_lab>Volumetric Lab</a> <a href="http://www.meetup.com/volumetric/">RGBDToolkit meetups</a>, this is the kind of work we do:</p><div style="text-align: center"><iframe style=border:0 height=281 width=500 src="http://player.vimeo.com/video/78873697?byline=0&amp;portrait=0"></iframe></div><p>Some more info on the lab:</p><p>The Volumetric Lab at CultureHub is an open source, member driven community dedicated to exploring 3D interactive software and hardware. Artists, innovators, and educators engage with interactive motion sensing technologies such as the Kinect and other depth sensing cameras to produce an array of research and experimental art projects. Participants are strongly encouraged to formulate projects with the open source philosophy in mind. We believe that free distribution and access to project development promotes innovation and empowerment.</p></div><a class=read-more href=/blog/exploring-depth-video-at-culturehub>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/introduction-to-ibeacons>Introduction to iBeacons</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-10-30>Wednesday 30 October, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Introduction%20to%20iBeacons&amp;url=http://jahya.net/blog/introduction-to-ibeacons&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Introduction%20to%20iBeacons&amp;u=http://jahya.net/blog/introduction-to-ibeacons"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Introduction%20to%20iBeacons&amp;u=http://jahya.net/blog/introduction-to-ibeacons"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>This post explores <em>iBeacons</em>. It may have been announced only quietly, but it is going to have a <a href="http://gigaom.com/2013/09/10/with-ibeacon-apple-is-going-to-dump-on-nfc-and-embrace-the-internet-of-things/">big impact</a> on how we perceive computing, and on the so-called Internet of Things.</p><p>iBeacons technology is being <a href="http://estimote.com/">promoted</a> as a service, particularly for <a href="http://www.tested.com/tech/smartphones/457481-why-apples-bluetooth-ibeacon-could-be-big-deal/">innovative brick-and-mortar retailers</a> to differentiate via micro-location, context-aware apps and analytics. However the technology has an equally powerful reductive effect on privacy, for those who choose not to be conscientious and self-impose limits.</p><p>This post focuses on the technology. I'll be covering what an Estimote Beacon is, what other types of Beacons there are, and how you can make your own. I'll go on to talk about how iBeacons is giving <a href=http://en.wikipedia.org/wiki/Near_field_communication>NFC</a> a run for it's money.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><iframe width=480 height=298 src=//www.youtube.com/embed/sUIqfjpInxY></iframe></td></tr><tr><td class=tr-caption style="text-align: center">The Estimote Beacon elevator pitch</td></tr></tbody></table><p>This post also disassembles the overloaded term <em>iBeacon</em>, which can be a little confusing at first when you hear it used to describe many different pieces of a larger puzzle.</p><p><strong>A Word on Privacy</strong><br>I've focused mostly on technology in this article, but I want to take a moment to comment on privacy. iBeacons make it possible to easily engage with people in a physical space via their mobile devices. But part of what will make the experience so compelling is the ability to triangulate the precise physical position of each participant. And as long as the person has Bluetooth enabled and your app installed, it will be possible to do this without their permission.</p><p>The old adage fits here, that just because you can do a thing doesn't mean you should do a thing. Permission is key. It's not just on moral grounds, but on grounds of building trust. If I enter into a space where beacons are active, I want to be told what value I can expect, and what data I am giving away to get that value. Once I opt in, I am engaged. If I choose not to opt in that should be respected.</p><p>There is also a distinction between <em>using</em> real-time data and <em>storing</em> real-time data. But ultimately this technology is out there right now, and people are going to start using it. We must be aware of privacy concerns and go ahead and learn about the technology. That's what this article is for.</p><p><strong>What is an Estimote Beacon?</strong><br>An Estimote Beacon is a device made by a Polish company that utilizes a newly-available mode in Bluetooth called Bluetooth Low Energy (BLE). Later in this article I will describe the iBeacons service on your phone, and alternative Beacon services, but first let's focus on the now well-promoted Beacon called Estimote, and the interactions it can provide.</p><p>In the picture below, the Estimote Beacon is broadcasting using BLE. The phone receives the broadcast information over it's own BLE, and the beacon-enabled app recognizes the user's proximity to the beacon. In this case, the beacon is positioned near a bar, and the user is offered a discount on a favorite drink. The favorite drink is identified by the user's purchase history, retrieved via the app.</p><p>As the name infers, BLE communicates in a similar manner to regular Bluetooth, but consumes much less power, meaning that the Estimote Beacon can run for 2 years on a tiny coin battery. If your phone supports BLE, and Bluetooth is enabled, beacon-enabled apps can work out your proximity to a beacon. If there are several beacons, the app can use the relative strength of the beacon signals to work out your precise location. The app can use this information to send push notifications and deliver context-relevant experiences to your phone.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-nhSbJuOJNkY/UnEnfYuYEnI/AAAAAAAACVA/D96BPl5p8a8/s1600/estimote-beacons-retail-app-Geoawesomeness.jpg><img alt="An iPhone connecting to an Estimote Beacon" src=http://4.bp.blogspot.com/-nhSbJuOJNkY/UnEnfYuYEnI/AAAAAAAACVA/D96BPl5p8a8/s500/estimote-beacons-retail-app-Geoawesomeness.jpg width=500 height=260></a></td></tr><tr><td class=tr-caption style="text-align: center">An iPhone connecting to an Estimote Beacon</td></tr></tbody></table><p>The canonical example is a retail store and merchandise stands. When people enter a space or visit certain stands they can be sent targeted information - text and small images, or linked to online rich media such as video or sound. This could be promotions, coupons, recommendations, marketing or informational content, and if there is an app running on the shopper's device and they are logged in, this can be personalized.</p><p>There can also be mashups, for example the micro-location equivalent of Google Maps. Users can search for a particular item and be guided to it's precise location, across the shop floor, up the elevator and so on. Ultimately, this is an early-stage technology and the possibilities are open.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://4.bp.blogspot.com/-8n82UHt8rYM/UnEnfdJpNnI/AAAAAAAACVE/uRtOSz5-dtY/s1600/applications1.jpg><img alt="Potential retail applications for iBeacons" src=http://4.bp.blogspot.com/-8n82UHt8rYM/UnEnfdJpNnI/AAAAAAAACVE/uRtOSz5-dtY/s500/applications1.jpg width=500 height=368></a></td></tr><tr><td class=tr-caption style="text-align: center">Potential retail applications for iBeacons</td></tr></tbody></table><p>There is also the possibility for those with logged-in accounts to have contactless payment on leaving the store, since we know exactly when users are leaving. More on an alternative way of doing that with a PayPal dongle below.</p><p><strong>Analytics</strong><br>One other feature which will be coveted by many retailers is the ease with which you will be able to track visitor's precise movements through a store. This brings web-style analytics that much closer to physical retail, as we are able to get quantitative data on more and more details of people's interactions with store merchandise.</p><p>As discussed earlier use of this feature should be approached openly and in an opt-in fashion. Notice the people in the 'Proximity Marketing' section of the image above? These interactions should be handled delicately.</p><p><strong>What is/are iBeacons?</strong><br>Bluetooth Low Energy (BLE), mentioned earlier, is part of the Bluetooth Specification 4.0 (aka Bluetooth Smart). This flavour of Bluetooth is now available on many phone and tablet devices, and has been baked quietly into iOS7. Most new devices will be BLE compatible:</p><blockquote><p>"The majority of new devices entering the market, including the HTC One, Nokia Lumias, Samsung Galaxy Nexus and the Blackberry Z10 and Q10, among others, are all BLE compatible. In terms of iOS devices, the iPhone 4S and above, the iPad with Retina display and the iPad mini are all BLE compatible."<br><a href=http://mubaloo.com/news-info/ibeacons-and-bluetooth-low-energy-how-will-it-benefit-the-b2b-b2c-industries>Source - Mubaloo</a></p></blockquote><p>This feature of Apple devices and the services that go with it have been named <em>iBeacons</em>, in classic Apple 'iThing' style. But the term <em>iBeacon</em> has become overloaded. I have seen it used in the following contexts:</p><ul><li>a generic term for a physical beacon in real space, such as an Estimote Beacon</li><li>the software platform in iOS7 which allows Bluetooth 4.0 hardware to use BLE and talk to beacons</li><li>the overall service, including the two cases above as one (referred to plurally as iBeacons)</li></ul><p>And that's just with iOS, where the term originated.</p><p><strong>Who else is in this Space?</strong><br>Following closely behind Apple are Android equivalents to all of the above, and many times the naming includes the term <em>iBeacon</em> in a me-too fashion. Radius Networks, a start-up out of Washington D.C. offer physical <a href=http://www.radiusnetworks.com/ibeacon.html>iBeacons</a> ($99), and an Android SDK to allow developers to write apps for compatible devices, which they refer to as the <a href=https://github.com/RadiusNetworks/android-ibeacon-service>Android iBeacon Service</a>.</p><p>The physical iBeacons by Radius are just a <a href="http://www.beekn.net/2013/10/ibeacon-for-raspberry-pi-offered-by-radius-networks/">customized Raspberry Pi</a>, which tells you something about the nature of the service - all you need to get going with this is a Bluetooth 4.0 dongle and something to run free software on.</p><p>A San Francisco-based startup called COIN offers an <a href="http://beekn.net/2013/10/arduino-based-bluetooth-le-with-ibeacons-potential/">Arduino-based beacon</a> for $22. In the same vein, you can <a href=http://developer.radiusnetworks.com/2013/10/09/how-to-make-an-ibeacon-out-of-a-raspberry-pi.html>make your own</a> Beacon out of a Raspberry Pi and a Bluetooth 4.0 dongle.</p><p>PayPal have a strong presence in this space, having developed a Beacon-as-USB-dongle. You guessed it, this is just a Bluetooth 4.0 dongle for a retailer's laptop, with some bundled PayPal software. From a customer's perspective:</p><blockquote><p>"By checking in to a store &agrave; la Foursquare (you can even configure the app check you into places automatically), that retailer has access to the funds in your PayPal account and you can pay for your items directly with that money. It's proximity-based, so you do have to be physically present at the store. The security check happens when the retailer is shown a picture of your face to make sure that you're who you say you are. With that confirmed, your total purchase is deducted from your PayPal account."<br><a href=http://www.businessinsider.com/paypal-beacon-2013-10>Source - Business Insider</a></p></blockquote><p>And there is one more iBeacon. It's a <a href="http://www.ibeacon.co/">phone accessory on kickstarter</a> which was unfortunate enough to have labeled all their hardware with the <em>iBeacon</em> name. And has just had that name usurped...</p><p><strong>What are Virtual Beacons?</strong><br>An <a href=https://itunes.apple.com/us/app/estimote-virtual-beacon/id686915066>Estimote Virtual Beacon</a> is a free iOS app that turns your iWhatever into an iBeacon using it's existing BLE hardware (app released Sep 19). With it, you can set up one iPhone as an iBeacon, position it, and use it to track another. A whole iPhone is a bit heavy to serve as just a beacon, but it's useful for testing and playing with the SDK.</p><p>Of course, hot on the heels is an Android version (Oct 17), called <a href="https://play.google.com/store/apps/details?id=com.radiusnetworks.ibeaconlocate">iBeacon Locate</a>, also by Radius Networks. To use these apps your phone will need to support Bluetooth 4.0, as described above. In the case of Android, this means you will <a href=http://developer.android.com/guide/topics/connectivity/bluetooth-le.html>need to be running</a> Android 4.3.</p><p>You can also run an iBeacon <a href=http://developer.radiusnetworks.com/ibeacon/virtual.html>on a virtual machine</a>, although I'm not sure yet why you would want to do that...</p><p><strong>What are the Implications for NFC and RFID?</strong><br>NFC is RFID's <a href=http://www.howstuffworks.com/difference-between-rfid-and-nfc.htm>younger cousin</a>, which offers contactless payment and data exchange at short (4cm-ish) distances. Apple stalled for a long time when it comes to including NFC reader hardware on their devices, to the frustration of NFC advocates. On the other hand, many Android devices support NFC and for a while there was a question as to why Apple wasn't jumping on board.</p><p>Now we know. Apple is betting on BLE, because as well as winning on <a href="http://gigaom.com/2013/09/10/with-ibeacon-apple-is-going-to-dump-on-nfc-and-embrace-the-internet-of-things/">cost and range</a>, BLE is already <a href=https://github.com/RadiusNetworks/android-ibeacon-service>cross-platform</a>. Apple have <a href="http://gigaom.com/2013/09/10/with-ibeacon-apple-is-going-to-dump-on-nfc-and-embrace-the-internet-of-things/">played their hand against NFC</a>.</p><p>But RFID is bigger and more established than NFC. And it does things that iBeacons doesn't do. Humans may carry phones but physical objects don't! In-store merchandise, warehouse stock, any physical objects will be better served by RFID tags. Merchandise tracking is still a key area for RFID, and that translates in-store too. When I pick up a shirt and carry it to a changing room, this interaction will not be visible to any iBeacon.</p><p>If you want to use technology to offer participants a full experience including smart interactions with objects beyond localized areas like merchandising stands, you may want to consider a mixed solution involving RFID and iBeacons.</p></div><a class=read-more href=/blog/introduction-to-ibeacons>Read more &gt;</a><hr class=separator></article><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1><a href=/blog/volumetric-lab-at-culturehub-nyc>Volumetric Lab at CultureHub NYC</a></h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2013-10-02>Wednesday 2 October, 2013</time><ul class=social><li><a href="http://twitter.com/share?text=Volumetric%20Lab%20at%20CultureHub%20NYC&amp;url=http://jahya.net/blog/volumetric-lab-at-culturehub-nyc&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Volumetric%20Lab%20at%20CultureHub%20NYC&amp;u=http://jahya.net/blog/volumetric-lab-at-culturehub-nyc"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Volumetric%20Lab%20at%20CultureHub%20NYC&amp;u=http://jahya.net/blog/volumetric-lab-at-culturehub-nyc"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>I've been attending the newly-formed <a href=http://www.culturehub.org/volumetric_lab>Volumetric Lab</a> weekly since the <a href=//blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit workshop</a> got me started with depth video. It's a fun and experimental workshop run by Volumetric Society's Ellen Pearlman and CultureHub's S.O. O'Brien.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><a href=http://1.bp.blogspot.com/-lL5rwK8vI5Y/UkuNAxH4erI/AAAAAAAABXA/ZGAGYUEtJME/s1600/1.jpg><img alt="Showing Study: Existential at CultureHub NYC" src=http://1.bp.blogspot.com/-lL5rwK8vI5Y/UkuNAxH4erI/AAAAAAAABXA/ZGAGYUEtJME/s500/1.jpg width=500 height=333></a></td></tr><tr><td class=tr-caption style="text-align: center">Showing Study: Existential at CultureHub NYC (<a href="http://www.flickr.com/photos/culturehubnyc/10041365934/in/set-72157636091875545/">credit</a>)</td></tr></tbody></table><p>The lab's initial run culminated with the <a href=http://www.culturehub.org/events1/2013/9/25/season-kickoff-party-open-volumetric-lab-93013.html>kick-off party</a> for the new Volumetric Society season for 2013/14 yesterday. It was a great event with high energy and a confident feel. We put out a call to other artists and tinkerers who might want to <a href=http://www.culturehub.org/volumetric_lab_application>attend the Sunday lab</a>.</p><p>You can check out and find more about the <a href=//blog/study-existential>video I showed at the event here</a>.</p><p><strong>Chris Burke - This Spartan Life</strong><br>Also showing work was Chris Burke, who I later discovered was the creator of <a href=http://en.wikipedia.org/wiki/This_Spartan_Life>This Spartan Life</a>, a TV-style talk show shot realtime by characters in the space of a functioning online Halo game.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tbody><tr><td style="text-align: center"><iframe width=500 height=304 src="http://www.youtube.com/embed/videoseries?list=PL8qg4rgqBK3jpz3VMv_ZxGTTfyTJSkPo9"></iframe></td></tr><tr><td class=tr-caption style="text-align: center">This Spartan Life: Episode 1</td></tr></tbody></table><p>In the first video we find out some of the pitfalls of shooting a talk show in a space where people are actively trying to kill each other! We also see the show debut of The Solid Gold Elite Dancers, five players in the game showing their synchronized dance routine.</p><p>Videos of the show have recently been uploaded to Youtube, and I highly recommend checking it out!</p></div><a class=read-more href=/blog/volumetric-lab-at-culturehub-nyc>Read more &gt;</a><hr class=separator></article><ul id=paging class=cta><li><a href="/blog/">&lt; Previous page</a></li><li><a href="/blog/page3/">Next page &gt;</a></li></ul></main><aside class=right><nav><header><h1>All posts</h1></header><h2 id=2015-ref>2015</h2><ul><li><a href=/blog/new-site-for-hardware-hack-lab>New Site for Hardware Hack Lab</a></li></ul><h2 id=2014-ref>2014</h2><ul><li><a href=/blog/sound-control-at-future-interfaces>Sound Control at Future Interfaces</a></li><li><a href=/blog/projection-masking-not-projection>Projection Masking, not Projection Mapping</a></li><li><a href=/blog/addon-for-openframeworks-kinect-v2-and>Addon for openFrameworks, Kinect V2 and Mac</a></li><li><a href=/blog/john-cleese-on-creativity>John Cleese on Creativity</a></li><li><a href=/blog/takeaways-from-eyeo-2014>Takeaways from Eyeo 2014</a></li><li><a href=/blog/hardware-hacker-culture-of-new-york>Hardware Hacker Culture of New York</a></li><li><a href=/blog/future-visions-for-human-interaction>Future Visions for Human Interaction</a></li><li><a href=/blog/openbci-nears-its-kickstarter-goal>OpenBCI Nears it's Kickstarter Goal</a></li></ul><h2 id=2013-ref>2013</h2><ul><li><a href=/blog/sound-chamber-2013>Sound Chamber (2013)</a></li><li><a href=/blog/openbci-hackathon-at-thoughtworks>OpenBCI Hackathon at ThoughtWorks</a></li><li><a href=/blog/exploring-depth-video-at-culturehub>Exploring Depth Video at CultureHub</a></li><li><a href=/blog/introduction-to-ibeacons>Introduction to iBeacons</a></li><li><a href=/blog/volumetric-lab-at-culturehub-nyc>Volumetric Lab at CultureHub NYC</a></li><li><a href=/blog/randomness-in-algorithm>Randomness in the Algorithm</a></li><li><a href=/blog/study-existential>Video: Existential</a></li><li><a href=/blog/the-visual-art-of-brian-eno>The Visual Art of Brian Eno</a></li><li><a href=/blog/rgbdtoolkit-sketch-at-sampler>RGBDToolkit Sketch at The Sampler</a></li><li><a href=/blog/the-artist-geek-hybrid>The Artist-Geek Hybrid</a></li><li><a href=/blog/rgbdtoolkit-calibration-tutorial>RGBDToolkit Calibration Tutorial</a></li><li><a href=/blog/how-depth-sensor-works-in-5-minutes>How a Depth Sensor Works - in 5 Minutes</a></li><li><a href=/blog/focal-lengths-and-camera-sensors>Focal Lengths and Camera Sensors</a></li><li><a href=/blog/rgbdtoolkit-visualizer-tutorial>RGBDToolkit Visualizer Tutorial</a></li><li><a href=/blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit Workshop at Eyebeam</a></li><li><a href=/blog/nosql-distilled-to-keynote>NoSQL Distilled to a Keynote!</a></li><li><a href=/blog/paper-prototyping>Paper Prototyping</a></li><li><a href=/blog/git-vs-github>Is Git the Same Thing as Github!?</a></li></ul><h2 id=2012-ref>2012</h2><ul><li><a href=/blog/counterpoint-to-remote>Counterpoint to the Remote</a></li><li><a href=/blog/the-cascading-process>The Cascading Process</a></li><li><a href=/blog/working-with-total-space>Working with Total Space</a></li><li><a href=/blog/residency-begins-at-cac-troy>Residency Begins at CAC Troy</a></li><li><a href=/blog/installation-sketch-at-open-studios>Installation Sketch at Open Studios</a></li><li><a href=/blog/roman-moshenskys-mirror-world>Roman Moshensky's Mirror World</a></li><li><a href=/blog/open-studios-at-i-park>Open Studios at I-Park</a></li><li><a href=/blog/perception-as-creative-process>Perception as a Creative Process</a></li><li><a href=/blog/the-i-park-graveyard>The I-Park Graveyard</a></li><li><a href=/blog/scoping-out-land>Scoping Out the Land</a></li><li><a href=/blog/residency-begins-at-i-park>Residency Begins at I-Park</a></li><li><a href=/blog/residency-at-contemporary-artists-center>Residency at Contemporary Artists Center</a></li><li><a href=/blog/the-shopping-list-for-projection-bombing>First Shopping List for Projection-Bombing</a></li><li><a href=/blog/portable-projection-in-rural-context>Portable Projection in a Rural Context</a></li><li><a href=/blog/stephen-lumentas-sc-textmate-bundle>Stephen Lumenta's SC TextMate Bundle</a></li><li><a href=/blog/adding-openframeworks-addons>Adding OF Addons (ofxSuperCollider)</a></li><li><a href=/blog/setting-up-supercollider-with-textmate>Setting up SuperCollider with TextMate</a></li><li><a href=/blog/switching-to-macbook-pro>Switching to MacBook Pro</a></li><li><a href=/blog/quickref-for-supercollider>QuickRef for SuperCollider</a></li><li><a href=/blog/getting-started-with-supercollider>Getting Started with SuperCollider</a></li><li><a href=/blog/getting-started-with-openframeworks-in>Getting Started with OpenFrameworks</a></li><li><a href=/blog/overtones-harmonics-and-additive>Overtones, Harmonics and Additive Synthesis</a></li><li><a href=/blog/visit-to-cold-spring>Visit to Cold Spring</a></li><li><a href=/blog/i-park-residency>Residency at I-Park</a></li><li><a href=/blog/light-waves>Light Waves</a></li></ul><h2 id=2011-ref>2011</h2><ul><li><a href=/blog/final-exhibition>The Final Exhibition</a></li><li><a href=/blog/playing-with-particles>Playing with Particles</a></li><li><a href=/blog/responsive-granular-sound>Responsive Granular Sound</a></li><li><a href=/blog/kinecting-to-network>Kinecting to the Network</a></li><li><a href=/blog/first-working-day>First Working Day</a></li><li><a href=/blog/designs-for-freemote>Designs for Freemote</a></li><li><a href=/blog/freemote-utrecht>Freemote Utrecht</a></li><li><a href=/blog/untitled-picture-this-2011>Untitled - Picture This (2011)</a></li><li><a href=/blog/wider-context>The Wider Context?</a></li><li><a href=/blog/trading-time-for-space>Trading Time for Space</a></li><li><a href=/blog/talk-at-goldsmiths-digital-studios>Talk at Goldsmiths Digital Studios</a></li><li><a href=/blog/intro-to-marius-watz>Intro to Marius Watz</a></li><li><a href=/blog/practical-guide-to-generative-art>Practical Guide to Generative Art</a></li><li><a href=/blog/installation-at-alpha-ville>Installation at Alpha-Ville</a></li><li><a href=/blog/simple-harmonic-motion>Simple Harmonic Motion</a></li><li><a href=/blog/jaaga-journal-features>Jaaga Journal Features</a></li><li><a href=/blog/gravity-2011>Gravity (2011)</a></li><li><a href=/blog/reflections-2011>Reflections (2011)</a></li><li><a href=/blog/jaaga-sound-lights>Jaaga Sound & Lights</a></li><li><a href=/blog/two-works-for-jaaga-gravity-and-memory>Two Works for Jaaga: Gravity and Reflections</a></li><li><a href=/blog/cosm-collision-detection-and-volume>Cosm, Collision Detection and Volume</a></li><li><a href=/blog/vector-base-amplitude-panning>Vector-Base Amplitude Panning</a></li><li><a href=/blog/intuition-and-direction-of-project>Intuition, and Direction of the Project</a></li><li><a href=/blog/reflections-what-is-jaaga>Reflections: What is Jaaga?</a></li><li><a href=/blog/going-further-with-ambisonics>Going Further with Ambisonics</a></li><li><a href=/blog/introduction-to-ambisonics>Introduction to Ambisonics</a></li><li><a href=/blog/surface-light-sound-installation>Surface (2010)</a></li><li><a href=/blog/running-servo-motor>Servo Motors and Transistors</a></li><li><a href=/blog/spinning-12v-dc-motor>Spinning a 12V DC Motor</a></li><li><a href=/blog/spinning-dc-motor>Spinning a 5V DC Motor</a></li><li><a href=/blog/first-week-at-jaaga>First Week at Jaaga</a></li><li><a href=/blog/presentation-style>Presentation Style</a></li><li><a href=/blog/beginning-jaaga-fellowship>Beginning the Jaaga Fellowship</a></li><li><a href=/blog/brian-eno-role-models-and-direction>Brian Eno, Role Models and Direction</a></li><li><a href=/blog/hype-cycle_17>The Hype Cycle</a></li></ul><h2 id=2010-ref>2010</h2><ul><li><a href=/blog/working-with-3d-space>Working with 3D Space</a></li><li><a href=/blog/technology-and-luck>Technology and Luck</a></li><li><a href=/blog/gallery-types-and-commercial-gallery>Different Types of Gallery</a></li><li><a href=/blog/jason-bruges-studio>Jason Bruges Studio</a></li><li><a href=/blog/unstable-empathy-trust-and>Unstable Empathy & Gaining Trust</a></li><li><a href=/blog/chris-o-two-creative-cultures>Chris O'Shea & Two Creative Cultures</a></li></ul></nav></aside></div><script src=/javascript/script.e2be.js></script></body></html>