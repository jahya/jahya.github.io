<!DOCTYPE html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width, initial-scale=1.0"><meta name=author content="Andrew McWilliams"><title>Future Visions for Human Interaction</title><link href=http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css rel=stylesheet><link href="http://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic" rel=stylesheet type=text/css><link rel=stylesheet href=/stylesheets/style.83cf.css><!--[if lt IE 9]>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.2/html5shiv-printshiv.min.js"></script>
    <![endif]--><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.4f09.png><link rel="shortcut icon" href=/images/favicon.6537.png></head><body id=blog class=content><header role=banner><h1><a href="/"><span class=shift>Andrew</span> M<span class=sub>c</span>Williams</a></h1><nav role=navigation><h2>Navigation</h2><a id=menu-button href=#><i class="fa fa-bars"></i> <i class="fa fa-times"></i></a><ul class=cta><li><a href="/about/">About</a></li><li><a href="/works/">Works</a></li><li><a href="/blog/">Blog</a></li></ul></nav></header><div class=flex-row><main class=left><article role=article itemscope itemtype=http://schema.org/BlogPosting><header><h1>Future Visions for Human Interaction</h1><div class=widgets>Posted on <time itemprop=dateCreated datetime=2014-02-10>Monday 10 February, 2014</time><ul class=social><li><a href="http://twitter.com/share?text=Future%20Visions%20for%20Human%20Interaction&amp;url=http://jahya.net/blog/future-visions-for-human-interaction&amp;via=hardwarehacklab"><i class="fa fa-twitter-square"></i></a></li><li><a href="http://www.facebook.com/sharer/sharer.php?title=Future%20Visions%20for%20Human%20Interaction&amp;u=http://jahya.net/blog/future-visions-for-human-interaction"><i class="fa fa-facebook-square"></i></a></li><li><a href="http://www.tumblr.com/share?v=3&amp;t=Future%20Visions%20for%20Human%20Interaction&amp;u=http://jahya.net/blog/future-visions-for-human-interaction"><i class="fa fa-tumblr-square"></i></a></li></ul></div></header><div class=post-content><p>Over the course of an hour at ThoughtWorks last week Ken Perlin described a vision for the future of immersive human interaction.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><iframe width=500 height=281 src=//www.youtube.com/embed/nHBAGke3eCU></iframe></td></tr></table><p>It was rich and varied subject matter, and it drew a line from Ken's early inspirations right through current research and beyond.</p><p>The Internet Society's Joly MacFie was on hand to film it (above), but I'll summarize the thrust of the argument here so you can get a sense. I also want to sprinkle in a few of my own comments and reactions.</p><p><strong>What this talk is really about</strong><br>Over the course of the hour, Ken weaves through a range of subjects including narrative, immersion, imagination, creativity, shorthand pop culture references to the 'future', and human nature - already a lot for one hour.</p><p>Add to that a range of technology subjects - wearables, implants, depth &amp; holography, virtual &amp; augmented reality, machine learning, kinematics and software programming - and you find yourself with plenty of rabbit holes to go down.</p><p>However, the real vision is all here in this 1-minute clip. In the video below, Professor Whoopee helpfully explains the functioning of a CRT using his 3DBB - his 3-dimensional blackboard (ofcourse). Check it out, and particularly watch how Professor Whoopee uses the 3DBB to communicate and interact with the other characters around him:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><iframe width=420 height=315 src=//www.youtube.com/embed/D3OSTflMO80></iframe></td></tr></table><p>The 3DBB is like an immersive environment, in which Professor Whoopee can create and operate a virtual, functioning CRT, which he and the other characters around him can all have a shared, volumetric experience of.</p><p>Bearing in mind that context, have a look through a transcript of Ken's closing statements to the audience last Tuesday:</p><p><blockquote>"Eventually, when you and I are face-to-face in an augmented version of reality, and we have nothing but our bodies and our eyes and our hands and each other... then we'll be able to use these very very simple techniques, because we've understood the semantics of how I create something for you in realtime. And what I can offer is this library [of intuitively instantiated, intelligent and directable, interactive yet autonomous virtual objects]..."</blockquote></p><p><blockquote>"Then the virtual world we have between us becomes something that is not just a replication of our physical reality, but actually the kind of reality we'd really like to be in"</blockquote></p><p>In this vision, we are able to spontaneously create and manipulate logically-consistent virtual objects, or we might say 'directable actors' since they are semi-autonomous. These object/actors surrounding us are not impositions on physical reality. As a consequence of immersion, they are indistinguishable, an effectively inseparable component of reality itself.</p><p>The actor/objects are viewable and interoperable in the space between multiple people, similar to how we currently imagine future holographic interfaces.</p><p>But we don't interact with these objects by using a portion of space which we would identifiably define as being an interface. The net effect of immersion combined with shared experience is that the virtual-physical reality we inhabit precludes the need for an interface.</p><p>All of it, or rather none of it is the interface.</p><p><strong>This versus other visions</strong><br>I think a lot of people will find that hard to imagine. One way to try and imagine it is to contrast it with other visions of the future.</p><p>In his <a href=http://worrydream.com/#!/ABriefRantOnTheFutureOfInteractionDesign>Brief Rant on the Future of Interaction Design</a>, Bret Victor takes a decent swipe at the vision presented in this video:</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><iframe width=500 height=281 src=//www.youtube.com/embed/a6cNdhOKwi0></iframe></td></tr></table><p>Bret's criticism is to note that the characters in such visions are immersed in an experience which can take on any shape or form (so long as that form blends the possible characteristics of virtual and physical worlds).</p><p>And yet to interact with this immersive reality, they turn to their hands and manipulate a little virtual 'phone'.</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><a href=""><img alt="" src=http://2.bp.blogspot.com/-PeGlFGKC1s4/Uva55r4cSoI/AAAAAAAAE-8/EvD86sJcLGU/s250/interaction1.jpg width=250 height=187></a><tr><td class=tr-caption style="text-align: center">Interacting with a contrived interface (<a href=http://worrydream.com/#!/ABriefRantOnTheFutureOfInteractionDesign>credit</a>)</td></tr></td></tr></table><p>It doesn't make sense.</p><p>A real, modern-day phone is like a glass window which you can swipe and prod, and there isn't a great deal of immersive haptic feedback.</p><p>Compare that to your intuitive sense of your place in a book by the relative density of pages in each hand. Or the amount of water by the shift in weight distribution as you tilt a glass.</p><table align=center class=tr-caption-container style="margin-left: auto; margin-right:auto; text-align: center"><tbody><tr><td style="text-align: center"><a href=http://3.bp.blogspot.com/-w3PQCe9EiGg/TuFNc6POW0I/AAAAAAAAATU/l7Z73pIRbn8/s1600/SL372378.JPG style="margin-left: 1em; margin-right: 1em"><img height=150 src=http://3.bp.blogspot.com/-KuU9iPMN7O8/Uva55pfjr8I/AAAAAAAAE_E/dgqbmKN-tJU/s200/interaction3.jpg width=200 alt="Interacting with a book"></a>&nbsp;&nbsp;&nbsp;&nbsp;<a href=http://4.bp.blogspot.com/-VWoX7sjxtn0/TuFNnXRMqqI/AAAAAAAAATc/9YcTiCHaVrs/s1600/SL372383.JPG style="margin-left: 1em; margin-right: 1em"><img height=150 src=http://4.bp.blogspot.com/-CmM-VzIYxE0/Uva55uV8p5I/AAAAAAAAE_A/WIRcuZ5nKoQ/s200/interaction2.jpg width=200 alt="Interacting with a glass"></a></td></tr><tr><td class=tr-caption style="text-align: center">Interacting with seamless, integral interfaces (<a href=http://worrydream.com/#!/ABriefRantOnTheFutureOfInteractionDesign>credit</a>)</td></tr></tbody></table><p>Natural human interactions are richly physical, and both Ken and Victor point out that we are currently going through a very odd transitional stage - walking around with our heads facing down, glued by our eyes and fingers to screens. Visions for future human interaction should allow that as soon as our dependency on physical devices for virtual interaction goes, so too goes that framework of interaction.</p><p><strong>If no interface, then what?</strong><br>Ken's answer is that the spontaneous creation of shared virtual actors will become a new staple of human communication, in much the same way that the ability to instantly communicate by video with people the other side of the world became a staple of human communication.</p><p>These actors will be scriptable and responsive to their environment, much like real actors on a stage. But hang on - we've already seen this type of thing recently haven't we?</p><table class=tr-caption-container style="margin-left: auto; margin-right: auto"><tr><td style="text-align: center"><iframe width=500 height=281 src=//www.youtube.com/embed/vv5uI2vlXE8></iframe></td></tr></table><p>The big difference between this and Ken's vision is that in the PS4 you enter into a contrived interaction with a specific subset of actors and scenarios. Because the hardware is not a seamless, integral element of your everyday experience, the user-experience case for being able to instantiate your own actors at will is much weaker.</p><p>This begs another question. If the interactions in Ken's vision are not to be contrived, doesn't this rely on each individual user to craft and nurture their own individual libraries of symbolic 'actors'? Or do most people subscribe to commercially available 'packs' of actors, and combine them to curate their own unique library?</p><p>Questions start flooding in. In what way does a library contribute to, or become reflective of, a person's personality? And tangentially, if these actors can't provide haptic feedback, are they really so different from the Minority Report interface?</p><p>Perhaps a further modification. What if it is the ability to transfer virtual characteristics to physical objects, and have those physical objects respond in a haptically-meaningful way that will provide the most engaging experience?</p><p><strong>Yes, great conjecture - but is any of this even possible?</strong><br>Ken spends most of his talk explaining how far he and his students have come, and how he expects technology to develop in the near future. I'd recommend watching if any of this interests you.</p><p>We will keep an eye on Ken's work over at Volumetric and catch up with him again to explore these questions in the future.</p></div></article><ul id=paging class=cta><li><a href=/blog/openbci-nears-its-kickstarter-goal>&lt; Previous post</a></li><li><a href=/blog/hardware-hacker-culture-of-new-york>Next post &gt;</a></li></ul></main><aside class=right><nav><header><h1>All posts</h1></header><h2 id=2015-ref>2015</h2><ul><li><a href=/blog/new-site-for-hardware-hack-lab>New Site for Hardware Hack Lab</a></li></ul><h2 id=2014-ref>2014</h2><ul><li><a href=/blog/sound-control-at-future-interfaces>Sound Control at Future Interfaces</a></li><li><a href=/blog/projection-masking-not-projection>Projection Masking, not Projection Mapping</a></li><li><a href=/blog/addon-for-openframeworks-kinect-v2-and>Addon for openFrameworks, Kinect V2 and Mac</a></li><li><a href=/blog/john-cleese-on-creativity>John Cleese on Creativity</a></li><li><a href=/blog/takeaways-from-eyeo-2014>Takeaways from Eyeo 2014</a></li><li><a href=/blog/hardware-hacker-culture-of-new-york>Hardware Hacker Culture of New York</a></li><li><a href=/blog/future-visions-for-human-interaction>Future Visions for Human Interaction</a></li><li><a href=/blog/openbci-nears-its-kickstarter-goal>OpenBCI Nears it's Kickstarter Goal</a></li></ul><h2 id=2013-ref>2013</h2><ul><li><a href=/blog/sound-chamber-2013>Sound Chamber (2013)</a></li><li><a href=/blog/openbci-hackathon-at-thoughtworks>OpenBCI Hackathon at ThoughtWorks</a></li><li><a href=/blog/exploring-depth-video-at-culturehub>Exploring Depth Video at CultureHub</a></li><li><a href=/blog/introduction-to-ibeacons>Introduction to iBeacons</a></li><li><a href=/blog/volumetric-lab-at-culturehub-nyc>Volumetric Lab at CultureHub NYC</a></li><li><a href=/blog/randomness-in-algorithm>Randomness in the Algorithm</a></li><li><a href=/blog/study-existential>Video: Existential</a></li><li><a href=/blog/the-visual-art-of-brian-eno>The Visual Art of Brian Eno</a></li><li><a href=/blog/rgbdtoolkit-sketch-at-sampler>RGBDToolkit Sketch at The Sampler</a></li><li><a href=/blog/the-artist-geek-hybrid>The Artist-Geek Hybrid</a></li><li><a href=/blog/rgbdtoolkit-calibration-tutorial>RGBDToolkit Calibration Tutorial</a></li><li><a href=/blog/how-depth-sensor-works-in-5-minutes>How a Depth Sensor Works - in 5 Minutes</a></li><li><a href=/blog/focal-lengths-and-camera-sensors>Focal Lengths and Camera Sensors</a></li><li><a href=/blog/rgbdtoolkit-visualizer-tutorial>RGBDToolkit Visualizer Tutorial</a></li><li><a href=/blog/rgbdtoolkit-workshop-at-eyebeam>RGBDToolkit Workshop at Eyebeam</a></li><li><a href=/blog/nosql-distilled-to-keynote>NoSQL Distilled to a Keynote!</a></li><li><a href=/blog/paper-prototyping>Paper Prototyping</a></li><li><a href=/blog/git-vs-github>Is Git the Same Thing as Github!?</a></li></ul><h2 id=2012-ref>2012</h2><ul><li><a href=/blog/counterpoint-to-remote>Counterpoint to the Remote</a></li><li><a href=/blog/the-cascading-process>The Cascading Process</a></li><li><a href=/blog/working-with-total-space>Working with Total Space</a></li><li><a href=/blog/residency-begins-at-cac-troy>Residency Begins at CAC Troy</a></li><li><a href=/blog/installation-sketch-at-open-studios>Installation Sketch at Open Studios</a></li><li><a href=/blog/roman-moshenskys-mirror-world>Roman Moshensky's Mirror World</a></li><li><a href=/blog/open-studios-at-i-park>Open Studios at I-Park</a></li><li><a href=/blog/perception-as-creative-process>Perception as a Creative Process</a></li><li><a href=/blog/the-i-park-graveyard>The I-Park Graveyard</a></li><li><a href=/blog/scoping-out-land>Scoping Out the Land</a></li><li><a href=/blog/residency-begins-at-i-park>Residency Begins at I-Park</a></li><li><a href=/blog/residency-at-contemporary-artists-center>Residency at Contemporary Artists Center</a></li><li><a href=/blog/the-shopping-list-for-projection-bombing>First Shopping List for Projection-Bombing</a></li><li><a href=/blog/portable-projection-in-rural-context>Portable Projection in a Rural Context</a></li><li><a href=/blog/stephen-lumentas-sc-textmate-bundle>Stephen Lumenta's SC TextMate Bundle</a></li><li><a href=/blog/adding-openframeworks-addons>Adding OF Addons (ofxSuperCollider)</a></li><li><a href=/blog/setting-up-supercollider-with-textmate>Setting up SuperCollider with TextMate</a></li><li><a href=/blog/switching-to-macbook-pro>Switching to MacBook Pro</a></li><li><a href=/blog/quickref-for-supercollider>QuickRef for SuperCollider</a></li><li><a href=/blog/getting-started-with-supercollider>Getting Started with SuperCollider</a></li><li><a href=/blog/getting-started-with-openframeworks-in>Getting Started with OpenFrameworks</a></li><li><a href=/blog/overtones-harmonics-and-additive>Overtones, Harmonics and Additive Synthesis</a></li><li><a href=/blog/visit-to-cold-spring>Visit to Cold Spring</a></li><li><a href=/blog/i-park-residency>Residency at I-Park</a></li><li><a href=/blog/light-waves>Light Waves</a></li></ul><h2 id=2011-ref>2011</h2><ul><li><a href=/blog/final-exhibition>The Final Exhibition</a></li><li><a href=/blog/playing-with-particles>Playing with Particles</a></li><li><a href=/blog/responsive-granular-sound>Responsive Granular Sound</a></li><li><a href=/blog/kinecting-to-network>Kinecting to the Network</a></li><li><a href=/blog/first-working-day>First Working Day</a></li><li><a href=/blog/designs-for-freemote>Designs for Freemote</a></li><li><a href=/blog/freemote-utrecht>Freemote Utrecht</a></li><li><a href=/blog/untitled-picture-this-2011>Untitled - Picture This (2011)</a></li><li><a href=/blog/wider-context>The Wider Context?</a></li><li><a href=/blog/trading-time-for-space>Trading Time for Space</a></li><li><a href=/blog/talk-at-goldsmiths-digital-studios>Talk at Goldsmiths Digital Studios</a></li><li><a href=/blog/intro-to-marius-watz>Intro to Marius Watz</a></li><li><a href=/blog/practical-guide-to-generative-art>Practical Guide to Generative Art</a></li><li><a href=/blog/installation-at-alpha-ville>Installation at Alpha-Ville</a></li><li><a href=/blog/simple-harmonic-motion>Simple Harmonic Motion</a></li><li><a href=/blog/jaaga-journal-features>Jaaga Journal Features</a></li><li><a href=/blog/gravity-2011>Gravity (2011)</a></li><li><a href=/blog/reflections-2011>Reflections (2011)</a></li><li><a href=/blog/jaaga-sound-lights>Jaaga Sound & Lights</a></li><li><a href=/blog/two-works-for-jaaga-gravity-and-memory>Two Works for Jaaga: Gravity and Reflections</a></li><li><a href=/blog/cosm-collision-detection-and-volume>Cosm, Collision Detection and Volume</a></li><li><a href=/blog/vector-base-amplitude-panning>Vector-Base Amplitude Panning</a></li><li><a href=/blog/intuition-and-direction-of-project>Intuition, and Direction of the Project</a></li><li><a href=/blog/reflections-what-is-jaaga>Reflections: What is Jaaga?</a></li><li><a href=/blog/going-further-with-ambisonics>Going Further with Ambisonics</a></li><li><a href=/blog/introduction-to-ambisonics>Introduction to Ambisonics</a></li><li><a href=/blog/surface-light-sound-installation>Surface (2010)</a></li><li><a href=/blog/running-servo-motor>Servo Motors and Transistors</a></li><li><a href=/blog/spinning-12v-dc-motor>Spinning a 12V DC Motor</a></li><li><a href=/blog/spinning-dc-motor>Spinning a 5V DC Motor</a></li><li><a href=/blog/first-week-at-jaaga>First Week at Jaaga</a></li><li><a href=/blog/presentation-style>Presentation Style</a></li><li><a href=/blog/beginning-jaaga-fellowship>Beginning the Jaaga Fellowship</a></li><li><a href=/blog/brian-eno-role-models-and-direction>Brian Eno, Role Models and Direction</a></li><li><a href=/blog/hype-cycle_17>The Hype Cycle</a></li></ul><h2 id=2010-ref>2010</h2><ul><li><a href=/blog/working-with-3d-space>Working with 3D Space</a></li><li><a href=/blog/technology-and-luck>Technology and Luck</a></li><li><a href=/blog/gallery-types-and-commercial-gallery>Different Types of Gallery</a></li><li><a href=/blog/jason-bruges-studio>Jason Bruges Studio</a></li><li><a href=/blog/unstable-empathy-trust-and>Unstable Empathy & Gaining Trust</a></li><li><a href=/blog/chris-o-two-creative-cultures>Chris O'Shea & Two Creative Cultures</a></li></ul></nav></aside></div><script src=/javascript/script.e2be.js></script></body></html>